{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1e097df-bdb8-4625-9960-2d05603e4a49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from /root/autodl-tmp/Pro/qwen2.5-sft-full\n",
      "Tokenizer loaded.\n",
      "  eos_token: <|endoftext|>\n",
      "  bos_token: <|startoftext|>\n",
      "  pad_token: <|endoftext|>\n",
      "  pad_token_id: 151643\n",
      "  vocab_size: 151665\n",
      "Reading Ulysses txt: /root/autodl-tmp/Pro/Ulysses尤利西斯.txt\n",
      "Found 7169 paragraphs; will pack paragraphs into chunks <= 1024 tokens.\n",
      "Total token chunks prepared: 459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset built: 459 examples\n",
      "Loading base model from: /root/autodl-tmp/Pro/qwen2.5-sft-full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.12it/s]\n",
      "/root/miniconda3/envs/qwen_text/lib/python3.11/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/qwen_text/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,261,568 || all params: 7,616,878,080 || trainable%: 0.0166\n",
      "Model device: cuda:0\n",
      "Training config -- epochs: 2 batches_per_epoch: 459 total_steps(approx): 229\n",
      "[Epoch 1] Step 20 | loss: 2.4292 | step_time: 00:00:04\n",
      "[Epoch 1] Step 40 | loss: 2.8493 | step_time: 00:00:07\n",
      "[Epoch 1] Step 60 | loss: 3.1239 | step_time: 00:00:11\n",
      "[Epoch 1] Step 80 | loss: 2.8827 | step_time: 00:00:15\n",
      "[Epoch 1] Step 100 | loss: 2.8391 | step_time: 00:00:19\n",
      "[Epoch 1] Step 120 | loss: 2.8489 | step_time: 00:00:22\n",
      "[Epoch 1] Step 140 | loss: 2.8245 | step_time: 00:00:26\n",
      "[Epoch 1] Step 160 | loss: 2.8936 | step_time: 00:00:30\n",
      "[Epoch 1] Step 180 | loss: 2.7989 | step_time: 00:00:34\n",
      "[Epoch 1] Step 200 | loss: 2.7810 | step_time: 00:00:37\n",
      "[Epoch 1] Step 220 | loss: 2.7274 | step_time: 00:00:41\n",
      "[Epoch 1] Step 240 | loss: 2.6937 | step_time: 00:00:45\n",
      "[Epoch 1] Step 260 | loss: 2.6742 | step_time: 00:00:48\n",
      "[Epoch 1] Step 280 | loss: 2.6603 | step_time: 00:00:52\n",
      "[Epoch 1] Step 300 | loss: 2.6799 | step_time: 00:00:56\n",
      "[Epoch 1] Step 320 | loss: 2.6282 | step_time: 00:00:59\n",
      "[Epoch 1] Step 340 | loss: 2.5726 | step_time: 00:01:03\n",
      "[Epoch 1] Step 360 | loss: 2.5276 | step_time: 00:01:07\n",
      "[Epoch 1] Step 380 | loss: 2.4986 | step_time: 00:01:11\n",
      "[Epoch 1] Step 400 | loss: 2.4473 | step_time: 00:01:14\n",
      "[Epoch 1] Step 420 | loss: 2.4162 | step_time: 00:01:18\n",
      "[Epoch 1] Step 440 | loss: 2.3848 | step_time: 00:01:22\n",
      "Epoch 1 finished in 00:01:25\n",
      "[Epoch 2] Step 460 | loss: 0.0085 | step_time: 00:00:00\n",
      "[Epoch 2] Step 480 | loss: 0.0791 | step_time: 00:00:03\n",
      "[Epoch 2] Step 500 | loss: 0.1484 | step_time: 00:00:07\n",
      "[Epoch 2] Step 520 | loss: 0.1840 | step_time: 00:00:11\n",
      "[Epoch 2] Step 540 | loss: 0.2568 | step_time: 00:00:15\n",
      "[Epoch 2] Step 560 | loss: 0.3102 | step_time: 00:00:18\n",
      "[Epoch 2] Step 580 | loss: 0.3674 | step_time: 00:00:22\n",
      "[Epoch 2] Step 600 | loss: 0.4173 | step_time: 00:00:26\n",
      "[Epoch 2] Step 620 | loss: 0.4538 | step_time: 00:00:29\n",
      "[Epoch 2] Step 640 | loss: 0.4886 | step_time: 00:00:33\n",
      "[Epoch 2] Step 660 | loss: 0.5320 | step_time: 00:00:37\n",
      "[Epoch 2] Step 680 | loss: 0.5606 | step_time: 00:00:41\n",
      "[Epoch 2] Step 700 | loss: 0.5922 | step_time: 00:00:44\n",
      "[Epoch 2] Step 720 | loss: 0.6334 | step_time: 00:00:48\n",
      "[Epoch 2] Step 740 | loss: 0.6604 | step_time: 00:00:52\n",
      "[Epoch 2] Step 760 | loss: 0.6994 | step_time: 00:00:56\n",
      "[Epoch 2] Step 780 | loss: 0.7172 | step_time: 00:00:59\n",
      "[Epoch 2] Step 800 | loss: 0.7253 | step_time: 00:01:03\n",
      "[Epoch 2] Step 820 | loss: 0.7569 | step_time: 00:01:07\n",
      "[Epoch 2] Step 840 | loss: 0.7746 | step_time: 00:01:11\n",
      "[Epoch 2] Step 860 | loss: 0.8025 | step_time: 00:01:14\n",
      "[Epoch 2] Step 880 | loss: 0.8290 | step_time: 00:01:18\n",
      "[Epoch 2] Step 900 | loss: 0.8403 | step_time: 00:01:22\n",
      "Epoch 2 finished in 00:01:25\n",
      "Saving finetuned model to: /root/autodl-tmp/Pro/qwen2.5-mindflow-ulysses\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# finetune_mindflow_ulysses.py 初版初级匹配意识流\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, PreTrainedTokenizerFast\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch.nn as nn\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG - 请按需修改\n",
    "# -------------------------\n",
    "CONFIG = {\n",
    "    \"base_model_path\": \"/root/autodl-tmp/Pro/qwen2.5-sft-full\",   # 模型一：已微调的诗歌模型（基座）\n",
    "    \"ulysses_txt\": \"/root/autodl-tmp/Pro/Ulysses尤利西斯.txt\",    # 尤利西斯原文 txt（本地）\n",
    "    \"output_dir\": \"/root/autodl-tmp/Pro/qwen2.5-mindflow-ulysses\",# 输出模型（二）\n",
    "    \"max_len\": 1024,      # 单个样本最大 token 长度（可调：1024/2048）\n",
    "    \"stride\": 512,        # 滑动窗口重叠\n",
    "    \"batch_size\": 1,      # per device batch size（通常 1 或 2）\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"num_train_epochs\": 2,\n",
    "    \"fp16\": True,\n",
    "    \"lora_r\": 4,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.01,\n",
    "    \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
    "    \"print_every_steps\": 20,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(CONFIG[\"seed\"])\n",
    "random.seed(CONFIG[\"seed\"])\n",
    "\n",
    "# -------------------------\n",
    "# 1. 加载 tokenizer（Qwen2 正确范式）\n",
    "# -------------------------\n",
    "print(\"Loading tokenizer from\", CONFIG[\"base_model_path\"])\n",
    "\n",
    "tokenizer_path = os.path.join(CONFIG[\"base_model_path\"], \"tokenizer.json\")\n",
    "if os.path.exists(tokenizer_path):\n",
    "    tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_path)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"base_model_path\"], trust_remote_code=True)\n",
    "\n",
    "# ======【Qwen2 推荐标准设置】======\n",
    "# 1) 明确设置 eos / bos（大多数 qwen2 tokenizer.json 自带，但确保一致）\n",
    "tokenizer.eos_token = \"<|endoftext|>\"\n",
    "tokenizer.bos_token = \"<|startoftext|>\"\n",
    "\n",
    "# 2) 添加 im_start / im_end\n",
    "# 注意：这一步会重建 vocab → pad_token 会丢失\n",
    "tokenizer.add_special_tokens({\n",
    "    \"additional_special_tokens\": [\"<|im_start|>\", \"<|im_end|>\"]\n",
    "})\n",
    "\n",
    "# 3) ★ 关键：add_special_tokens 后必须重新设置 pad_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Tokenizer loaded.\")\n",
    "print(\"  eos_token:\", tokenizer.eos_token)\n",
    "print(\"  bos_token:\", tokenizer.bos_token)\n",
    "print(\"  pad_token:\", tokenizer.pad_token)\n",
    "print(\"  pad_token_id:\", tokenizer.pad_token_id)\n",
    "print(\"  vocab_size:\", len(tokenizer))\n",
    "# =================================\n",
    "\n",
    "# -------------------------\n",
    "# 2. 从 Ulysses txt 生成 token chunks（滑动窗口）\n",
    "# -------------------------\n",
    "def load_and_tokenize_txt(txt_path, tokenizer, max_len, stride):\n",
    "    print(\"Reading Ulysses txt:\", txt_path)\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = f.read()\n",
    "\n",
    "    # 简单按段落拆分（保留语义完整性），段落间保留换行\n",
    "    paragraphs = [p.strip() for p in raw.split(\"\\n\\n\") if p.strip()]\n",
    "    print(f\"Found {len(paragraphs)} paragraphs; will pack paragraphs into chunks <= {max_len} tokens.\")\n",
    "\n",
    "    all_chunks = []\n",
    "    cur_ids = []\n",
    "    cur_chars = []\n",
    "\n",
    "    for para in paragraphs:\n",
    "        para_ids = tokenizer(para, add_special_tokens=False)[\"input_ids\"]\n",
    "        if len(para_ids) > max_len:\n",
    "            # 段落本身超长：按 token 窗口切分\n",
    "            start = 0\n",
    "            while start < len(para_ids):\n",
    "                end = min(start + max_len, len(para_ids))\n",
    "                chunk_ids = para_ids[start:end]\n",
    "                chunk_text = tokenizer.decode(chunk_ids, skip_special_tokens=True)\n",
    "                all_chunks.append(chunk_text)\n",
    "                if end == len(para_ids):\n",
    "                    break\n",
    "                start += max_len - stride\n",
    "            continue\n",
    "\n",
    "        # 若加入当前 chunk 后不超长则合并\n",
    "        if len(cur_ids) + len(para_ids) <= max_len:\n",
    "            cur_ids.extend(para_ids)\n",
    "            cur_chars.append(para)\n",
    "        else:\n",
    "            # flush current\n",
    "            if cur_ids:\n",
    "                all_chunks.append(tokenizer.decode(cur_ids, skip_special_tokens=True))\n",
    "            # start new\n",
    "            cur_ids = para_ids.copy()\n",
    "            cur_chars = [para]\n",
    "\n",
    "    # flush tail\n",
    "    if cur_ids:\n",
    "        all_chunks.append(tokenizer.decode(cur_ids, skip_special_tokens=True))\n",
    "\n",
    "    # 进一步应用滑动窗口以增加多样性（可选）\n",
    "    # 这里我们再把每 chunk 做 stride 分割，避免遗漏跨段上下文\n",
    "    final_chunks = []\n",
    "    for chunk in all_chunks:\n",
    "        ids = tokenizer(chunk, add_special_tokens=False)[\"input_ids\"]\n",
    "        start = 0\n",
    "        while start < len(ids):\n",
    "            end = min(start + max_len, len(ids))\n",
    "            part = tokenizer.decode(ids[start:end], skip_special_tokens=True)\n",
    "            final_chunks.append(part)\n",
    "            if end == len(ids):\n",
    "                break\n",
    "            start += max_len - stride\n",
    "\n",
    "    print(f\"Total token chunks prepared: {len(final_chunks)}\")\n",
    "    return final_chunks\n",
    "\n",
    "chunks = load_and_tokenize_txt(CONFIG[\"ulysses_txt\"], tokenizer, CONFIG[\"max_len\"], CONFIG[\"stride\"])\n",
    "\n",
    "# -------------------------\n",
    "# 3. 构造 Dataset（对话格式 + prefix-only mask）\n",
    "# -------------------------\n",
    "class UlyssesDataset(Dataset):\n",
    "    def __init__(self, chunks, tokenizer, max_len):\n",
    "        self.examples = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "        for idx, chunk_text in enumerate(chunks):\n",
    "            # user instruction: 要求生成意识流风格的段落（不抄袭原文）\n",
    "            user_instr = (\n",
    "                \"Write an original paragraph in James Joyce's stream-of-consciousness style \"\n",
    "                \"that captures similar imagery and tone as the following excerpt. \"\n",
    "                \"Do not copy exact phrases; be original.\\n\\n\"\n",
    "                f\"Excerpt:\\n{chunk_text}\"\n",
    "            )\n",
    "\n",
    "            # We will teach the model to generate text in this style by using the chunk_text\n",
    "            # as the assistant target. (You can replace this by paraphrases if desired.)\n",
    "            assistant_target = chunk_text\n",
    "\n",
    "            full = f\"<|im_start|>user\\n{user_instr}<|im_end|>\\n<|im_start|>assistant\\n{assistant_target}<|im_end|>\"\n",
    "            enc = tokenizer(full, max_length=self.max_len, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "            input_ids = enc[\"input_ids\"].squeeze(0)\n",
    "            attention_mask = enc[\"attention_mask\"].squeeze(0)\n",
    "            labels = input_ids.clone()\n",
    "\n",
    "            # find assistant start token index to mask prefix\n",
    "            # We search for the tokenizer encoding of \"<|im_start|>assistant\\n\"\n",
    "            # Simpler: find the first occurrence of \"<|im_start|>\" after the user block.\n",
    "            special_id = tokenizer.convert_tokens_to_ids(\"<|im_start|>\")\n",
    "            # locate positions\n",
    "            ids_list = input_ids.tolist()\n",
    "            assistant_pos = 0\n",
    "            try:\n",
    "                # find second occurrence of im_start (first is user)\n",
    "                first = ids_list.index(special_id)\n",
    "                second = ids_list.index(special_id, first + 1)\n",
    "                assistant_pos = second\n",
    "            except ValueError:\n",
    "                # fallback: try to find the textual marker\n",
    "                txt = tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "                marker = \"<|im_start|>assistant\\n\"\n",
    "                if marker in txt:\n",
    "                    assistant_pos = tokenizer(marker, add_special_tokens=False)[\"input_ids\"][0]\n",
    "                    # not reliable; fallback to 0\n",
    "                else:\n",
    "                    assistant_pos = 0\n",
    "\n",
    "            # mask prefix (user + instruction) so loss is only computed on assistant part\n",
    "            labels[:assistant_pos] = -100\n",
    "\n",
    "            self.examples.append({\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"labels\": labels\n",
    "            })\n",
    "\n",
    "        print(f\"Dataset built: {len(self.examples)} examples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "dataset = UlyssesDataset(chunks, tokenizer, CONFIG[\"max_len\"])\n",
    "dataloader = DataLoader(dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True, num_workers=0, drop_last=True)\n",
    "\n",
    "# -------------------------\n",
    "# 4. 加载基座模型并装上 LoRA\n",
    "# -------------------------\n",
    "print(\"Loading base model from:\", CONFIG[\"base_model_path\"])\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"base_model_path\"],\n",
    "    torch_dtype=torch.float16 if CONFIG[\"fp16\"] else torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    target_modules=CONFIG[\"target_modules\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()\n",
    "model.train()\n",
    "\n",
    "# device for inputs: model may be sharded; use next(model.parameters()).device\n",
    "model_device = next(model.parameters()).device\n",
    "print(\"Model device:\", model_device)\n",
    "\n",
    "# -------------------------\n",
    "# 5. optimizer & loss\n",
    "# -------------------------\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# -------------------------\n",
    "# 6. 训练循环\n",
    "# -------------------------\n",
    "def format_time(seconds):\n",
    "    m, s = divmod(int(seconds), 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "\n",
    "total_steps = len(dataloader) * CONFIG[\"num_train_epochs\"] // CONFIG[\"gradient_accumulation_steps\"]\n",
    "print(\"Training config -- epochs:\", CONFIG[\"num_train_epochs\"], \"batches_per_epoch:\", len(dataloader), \"total_steps(approx):\", total_steps)\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(CONFIG[\"num_train_epochs\"]):\n",
    "    epoch_start = time.time()\n",
    "    running_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        global_step += 1\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(model_device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(model_device)\n",
    "        labels = batch[\"labels\"].to(model_device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss / CONFIG[\"gradient_accumulation_steps\"]\n",
    "        loss.backward()\n",
    "\n",
    "        if global_step % CONFIG[\"gradient_accumulation_steps\"] == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.5)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item() * CONFIG[\"gradient_accumulation_steps\"]\n",
    "\n",
    "        if global_step % CONFIG[\"print_every_steps\"] == 0:\n",
    "            avg_loss = running_loss / (global_step if global_step>0 else 1)\n",
    "            print(f\"[Epoch {epoch+1}] Step {global_step} | loss: {avg_loss:.4f} | step_time: {format_time(time.time()-epoch_start)}\")\n",
    "\n",
    "    epoch_time = format_time(time.time() - epoch_start)\n",
    "    print(f\"Epoch {epoch+1} finished in {epoch_time}\")\n",
    "\n",
    "# -------------------------\n",
    "# 7. 保存微调后模型（包含 LoRA）\n",
    "# -------------------------\n",
    "print(\"Saving finetuned model to:\", CONFIG[\"output_dir\"])\n",
    "model.save_pretrained(CONFIG[\"output_dir\"])\n",
    "tokenizer.save_pretrained(CONFIG[\"output_dir\"])\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e24ff8b-1476-4576-b3a8-cd082a9dcc12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#加要求的意识流风格微调优化方向（LoRA 升级、损失函数扩展、训练参数调整）进行适配\n",
    "# finetune_mindflow_ulysses.py\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, PreTrainedTokenizerFast\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG - 意识流风格微调优化配置（修复LoRA目标模块）\n",
    "# -------------------------\n",
    "CONFIG = {\n",
    "    \"base_model_path\": \"/root/autodl-tmp/Pro/qwen2.5-sft-full\",   # 模型一：已微调的诗歌模型（基座）\n",
    "    \"ulysses_txt\": \"/root/autodl-tmp/Pro/Ulysses尤利西斯.txt\",    # 尤利西斯原文 txt（本地）\n",
    "    \"output_dir\": \"/root/autodl-tmp/Pro/qwen2.5-mindflow-ulysses\",# 输出模型（二）\n",
    "    \"max_len\": 1024,      # 单个样本最大 token 长度（可调：1024/2048）\n",
    "    \"stride\": 512,        # 滑动窗口重叠\n",
    "    \"batch_size\": 1,      # per device batch size（通常 1 或 2）\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"learning_rate\": 1e-5,  # 优化：降至1e-5（慢学习，精准拟合风格）\n",
    "    \"num_train_epochs\": 5,   # 优化：增至5轮\n",
    "    \"fp16\": True,\n",
    "    \"lora_r\": 16,            # 优化：提升至16（增强风格拟合）\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.01,\n",
    "    # 修复：Qwen2.5的MLP层具体Linear子层名称（替换原mlp）\n",
    "    \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \n",
    "                       \"gate_proj\", \"up_proj\", \"down_proj\"],  \n",
    "    \"print_every_steps\": 20,\n",
    "    \"seed\": 42,\n",
    "    # 新增：多样性损失&句式风格损失参数\n",
    "    \"diversity_alpha\": 0.1,  # 多样性损失α提升至0.1\n",
    "    \"style_beta\": 0.05,      # 句式风格损失β系数\n",
    "    \"eps\": 1e-6,             # 数值稳定参数\n",
    "    \"max_norm\": 1.5,         # 梯度裁剪阈值\n",
    "    # 同义词替换参数（保留NLTK wordnet能力）\n",
    "    \"augment_prob\": 0.5      # 50%概率进行同义词替换增强\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(CONFIG[\"seed\"])\n",
    "random.seed(CONFIG[\"seed\"])\n",
    "\n",
    "# -------------------------\n",
    "# 仅下载NLTK wordnet（同义词替换用），不下载punkt\n",
    "# -------------------------\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "except LookupError:\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# -------------------------\n",
    "# 保留：英文文本同义词替换增强（基于NLTK wordnet）\n",
    "# -------------------------\n",
    "def augment_english_text(text):\n",
    "    \"\"\"基于NLTK wordnet的同义词替换，保留语义同时提升多样性\"\"\"\n",
    "    if not text or len(text.split()) < 3:\n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        # 词性标注，仅替换名词/动词/形容词\n",
    "        pos_tags = nltk.pos_tag(text.split())\n",
    "        augmented_tokens = []\n",
    "        for token, pos in pos_tags:\n",
    "            if pos.startswith('N') or pos.startswith('V') or pos.startswith('J'):\n",
    "                synonyms = wordnet.synsets(token)\n",
    "                if synonyms and len(synonyms) > 0:\n",
    "                    synonym_lemma = synonyms[0].lemmas()[0].name()\n",
    "                    # 严格校验替换词\n",
    "                    if (synonym_lemma != token and \n",
    "                        len(synonym_lemma) <= len(token)+2 and\n",
    "                        not synonym_lemma.isdigit()):\n",
    "                        augmented_tokens.append(synonym_lemma.replace('_', ' '))\n",
    "                    else:\n",
    "                        augmented_tokens.append(token)\n",
    "                else:\n",
    "                    augmented_tokens.append(token)\n",
    "            else:\n",
    "                augmented_tokens.append(token)\n",
    "        return ' '.join(augmented_tokens)\n",
    "    except:\n",
    "        return text  # 增强失败时返回原文本\n",
    "\n",
    "# -------------------------\n",
    "# 1. 加载 tokenizer（Qwen2 正确范式）\n",
    "# -------------------------\n",
    "print(\"Loading tokenizer from\", CONFIG[\"base_model_path\"])\n",
    "\n",
    "tokenizer_path = os.path.join(CONFIG[\"base_model_path\"], \"tokenizer.json\")\n",
    "if os.path.exists(tokenizer_path):\n",
    "    tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_path)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"base_model_path\"], trust_remote_code=True)\n",
    "\n",
    "# ======【Qwen2 推荐标准设置】======\n",
    "# 1) 明确设置 eos / bos（大多数 qwen2 tokenizer.json 自带，但确保一致）\n",
    "tokenizer.eos_token = \"<|endoftext|>\"\n",
    "tokenizer.bos_token = \"<|startoftext|>\"\n",
    "\n",
    "# 2) 添加 im_start / im_end\n",
    "# 注意：这一步会重建 vocab → pad_token 会丢失\n",
    "tokenizer.add_special_tokens({\n",
    "    \"additional_special_tokens\": [\"<|im_start|>\", \"<|im_end|>\"]\n",
    "})\n",
    "\n",
    "# 3) ★ 关键：add_special_tokens 后必须重新设置 pad_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Tokenizer loaded.\")\n",
    "print(\"  eos_token:\", tokenizer.eos_token)\n",
    "print(\"  bos_token:\", tokenizer.bos_token)\n",
    "print(\"  pad_token:\", tokenizer.pad_token)\n",
    "print(\"  pad_token_id:\", tokenizer.pad_token_id)\n",
    "print(\"  vocab_size:\", len(tokenizer))\n",
    "# =================================\n",
    "\n",
    "# -------------------------\n",
    "# 2. 从 Ulysses txt 生成 token chunks（滑动窗口 + 文本增强）\n",
    "# -------------------------\n",
    "def load_and_tokenize_txt(txt_path, tokenizer, max_len, stride):\n",
    "    print(\"Reading Ulysses txt:\", txt_path)\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = f.read()\n",
    "\n",
    "    # 简单按段落拆分（保留语义完整性），段落间保留换行\n",
    "    paragraphs = [p.strip() for p in raw.split(\"\\n\\n\") if p.strip()]\n",
    "    print(f\"Found {len(paragraphs)} paragraphs; will pack paragraphs into chunks <= {max_len} tokens.\")\n",
    "\n",
    "    all_chunks = []\n",
    "    cur_ids = []\n",
    "    cur_chars = []\n",
    "\n",
    "    for para in paragraphs:\n",
    "        # 对段落进行同义词替换增强（保留意识流风格）\n",
    "        if random.random() < CONFIG[\"augment_prob\"]:\n",
    "            para = augment_english_text(para)\n",
    "        \n",
    "        para_ids = tokenizer(para, add_special_tokens=False)[\"input_ids\"]\n",
    "        if len(para_ids) > max_len:\n",
    "            # 段落本身超长：按 token 窗口切分\n",
    "            start = 0\n",
    "            while start < len(para_ids):\n",
    "                end = min(start + max_len, len(para_ids))\n",
    "                chunk_ids = para_ids[start:end]\n",
    "                chunk_text = tokenizer.decode(chunk_ids, skip_special_tokens=True)\n",
    "                all_chunks.append(chunk_text)\n",
    "                if end == len(para_ids):\n",
    "                    break\n",
    "                start += max_len - stride\n",
    "            continue\n",
    "\n",
    "        # 若加入当前 chunk 后不超长则合并\n",
    "        if len(cur_ids) + len(para_ids) <= max_len:\n",
    "            cur_ids.extend(para_ids)\n",
    "            cur_chars.append(para)\n",
    "        else:\n",
    "            # flush current\n",
    "            if cur_ids:\n",
    "                all_chunks.append(tokenizer.decode(cur_ids, skip_special_tokens=True))\n",
    "            # start new\n",
    "            cur_ids = para_ids.copy()\n",
    "            cur_chars = [para]\n",
    "\n",
    "    # flush tail\n",
    "    if cur_ids:\n",
    "        all_chunks.append(tokenizer.decode(cur_ids, skip_special_tokens=True))\n",
    "\n",
    "    # 进一步应用滑动窗口以增加多样性（可选）\n",
    "    final_chunks = []\n",
    "    for chunk in all_chunks:\n",
    "        ids = tokenizer(chunk, add_special_tokens=False)[\"input_ids\"]\n",
    "        start = 0\n",
    "        while start < len(ids):\n",
    "            end = min(start + max_len, len(ids))\n",
    "            part = tokenizer.decode(ids[start:end], skip_special_tokens=True)\n",
    "            final_chunks.append(part)\n",
    "            if end == len(ids):\n",
    "                break\n",
    "            start += max_len - stride\n",
    "\n",
    "    print(f\"Total token chunks prepared: {len(final_chunks)}\")\n",
    "    return final_chunks\n",
    "\n",
    "chunks = load_and_tokenize_txt(CONFIG[\"ulysses_txt\"], tokenizer, CONFIG[\"max_len\"], CONFIG[\"stride\"])\n",
    "\n",
    "# -------------------------\n",
    "# 调整：句子长度分析工具（无需punkt，基于标点/空格分词）\n",
    "# -------------------------\n",
    "def analyze_sentence_length(text):\n",
    "    \"\"\"\n",
    "    分析文本中的短句占比（无需punkt）\n",
    "    短句定义：≤5个单词的文本片段（按 . ! ? ; , 分割）\n",
    "    \"\"\"\n",
    "    # 清理特殊符号，保留核心文本\n",
    "    clean_text = re.sub(r'<\\|.*?\\|>', '', text)\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
    "    \n",
    "    # 按标点分割文本片段（模拟分句，无需punkt）\n",
    "    fragments = re.split(r'[.!?;,]', clean_text)\n",
    "    fragments = [f.strip() for f in fragments if f.strip()]\n",
    "    total_fragments = len(fragments)\n",
    "    \n",
    "    if total_fragments == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # 统计短句数量\n",
    "    short_frag_count = 0\n",
    "    for frag in fragments:\n",
    "        # 分词（仅保留单词）\n",
    "        words = re.findall(r'\\w+', frag)\n",
    "        if len(words) <= 5:\n",
    "            short_frag_count += 1\n",
    "    \n",
    "    # 返回短句占比\n",
    "    return short_frag_count / total_fragments\n",
    "\n",
    "# -------------------------\n",
    "# 新增：损失函数扩展（多样性损失 + 句式风格损失）\n",
    "# -------------------------\n",
    "def diversity_loss(logits, alpha=CONFIG[\"diversity_alpha\"]):\n",
    "    \"\"\"多样性损失：惩罚低熵预测，增强生成多样性\"\"\"\n",
    "    # 转换为fp32计算，避免数值溢出\n",
    "    logits = logits.float()\n",
    "    \n",
    "    # 计算softmax概率分布\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    # 限制概率范围，防止log(0)\n",
    "    probs = torch.clamp(probs, min=CONFIG[\"eps\"], max=1.0 - CONFIG[\"eps\"])\n",
    "    \n",
    "    # 计算熵值\n",
    "    entropy = -torch.sum(probs * torch.log(probs), dim=-1)\n",
    "    # 最大熵（词汇表大小的对数）\n",
    "    vocab_size = probs.size(-1)\n",
    "    max_entropy = torch.log(torch.tensor(vocab_size, dtype=torch.float32, device=logits.device))\n",
    "    # 归一化熵值\n",
    "    entropy_norm = entropy / (max_entropy + CONFIG[\"eps\"])\n",
    "    entropy_norm = torch.clamp(entropy_norm, min=0.0, max=1.0)\n",
    "    \n",
    "    # 惩罚低熵值（保守预测）\n",
    "    penalty = alpha * (1 - entropy_norm)\n",
    "    penalty = torch.nan_to_num(penalty, nan=0.0, posinf=alpha, neginf=0.0)\n",
    "    \n",
    "    return torch.mean(penalty)\n",
    "\n",
    "def style_loss(generated_texts, beta=CONFIG[\"style_beta\"]):\n",
    "    \"\"\"句式风格损失：惩罚长句占比过高，强化碎片化\"\"\"\n",
    "    total_style_penalty = 0.0\n",
    "    valid_text_count = 0\n",
    "    \n",
    "    for text in generated_texts:\n",
    "        short_frag_ratio = analyze_sentence_length(text)\n",
    "        # 惩罚项：1 - 短句占比（短句越少，惩罚越高）\n",
    "        penalty = 1 - short_frag_ratio\n",
    "        total_style_penalty += penalty\n",
    "        valid_text_count += 1\n",
    "    \n",
    "    if valid_text_count == 0:\n",
    "        return torch.tensor(0.0, device=device)\n",
    "    \n",
    "    # 计算平均惩罚并乘以beta系数\n",
    "    avg_penalty = total_style_penalty / valid_text_count\n",
    "    return beta * torch.tensor(avg_penalty, device=device, dtype=torch.float32)\n",
    "\n",
    "# -------------------------\n",
    "# 3. 构造 Dataset（对话格式 + prefix-only mask）\n",
    "# -------------------------\n",
    "class UlyssesDataset(Dataset):\n",
    "    def __init__(self, chunks, tokenizer, max_len):\n",
    "        self.examples = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "        for idx, chunk_text in enumerate(chunks):\n",
    "            # user instruction: 要求生成意识流风格的段落（不抄袭原文）\n",
    "            user_instr = (\n",
    "                \"Write an original paragraph in James Joyce's stream-of-consciousness style \"\n",
    "                \"that captures similar imagery and tone as the following excerpt. \"\n",
    "                \"Do not copy exact phrases; be original.\\n\\n\"\n",
    "                f\"Excerpt:\\n{chunk_text}\"\n",
    "            )\n",
    "\n",
    "            # 目标文本（可替换为增强后的文本）\n",
    "            assistant_target = chunk_text\n",
    "\n",
    "            full = f\"<|im_start|>user\\n{user_instr}<|im_end|>\\n<|im_start|>assistant\\n{assistant_target}<|im_end|>\"\n",
    "            enc = tokenizer(full, max_length=self.max_len, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "            input_ids = enc[\"input_ids\"].squeeze(0)\n",
    "            attention_mask = enc[\"attention_mask\"].squeeze(0)\n",
    "            labels = input_ids.clone()\n",
    "\n",
    "            # 定位assistant部分起始位置，mask掉user部分的loss\n",
    "            special_id = tokenizer.convert_tokens_to_ids(\"<|im_start|>\")\n",
    "            ids_list = input_ids.tolist()\n",
    "            assistant_pos = 0\n",
    "            try:\n",
    "                # 找到第二个<|im_start|>（第一个是user）\n",
    "                first = ids_list.index(special_id)\n",
    "                second = ids_list.index(special_id, first + 1)\n",
    "                assistant_pos = second\n",
    "            except ValueError:\n",
    "                assistant_pos = 0\n",
    "\n",
    "            # mask prefix（user + instruction）so loss is only computed on assistant part\n",
    "            labels[:assistant_pos] = -100\n",
    "\n",
    "            self.examples.append({\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"labels\": labels,\n",
    "                \"assistant_text\": assistant_target  # 保存目标文本用于风格损失计算\n",
    "            })\n",
    "\n",
    "        print(f\"Dataset built: {len(self.examples)} examples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "dataset = UlyssesDataset(chunks, tokenizer, CONFIG[\"max_len\"])\n",
    "dataloader = DataLoader(dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True, num_workers=0, drop_last=True)\n",
    "\n",
    "# -------------------------\n",
    "# 4. 加载基座模型并装上 LoRA（修复配置）\n",
    "# -------------------------\n",
    "print(\"Loading base model from:\", CONFIG[\"base_model_path\"])\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"base_model_path\"],\n",
    "    torch_dtype=torch.float16 if CONFIG[\"fp16\"] else torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True  # 关键：Qwen2需要trust_remote_code\n",
    ")\n",
    "\n",
    "# 修复：LoRA配置（目标模块为Qwen2.5的具体Linear层）\n",
    "lora_cfg = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    target_modules=CONFIG[\"target_modules\"],  # q_proj/v_proj/k_proj/o_proj + MLP的Linear层\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    bias=\"none\",  # 避免bias训练导致的数值问题\n",
    "    modules_to_save=[],  # 不额外保存模块\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()  # 打印可训练参数占比\n",
    "model.train()\n",
    "\n",
    "# 获取模型设备\n",
    "model_device = next(model.parameters()).device\n",
    "print(\"Model device:\", model_device)\n",
    "\n",
    "# -------------------------\n",
    "# 5. optimizer & loss\n",
    "# -------------------------\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "ce_loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# -------------------------\n",
    "# 6. 训练循环（集成新损失函数）\n",
    "# -------------------------\n",
    "def format_time(seconds):\n",
    "    m, s = divmod(int(seconds), 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "\n",
    "total_steps = len(dataloader) * CONFIG[\"num_train_epochs\"] // CONFIG[\"gradient_accumulation_steps\"]\n",
    "print(\"Training config -- epochs:\", CONFIG[\"num_train_epochs\"], \"batches_per_epoch:\", len(dataloader), \"total_steps(approx):\", total_steps)\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(CONFIG[\"num_train_epochs\"]):\n",
    "    epoch_start = time.time()\n",
    "    running_loss = 0.0\n",
    "    running_ce_loss = 0.0\n",
    "    running_div_loss = 0.0\n",
    "    running_style_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        global_step += 1\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(model_device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(model_device)\n",
    "        labels = batch[\"labels\"].to(model_device)\n",
    "        assistant_texts = batch[\"assistant_text\"]  # 目标文本用于风格损失计算\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        ce_loss = ce_loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        \n",
    "        # 计算多样性损失\n",
    "        div_loss = diversity_loss(logits.view(-1, logits.size(-1)))\n",
    "        \n",
    "        # 计算句式风格损失\n",
    "        style_loss_val = style_loss(assistant_texts)\n",
    "        \n",
    "        # 总损失：交叉熵 + 多样性损失 + 风格损失\n",
    "        total_loss = (ce_loss + div_loss + style_loss_val) / CONFIG[\"gradient_accumulation_steps\"]\n",
    "        total_loss.backward()\n",
    "\n",
    "        # 梯度裁剪 + 优化器更新\n",
    "        if global_step % CONFIG[\"gradient_accumulation_steps\"] == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG[\"max_norm\"])\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # 累计损失\n",
    "        running_loss += total_loss.item() * CONFIG[\"gradient_accumulation_steps\"]\n",
    "        running_ce_loss += ce_loss.item()\n",
    "        running_div_loss += div_loss.item()\n",
    "        running_style_loss += style_loss_val.item()\n",
    "\n",
    "        # 打印训练日志\n",
    "        if global_step % CONFIG[\"print_every_steps\"] == 0:\n",
    "            avg_loss = running_loss / (global_step if global_step>0 else 1)\n",
    "            avg_ce = running_ce_loss / (global_step if global_step>0 else 1)\n",
    "            avg_div = running_div_loss / (global_step if global_step>0 else 1)\n",
    "            avg_style = running_style_loss / (global_step if global_step>0 else 1)\n",
    "            print(f\"[Epoch {epoch+1}] Step {global_step} | total_loss: {avg_loss:.4f} | ce_loss: {avg_ce:.4f} | div_loss: {avg_div:.4f} | style_loss: {avg_style:.4f} | time: {format_time(time.time()-epoch_start)}\")\n",
    "\n",
    "    epoch_time = format_time(time.time() - epoch_start)\n",
    "    print(f\"Epoch {epoch+1} finished in {epoch_time} | avg_epoch_loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 7. 保存微调后模型（包含 LoRA）\n",
    "# -------------------------\n",
    "print(\"Saving finetuned model to:\", CONFIG[\"output_dir\"])\n",
    "model.save_pretrained(CONFIG[\"output_dir\"])\n",
    "tokenizer.save_pretrained(CONFIG[\"output_dir\"])\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438f5744-afb9-435b-890c-b155ae9e7d93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# ====================== 核心配置 ======================\n",
    "BASE_MODEL_PATH = \"/root/autodl-tmp/Pro/qwen2.5-sft-full\"\n",
    "LORA_VER2_PATH = \"/root/autodl-tmp/Pro/qwen2.5-mindflow-ulysses-ver2\"\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # 强制指定cuda:0\n",
    "\n",
    "# ====================== 加载tokenizer ======================\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ====================== 加载模型（强制单设备） ======================\n",
    "# 加载基座模型（关闭auto设备映射，强制指定DEVICE）\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": DEVICE},  # 强制所有层都在指定设备\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 加载LoRA并移到指定设备\n",
    "model = PeftModel.from_pretrained(model, LORA_VER2_PATH)\n",
    "model = model.to(DEVICE)  # 强制模型整体移到DEVICE\n",
    "model.eval()\n",
    "\n",
    "# ====================== 生成函数 ======================\n",
    "def generate_poem(prompt):\n",
    "    # 封装prompt\n",
    "    input_text = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    \n",
    "    # 编码（强制移到DEVICE）\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(DEVICE)  # 关键：输入张量移到DEVICE\n",
    "    \n",
    "    # 生成\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,  # 直接传字典，包含input_ids和attention_mask\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.0,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=True,\n",
    "            device=DEVICE  # 显式指定生成设备\n",
    "        )\n",
    "    \n",
    "    # 解析结果\n",
    "    poem = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    poem = poem.split(\"<|im_start|>assistant\\n\")[-1].strip()\n",
    "    return poem\n",
    "\n",
    "# ====================== 测试 ======================\n",
    "if __name__ == \"__main__\":\n",
    "    result = generate_poem(\"写一首诗\")\n",
    "    print(\"=== 生成的诗歌 ===\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0832566-8571-4c51-ab21-7f1b3c983d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "================================================================================\n",
      "Test Prompt:\n",
      "write a poem in the style of virginia wolf, with keyword: willow, wind, fluffy, charmed.\n",
      "================================================================================\n",
      "\n",
      "[Step 1/3] Loading Original Poetry Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step 2/3] Loading Mindflow Enhanced Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.25it/s]\n",
      "/root/miniconda3/envs/qwen_text/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step 3/3] Generating Poems...\n",
      "\n",
      "=== 1. Original Poetry Model Output ===\n",
      "All the world was green, and so was she. She was all wind, and wind was all her hair.\n",
      "The wind blew her dress about like the sails of a ship, and it was all of willow. She went, she went, she was all fluff. She was all fluff, and all of willow. She had no heart, she was all fluff. She was all fluff, she had no heart. She had no heart, and she was all fluff. She was all fluff, she had no heart. She had no heart, and she was all fluff. She was all fluff, she had no heart. She had no heart, she was all fluff. She was all fluff, she had no heart.\n",
      "\n",
      "=== 2. Mindflow Enhanced Model Output ===\n",
      "In a garden where the willow weeps and wails,\n",
      "Under skies that weave their silvery trails,\n",
      "The wind, a witch, does softly play,\n",
      "With leaves and boughs that dance away.\n",
      "\n",
      "A fluffy cloud, like cotton in the breeze,\n",
      "Drifts over willow, soft as eves,\n",
      "In a charmed garden, far from madding crowd,\n",
      "Where shadows lie, and dreams are loud.\n",
      "\n",
      "The willow whispers, \"Listen, hark!\"\n",
      "To secrets shared by wind and bark,\n",
      "And as the gusts with gentle force do blow,\n",
      "They dance and tumble, in a playful flow.\n",
      "\n",
      "In twilight's lap, the willow stands,\n",
      "A guardian of the magic lands,\n",
      "Charmed by whispers and the wind's soft breath,\n",
      "In gardens woven from leaf and death.\n",
      "\n",
      "So, sit you down, in the willow's shade,\n",
      "Beneath the spell of night's own shade,\n",
      "And listen to the wind's sweet song,\n",
      "Charmed by nature, charmed by long.\n"
     ]
    }
   ],
   "source": [
    "# inference_mindflow.py\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# -------------------------\n",
    "# 1. 配置路径（新增原始诗歌LoRA路径）\n",
    "# -------------------------\n",
    "BASE_MODEL = \"/root/autodl-tmp/Pro/qwen2.5-sft-full\"               # 基础诗歌微调基座（模型一）\n",
    "LORA_POETRY = \"\"  # 【必填】原始诗歌LoRA适配器路径（若模型一是全量微调则留空）\n",
    "LORA_MINDFLOW = \"/root/autodl-tmp/Pro/qwen2.5-mindflow-ulysses\"  # 意识流增强LoRA（模型二）\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "GEN_CFG = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"temperature\": 0.85,\n",
    "    \"top_k\": 40,\n",
    "    \"top_p\": 0.92,\n",
    "    \"repetition_penalty\": 1.02,\n",
    "    \"do_sample\": True,  # 确保采样生成，增强多样性\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# 2. 加载通用tokenizer\n",
    "# -------------------------\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 必须设置 pad_token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# -------------------------\n",
    "# 3. 模型加载工具函数（复用基座，切换LoRA）\n",
    "# -------------------------\n",
    "def load_model_with_lora(base_model_path, lora_path=None):\n",
    "    \"\"\"\n",
    "    加载基座模型 + 指定LoRA适配器\n",
    "    :param base_model_path: 基座模型路径\n",
    "    :param lora_path: LoRA适配器路径（None则仅加载基座）\n",
    "    :return: 加载完成的模型\n",
    "    \"\"\"\n",
    "    # 加载基座模型\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_path,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # 加载LoRA（若指定）\n",
    "    if lora_path and lora_path.strip() != \"\":\n",
    "        model = PeftModel.from_pretrained(model, lora_path)\n",
    "    \n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# 4. 统一推理函数\n",
    "# -------------------------\n",
    "def generate_poem(model, tokenizer, prompt: str, gen_cfg: dict):\n",
    "    \"\"\"\n",
    "    统一生成函数，适配任意模型\n",
    "    \"\"\"\n",
    "    # 保持和训练一致的对话格式\n",
    "    formatted = (\n",
    "        f\"<|im_start|>user\\n{prompt}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n\"\n",
    "    )\n",
    "\n",
    "    input_ids = tokenizer(\n",
    "        formatted,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            **gen_cfg\n",
    "        )\n",
    "\n",
    "    # 清理生成结果\n",
    "    text = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "    if \"<|im_start|>assistant\\n\" in text:\n",
    "        text = text.split(\"<|im_start|>assistant\\n\")[1]\n",
    "    text = text.replace(\"<|im_end|>\", \"\").replace(\"<|endoftext|>\", \"\").strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# -------------------------\n",
    "# 5. 主推理流程（双模型对比）\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 测试Prompt\n",
    "    test_prompt = (\n",
    "        \"write a poem in the style of virginia wolf, with keyword: willow, wind, fluffy, charmed.\"\n",
    "    )\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"Test Prompt:\\n{test_prompt}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # 加载模型1：原始诗歌微调模型\n",
    "    print(\"\\n[Step 1/3] Loading Original Poetry Model...\")\n",
    "    model_poetry = load_model_with_lora(BASE_MODEL, LORA_POETRY)\n",
    "    \n",
    "    # 加载模型2：意识流增强微调模型\n",
    "    print(\"\\n[Step 2/3] Loading Mindflow Enhanced Model...\")\n",
    "    model_mindflow = load_model_with_lora(BASE_MODEL, LORA_MINDFLOW)\n",
    "\n",
    "    # 生成诗歌（双模型）\n",
    "    print(\"\\n[Step 3/3] Generating Poems...\")\n",
    "    print(\"\\n=== 1. Original Poetry Model Output ===\")\n",
    "    poem_poetry = generate_poem(model_poetry, tokenizer, test_prompt, GEN_CFG)\n",
    "    print(poem_poetry)\n",
    "\n",
    "    print(\"\\n=== 2. Mindflow Enhanced Model Output ===\")\n",
    "    poem_mindflow = generate_poem(model_mindflow, tokenizer, test_prompt, GEN_CFG)\n",
    "    print(poem_mindflow)\n",
    "\n",
    "    # 释放显存（可选）\n",
    "    del model_poetry, model_mindflow\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd187bbd-dfb7-420e-820c-da9fa02283f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/qwen_text/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA adapter...\n",
      "\n",
      "🧠 Mindflow Poetry Generator\n",
      "输入 prompt 回车生成\n",
      "输入 reset 清空记忆\n",
      "输入 exit 退出并保存终版诗歌\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/qwen_text/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>  write a poem in the style of james joyce with theme: nostalgic, somber\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generated ===\n",
      "\n",
      "In bygone days when life was fair,\n",
      "I wandered through a land so rare,\n",
      "Where every stone and every tree,\n",
      "Was tinged with memories to be.\n",
      "\n",
      "The sun that shone upon my face,\n",
      "No more shall gladden me in place.\n",
      "For time doth run with unrelenting pace,\n",
      "And all things fade into the space.\n",
      "\n",
      "Alas, those streets I used to roam,\n",
      "Now hold no echoes from the home.\n",
      "The laughter's gone, the song is dead,\n",
      "A ghostly presence now it's led.\n",
      "\n",
      "My thoughts they wander far away,\n",
      "To places where love did play.\n",
      "Those golden hours that seemed to stretch,\n",
      "Have slipped away like water in a ditch.\n",
      "\n",
      "Oh, Dublin town, thou city fair!\n",
      "How bitter tastes thy memory there.\n",
      "Once vibrant, filled with hope and dreams,\n",
      "Now shadows loom o'er all your streams.\n",
      "\n",
      "The fields we trod, the paths we trode,\n",
      "Are lost forever from the sod.\n",
      "Our joys, our sorrows, what remained?\n",
      "But not within these walls again.\n",
      "\n",
      "I search for peace amidst this strife,\n",
      "Yet find but ruins, broken life.\n",
      "Nostalgic heart that longs to see,\n",
      "What vanished long ago in glee. \n",
      "\n",
      "Thus sorrowfully I pen these lines,\n",
      "To capture all I cannot divine.\n",
      "The past, though fleeting, still does haunt,\n",
      "With bittersweet recollections at my cost.\n",
      "\n",
      "=================\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 终版诗歌已保存至：./poetry_results/final_poem_20251217_192004.txt\n",
      "Bye.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# -------------------------\n",
    "# 1. 配置\n",
    "# -------------------------\n",
    "BASE_MODEL = \"/root/autodl-tmp/Pro/qwen2.5-sft-full\"\n",
    "LORA_MODEL = \"/root/autodl-tmp/Pro/qwen2.5-mindflow-ulysses\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "GEN_CFG = {\n",
    "    \"max_new_tokens\": 400,\n",
    "    \"temperature\": 0.85,\n",
    "    \"top_k\": 40,\n",
    "    \"top_p\": 0.92,\n",
    "    \"repetition_penalty\": 1.12,\n",
    "}\n",
    "\n",
    "# 新增：保存配置\n",
    "SAVE_FOLDER = \"./poetry_results\"  # 保存目录\n",
    "os.makedirs(SAVE_FOLDER, exist_ok=True)  # 自动创建目录（不存在时）\n",
    "\n",
    "# -------------------------\n",
    "# 2. 加载 tokenizer\n",
    "# -------------------------\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# -------------------------\n",
    "# 3. 加载模型 + LoRA\n",
    "# -------------------------\n",
    "print(\"Loading base model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"Loading LoRA adapter...\")\n",
    "model = PeftModel.from_pretrained(model, LORA_MODEL)\n",
    "model.eval()\n",
    "\n",
    "# -------------------------\n",
    "# 4. 核心：带“记忆”的生成\n",
    "# -------------------------\n",
    "conversation_memory = \"\"   # ⭐ 存上一轮生成结果\n",
    "\n",
    "\n",
    "def generate_with_memory(user_prompt: str):\n",
    "    global conversation_memory\n",
    "\n",
    "    if conversation_memory:\n",
    "        # 在已有诗歌基础上 refinement\n",
    "        formatted = (\n",
    "            \"<|im_start|>user\\n\"\n",
    "            \"Here is the previous poem:\\n\"\n",
    "            f\"{conversation_memory}\\n\\n\"\n",
    "            \"Please revise or continue it according to the following instruction:\\n\"\n",
    "            f\"{user_prompt}\"\n",
    "            \"<|im_end|>\\n\"\n",
    "            \"<|im_start|>assistant\\n\"\n",
    "        )\n",
    "    else:\n",
    "        # 第一轮\n",
    "        formatted = (\n",
    "            f\"<|im_start|>user\\n{user_prompt}<|im_end|>\\n\"\n",
    "            f\"<|im_start|>assistant\\n\"\n",
    "        )\n",
    "\n",
    "    input_ids = tokenizer(formatted, return_tensors=\"pt\").input_ids.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            **GEN_CFG\n",
    "        )\n",
    "\n",
    "    text = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "\n",
    "    # 清理输出\n",
    "    if \"<|im_start|>assistant\\n\" in text:\n",
    "        text = text.split(\"<|im_start|>assistant\\n\")[1]\n",
    "    text = text.replace(\"<|im_end|>\", \"\").replace(\"<|endoftext|>\", \"\").strip()\n",
    "\n",
    "    # ⭐ 更新记忆\n",
    "    conversation_memory = text\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 5. 交互式输入\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n🧠 Mindflow Poetry Generator\")\n",
    "    print(\"输入 prompt 回车生成\")\n",
    "    print(\"输入 reset 清空记忆\")\n",
    "    print(\"输入 exit 退出并保存终版诗歌\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\">>> \").strip()\n",
    "\n",
    "        if user_input.lower() == \"exit\":\n",
    "            # 保存终版诗歌到txt文件\n",
    "            if conversation_memory:\n",
    "                # 生成带时间戳的文件名，避免覆盖\n",
    "                timestamp = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "                file_path = os.path.join(SAVE_FOLDER, f\"final_poem_{timestamp}.txt\")\n",
    "                \n",
    "                # 写入文件（UTF-8编码防止中文乱码）\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(\"=== Mindflow Poetry Final Version ===\\n\")\n",
    "                    f.write(f\"Generated Time: {time.ctime()}\\n\")\n",
    "                    f.write(\"=\" * 40 + \"\\n\\n\")\n",
    "                    f.write(conversation_memory)\n",
    "                \n",
    "                print(f\"\\n✅ 终版诗歌已保存至：{file_path}\")\n",
    "            else:\n",
    "                print(\"\\n⚠️  暂无生成的诗歌内容，未保存文件\")\n",
    "            \n",
    "            print(\"Bye.\")\n",
    "            break\n",
    "\n",
    "        if user_input.lower() == \"reset\":\n",
    "            conversation_memory = \"\"\n",
    "            print(\"🔄 Memory cleared.\\n\")\n",
    "            continue\n",
    "\n",
    "        output = generate_with_memory(user_input)\n",
    "\n",
    "        print(\"\\n=== Generated ===\\n\")\n",
    "        print(output)\n",
    "        print(\"\\n=================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f7349fb-842d-41aa-86e4-5ec97a2497ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "\n",
      "Loading BASE model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SFT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MINDFLOW model (base + LoRA)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ All models loaded successfully!\n",
      "\n",
      "\n",
      "🧠 Mindflow Poetry Generator - Multi-Model Mode\n",
      "======================================================================\n",
      "指令说明：\n",
      "  1. 直接输入文本      - 同时调用BASE/SFT/MINDFLOW三个模型生成诗歌\n",
      "  2. reset             - 清空所有模型的对话记忆\n",
      "  3. exit              - 退出并保存所有模型的终版诗歌\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/qwen_text/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>  Write a poem that flows like a stream of thoughts—start with the image of a rainy window, then let your mind wander to memories of childhood, the smell of fresh bread, and the sound of distant thunder. Keep the connection natural. Limit to 8 lines.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "🔄 Generating with BASE model...\n",
      "🔄 Generating with SFT model...\n",
      "🔄 Generating with MINDFLOW model...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "📝 BASE MODEL OUTPUT:\n",
      "\n",
      "Rain patters on the glass, a lullaby,\n",
      "Memories swim in soft afternoons gone by.\n",
      "Childhood summers, warm as honeyed toast,\n",
      "Baking daydreams beneath the wide, old oak.\n",
      "\n",
      "Thunder rumbles far away, a sleepy drum,\n",
      "Familiar scents of yeast rise up from somewhere.\n",
      "Mom's voice, gentle and kind, through time it calls,\n",
      "Echoes of laughter mingling with the rain's small fall.\n",
      "\n",
      "================================================================================\n",
      "📝 SFT MODEL OUTPUT:\n",
      "\n",
      "The rain makes a little drum on the windowpane   \n",
      "And I recall my sister's voice in the kitchen,\n",
      "When I was eight years old and she made me a slice   \n",
      "Of sweetbread, her way. The smell of it came through   \n",
      "To the front room where I sat at the piano playing Schubert.\n",
      "A song called \"Willkommen\" and as I played it the rain   \n",
      "Beat down hard and long and the thunder roared outside,\n",
      "So that the windowpanes shook and trembled inside.\n",
      "\n",
      "================================================================================\n",
      "📝 MINDFLOW MODEL OUTPUT:\n",
      "\n",
      "Rainy window pane, my gaze adrift,\n",
      "Childhood's echoes whisper in the mist.\n",
      "Muddy boots, forgotten toys below,\n",
      "Freshly baked, scent fills the air aglow.\n",
      "Thunder rolls, a far-off drumming beat,\n",
      "Leaves rustle soft where raindrops meet.\n",
      "Thoughts meander like the river's flow,\n",
      "Past, present blend, forever grow.\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 所有模型的终版诗歌已保存至：./poetry_results/all_models_poem_20251227_115912.txt\n",
      "\n",
      "👋 Bye!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# -------------------------\n",
    "# 1. 配置\n",
    "# -------------------------\n",
    "# 模型路径配置\n",
    "MODEL_PATHS = {\n",
    "    \"base\": \"/root/autodl-tmp/qwen2.5-7b/qwen2.5-7b\",\n",
    "    \"sft\": \"/root/autodl-tmp/Pro/qwen2.5-sft-full\",\n",
    "    \"mindflow\": {\n",
    "        \"base\": \"/root/autodl-tmp/Pro/qwen2.5-sft-full\",\n",
    "        \"lora\": \"/root/autodl-tmp/Pro/qwen2.5-mindflow-ulysses\"\n",
    "    }\n",
    "}\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "GEN_CFG = {\n",
    "    \"max_new_tokens\": 400,\n",
    "    \"temperature\": 0.85,\n",
    "    \"top_k\": 40,\n",
    "    \"top_p\": 0.92,\n",
    "    \"repetition_penalty\": 1.12,\n",
    "}\n",
    "\n",
    "# 新增：保存配置\n",
    "SAVE_FOLDER = \"./poetry_results\"  # 保存目录\n",
    "os.makedirs(SAVE_FOLDER, exist_ok=True)  # 自动创建目录（不存在时）\n",
    "\n",
    "# -------------------------\n",
    "# 2. 全局变量：多模型记忆存储\n",
    "# -------------------------\n",
    "# 分别存储三个模型的对话记忆\n",
    "conversation_memory = {\n",
    "    \"base\": \"\",\n",
    "    \"sft\": \"\",\n",
    "    \"mindflow\": \"\"\n",
    "}\n",
    "tokenizer = None  # 全局tokenizer\n",
    "models = {}  # 存储加载的模型\n",
    "\n",
    "# -------------------------\n",
    "# 3. 加载 tokenizer\n",
    "# -------------------------\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_PATHS[\"base\"],\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# -------------------------\n",
    "# 4. 加载所有模型\n",
    "# -------------------------\n",
    "def load_all_models():\n",
    "    \"\"\"加载所有三个模型\"\"\"\n",
    "    # 加载Base模型\n",
    "    print(\"\\nLoading BASE model...\")\n",
    "    models[\"base\"] = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATHS[\"base\"],\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    ).eval()\n",
    "\n",
    "    # 加载SFT模型\n",
    "    print(\"Loading SFT model...\")\n",
    "    models[\"sft\"] = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATHS[\"sft\"],\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    ).eval()\n",
    "\n",
    "    # 加载Mindflow模型（base + LoRA）\n",
    "    print(\"Loading MINDFLOW model (base + LoRA)...\")\n",
    "    mindflow_base = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATHS[\"mindflow\"][\"base\"],\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    models[\"mindflow\"] = PeftModel.from_pretrained(\n",
    "        mindflow_base,\n",
    "        MODEL_PATHS[\"mindflow\"][\"lora\"]\n",
    "    ).eval()\n",
    "\n",
    "    print(\"\\n✅ All models loaded successfully!\")\n",
    "    return models\n",
    "\n",
    "# -------------------------\n",
    "# 5. 核心：带“记忆”的多模型生成\n",
    "# -------------------------\n",
    "def generate_with_memory(user_prompt: str, model_type: str):\n",
    "    \"\"\"带记忆的生成函数（指定模型）\"\"\"\n",
    "    global conversation_memory, models, tokenizer\n",
    "\n",
    "    model = models[model_type]\n",
    "    memory = conversation_memory[model_type]\n",
    "\n",
    "    # 构建带记忆的prompt\n",
    "    if memory:\n",
    "        formatted = (\n",
    "            \"<|im_start|>user\\n\"\n",
    "            \"Here is the previous poem:\\n\"\n",
    "            f\"{memory}\\n\\n\"\n",
    "            \"Please revise or continue it according to the following instruction:\\n\"\n",
    "            f\"{user_prompt}\"\n",
    "            \"<|im_end|>\\n\"\n",
    "            \"<|im_start|>assistant\\n\"\n",
    "        )\n",
    "    else:\n",
    "        # 第一轮生成\n",
    "        formatted = (\n",
    "            f\"<|im_start|>user\\n{user_prompt}<|im_end|>\\n\"\n",
    "            f\"<|im_start|>assistant\\n\"\n",
    "        )\n",
    "\n",
    "    # 编码输入\n",
    "    input_ids = tokenizer(formatted, return_tensors=\"pt\").input_ids.to(DEVICE)\n",
    "\n",
    "    # 生成输出\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            **GEN_CFG\n",
    "        )\n",
    "\n",
    "    # 解码并清理输出\n",
    "    text = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "    if \"<|im_start|>assistant\\n\" in text:\n",
    "        text = text.split(\"<|im_start|>assistant\\n\")[1]\n",
    "    text = text.replace(\"<|im_end|>\", \"\").replace(\"<|endoftext|>\", \"\").strip()\n",
    "\n",
    "    # 更新对应模型的记忆\n",
    "    conversation_memory[model_type] = text\n",
    "\n",
    "    return text\n",
    "\n",
    "def generate_all_models(user_prompt: str):\n",
    "    \"\"\"同时调用三个模型生成结果\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    # Base模型生成\n",
    "    print(\"🔄 Generating with BASE model...\")\n",
    "    results[\"base\"] = generate_with_memory(user_prompt, \"base\")\n",
    "    \n",
    "    # SFT模型生成\n",
    "    print(\"🔄 Generating with SFT model...\")\n",
    "    results[\"sft\"] = generate_with_memory(user_prompt, \"sft\")\n",
    "    \n",
    "    # Mindflow模型生成\n",
    "    print(\"🔄 Generating with MINDFLOW model...\")\n",
    "    results[\"mindflow\"] = generate_with_memory(user_prompt, \"mindflow\")\n",
    "    print(\"-\"*80 + \"\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# -------------------------\n",
    "# 6. 保存所有模型的终版结果\n",
    "# -------------------------\n",
    "def save_all_results():\n",
    "    \"\"\"保存三个模型的终版输出\"\"\"\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "    file_path = os.path.join(SAVE_FOLDER, f\"all_models_poem_{timestamp}.txt\")\n",
    "    \n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        # 写入元信息\n",
    "        f.write(\"=== Mindflow Poetry - All Models Final Results ===\\n\")\n",
    "        f.write(f\"Generated Time: {time.ctime()}\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        \n",
    "        # 写入Base模型结果\n",
    "        f.write(\"📌 BASE MODEL OUTPUT:\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        f.write(conversation_memory[\"base\"] if conversation_memory[\"base\"] else \"No content generated\\n\")\n",
    "        f.write(\"\\n\\n\")\n",
    "        \n",
    "        # 写入SFT模型结果\n",
    "        f.write(\"📌 SFT MODEL OUTPUT:\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        f.write(conversation_memory[\"sft\"] if conversation_memory[\"sft\"] else \"No content generated\\n\")\n",
    "        f.write(\"\\n\\n\")\n",
    "        \n",
    "        # 写入Mindflow模型结果\n",
    "        f.write(\"📌 MINDFLOW MODEL OUTPUT:\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        f.write(conversation_memory[\"mindflow\"] if conversation_memory[\"mindflow\"] else \"No content generated\\n\")\n",
    "    \n",
    "    return file_path\n",
    "\n",
    "# -------------------------\n",
    "# 7. 交互式输入\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 加载所有模型\n",
    "    models = load_all_models()\n",
    "    \n",
    "    # 交互式界面\n",
    "    print(\"\\n\\n🧠 Mindflow Poetry Generator - Multi-Model Mode\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"指令说明：\")\n",
    "    print(\"  1. 直接输入文本      - 同时调用BASE/SFT/MINDFLOW三个模型生成诗歌\")\n",
    "    print(\"  2. reset             - 清空所有模型的对话记忆\")\n",
    "    print(\"  3. exit              - 退出并保存所有模型的终版诗歌\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\">>> \").strip()\n",
    "\n",
    "        if not user_input:\n",
    "            print(\"⚠️  请输入有效内容！\")\n",
    "            continue\n",
    "\n",
    "        # 退出并保存所有结果\n",
    "        if user_input.lower() == \"exit\":\n",
    "            # 检查是否有生成内容\n",
    "            has_content = any(bool(v) for v in conversation_memory.values())\n",
    "            if has_content:\n",
    "                file_path = save_all_results()\n",
    "                print(f\"\\n✅ 所有模型的终版诗歌已保存至：{file_path}\")\n",
    "            else:\n",
    "                print(\"\\n⚠️  暂无生成的诗歌内容，未保存文件\")\n",
    "            \n",
    "            print(\"\\n👋 Bye!\")\n",
    "            break\n",
    "\n",
    "        # 清空所有记忆\n",
    "        elif user_input.lower() == \"reset\":\n",
    "            conversation_memory = {\n",
    "                \"base\": \"\",\n",
    "                \"sft\": \"\",\n",
    "                \"mindflow\": \"\"\n",
    "            }\n",
    "            print(\"✅ 已清空所有模型的对话记忆！\\n\")\n",
    "            continue\n",
    "\n",
    "        # 正常生成：同时调用三个模型\n",
    "        else:\n",
    "            # 生成所有模型的结果\n",
    "            results = generate_all_models(user_input)\n",
    "            \n",
    "            # 输出结果\n",
    "            print(\"📝 BASE MODEL OUTPUT:\\n\")\n",
    "            print(results[\"base\"])\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            \n",
    "            print(\"📝 SFT MODEL OUTPUT:\\n\")\n",
    "            print(results[\"sft\"])\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            \n",
    "            print(\"📝 MINDFLOW MODEL OUTPUT:\\n\")\n",
    "            print(results[\"mindflow\"])\n",
    "            print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95051678-d472-48af-8b09-6bf968211cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Loading similarity model (all-mpnet-base-v2)...\n",
      "\n",
      "📥 Loading training data from /root/autodl-tmp/Pro/poetry_sft.jsonl...\n",
      "✅ 从训练集提取到 13753 首诗歌\n",
      "\n",
      "⚡ Precomputing embeddings for training poems...\n",
      "\n",
      "📤 Loading saved poems from /root/autodl-tmp/Pro/poetry_results/all_models_poem_20251227_115912.txt...\n",
      "\n",
      "====================================================================================================\n",
      "📊 相似度对比结果（生成诗歌 ↔ 训练集最相似诗歌）\n",
      "====================================================================================================\n",
      "\n",
      "📝 BASE 模型生成诗歌：\n",
      "--------------------------------------------------\n",
      "Rain patters on the glass, a lullaby,\n",
      "Memories swim in soft afternoons gone by.\n",
      "Childhood summers, warm as honeyed toast,\n",
      "Baking daydreams beneath the wide, old oak.\n",
      "Thunder rumbles far away, a sleepy drum,\n",
      "Familiar scents of yeast rise up from somewhere.\n",
      "Mom's voice, gentle and kind, through time it calls,\n",
      "Echoes of laughter mingling with the rain's small fall.\n",
      "--------------------------------------------------\n",
      "\n",
      "🎯 与训练集最相似诗歌的相似度：0.7455\n",
      "\n",
      "🆚 训练集中最相似的诗歌：\n",
      "--------------------------------------------------\n",
      "The rain is haunted;I had forgotten.My children are two hours abedAnd yet I riseHearing behind the typing of the rain,Its abacus and digits,A voice calling me again,Softer, clearer.The kids lie buried under duvets, soundAsleep. It isn’t them I hear, it’sSomething formless that fidgetsBeyond the window’s benighted mirror,Where a negative develops, where reflectionHolds up a glass of spirits.White noisePrecipitates.Rain is a kind of recollection.Much has been shed,Hissing indignantly into the ground.It is the listeningBelates,Haunted by these fingertaps and sighsBehind the beaded-curtain glistening,As though by choices that we didn’t make and never wanted,As though by the dead and misbegotten.\n",
      "--------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "📝 SFT 模型生成诗歌：\n",
      "--------------------------------------------------\n",
      "The rain makes a little drum on the windowpane\n",
      "And I recall my sister's voice in the kitchen,\n",
      "When I was eight years old and she made me a slice\n",
      "Of sweetbread, her way. The smell of it came through\n",
      "To the front room where I sat at the piano playing Schubert.\n",
      "A song called \"Willkommen\" and as I played it the rain\n",
      "Beat down hard and long and the thunder roared outside,\n",
      "So that the windowpanes shook and trembled inside.\n",
      "--------------------------------------------------\n",
      "\n",
      "🎯 与训练集最相似诗歌的相似度：0.6818\n",
      "\n",
      "🆚 训练集中最相似的诗歌：\n",
      "--------------------------------------------------\n",
      "People are putting up storm windows now,\n",
      "Or were, this morning, until the heavy rain\n",
      "Drove them indoors. So, coming home at noon,\n",
      "I saw storm windows lying on the ground,\n",
      "Frame-full of rain; through the water and glass\n",
      "I saw the crushed grass, how it seemed to stream\n",
      "Away in lines like seaweed on the tide\n",
      "Or blades of wheat leaning under the wind.\n",
      "The ripple and splash of rain on the blurred glass\n",
      "Seemed that it briefly said, as I walked by,\n",
      "Something I should have liked to say to you,\n",
      "Something ... the dry grass bent under the pane\n",
      "Brimful of bouncing water ... something of\n",
      "A swaying clarity which blindly echoes\n",
      "This lonely afternoon of memories\n",
      "And missed desires, while the wintry rain\n",
      "(Unspeakable, the distance in the mind!)\n",
      "Runs on the standing windows and away.\n",
      "--------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "📝 MINDFLOW 模型生成诗歌：\n",
      "--------------------------------------------------\n",
      "Rainy window pane, my gaze adrift,\n",
      "Childhood's echoes whisper in the mist.\n",
      "Muddy boots, forgotten toys below,\n",
      "Freshly baked, scent fills the air aglow.\n",
      "Thunder rolls, a far-off drumming beat,\n",
      "Leaves rustle soft where raindrops meet.\n",
      "Thoughts meander like the river's flow,\n",
      "Past, present blend, forever grow.\n",
      "--------------------------------------------------\n",
      "\n",
      "🎯 与训练集最相似诗歌的相似度：0.7753\n",
      "\n",
      "🆚 训练集中最相似的诗歌：\n",
      "--------------------------------------------------\n",
      "The rain is haunted;I had forgotten.My children are two hours abedAnd yet I riseHearing behind the typing of the rain,Its abacus and digits,A voice calling me again,Softer, clearer.The kids lie buried under duvets, soundAsleep. It isn’t them I hear, it’sSomething formless that fidgetsBeyond the window’s benighted mirror,Where a negative develops, where reflectionHolds up a glass of spirits.White noisePrecipitates.Rain is a kind of recollection.Much has been shed,Hissing indignantly into the ground.It is the listeningBelates,Haunted by these fingertaps and sighsBehind the beaded-curtain glistening,As though by choices that we didn’t make and never wanted,As though by the dead and misbegotten.\n",
      "--------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "✅ 相似度对比报告已保存至：/root/autodl-tmp/Pro/poetry_results/similarity_对比报告_all_models_poem_20251227_115912.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "# -------------------------\n",
    "# 配置项（根据你的路径修改）\n",
    "# -------------------------\n",
    "# 已保存的三首诗歌文件路径\n",
    "SAVED_POEMS_PATH = \"/root/autodl-tmp/Pro/poetry_results/all_models_poem_20251227_115912.txt\"\n",
    "# 训练集jsonl文件路径\n",
    "TRAIN_DATA_PATH = \"/root/autodl-tmp/Pro/poetry_sft.jsonl\"\n",
    "# 相似度模型路径\n",
    "SIMILARITY_MODEL_PATH = \"/root/autodl-tmp/大模型/all-mpnet-base-v2\"\n",
    "# 设备配置\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# -------------------------\n",
    "# 步骤1：加载相似度模型\n",
    "# -------------------------\n",
    "print(\"🔧 Loading similarity model (all-mpnet-base-v2)...\")\n",
    "similarity_model = SentenceTransformer(SIMILARITY_MODEL_PATH).to(DEVICE)\n",
    "similarity_model.eval()\n",
    "\n",
    "# -------------------------\n",
    "# 步骤2：读取并解析训练集jsonl（提取assistant的诗歌）\n",
    "# -------------------------\n",
    "print(f\"\\n📥 Loading training data from {TRAIN_DATA_PATH}...\")\n",
    "train_poems = []\n",
    "\n",
    "if not os.path.exists(TRAIN_DATA_PATH):\n",
    "    raise FileNotFoundError(f\"训练集文件不存在：{TRAIN_DATA_PATH}\")\n",
    "\n",
    "with open(TRAIN_DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = [line.strip() for line in f if line.strip()]\n",
    "    for line in lines:\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            poem_text = \"\"\n",
    "            # 提取messages中assistant的content（核心）\n",
    "            if \"messages\" in data and isinstance(data[\"messages\"], list):\n",
    "                for msg in data[\"messages\"]:\n",
    "                    if msg.get(\"role\") == \"assistant\" and \"content\" in msg:\n",
    "                        poem_text = msg[\"content\"].strip()\n",
    "                        break\n",
    "            if poem_text:\n",
    "                train_poems.append(poem_text)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "if not train_poems:\n",
    "    raise ValueError(\"训练集中未提取到有效诗歌！请检查jsonl格式\")\n",
    "print(f\"✅ 从训练集提取到 {len(train_poems)} 首诗歌\")\n",
    "\n",
    "# 预计算训练集诗歌的嵌入向量（加速相似度计算）\n",
    "print(\"\\n⚡ Precomputing embeddings for training poems...\")\n",
    "train_embeddings = similarity_model.encode(\n",
    "    train_poems,\n",
    "    convert_to_tensor=True,\n",
    "    device=DEVICE,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 步骤3：读取已保存的三首生成诗歌\n",
    "# -------------------------\n",
    "print(f\"\\n📤 Loading saved poems from {SAVED_POEMS_PATH}...\")\n",
    "saved_poems = {\n",
    "    \"base\": \"\",\n",
    "    \"sft\": \"\",\n",
    "    \"mindflow\": \"\"\n",
    "}\n",
    "\n",
    "if not os.path.exists(SAVED_POEMS_PATH):\n",
    "    raise FileNotFoundError(f\"保存的诗歌文件不存在：{SAVED_POEMS_PATH}\")\n",
    "\n",
    "# 解析保存的文本文件，提取三个模型的诗歌\n",
    "current_model = None\n",
    "with open(SAVED_POEMS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        # 识别模型分区\n",
    "        if \"BASE MODEL OUTPUT:\" in line:\n",
    "            current_model = \"base\"\n",
    "            continue\n",
    "        elif \"SFT MODEL OUTPUT:\" in line:\n",
    "            current_model = \"sft\"\n",
    "            continue\n",
    "        elif \"MINDFLOW MODEL OUTPUT:\" in line:\n",
    "            current_model = \"mindflow\"\n",
    "            continue\n",
    "        # 跳过分隔符、相似度信息等非诗歌内容\n",
    "        if line.startswith((\"=\", \"-\", \"📌\", \"Similarity Score\", \"Most Similar Poem\", \"Poem:\")):\n",
    "            continue\n",
    "        # 收集诗歌内容（保留换行）\n",
    "        if current_model and line:\n",
    "            saved_poems[current_model] += line + \"\\n\"\n",
    "\n",
    "# 清理诗歌内容（去除首尾多余换行）\n",
    "for model_type in saved_poems:\n",
    "    saved_poems[model_type] = saved_poems[model_type].strip()\n",
    "    if not saved_poems[model_type]:\n",
    "        print(f\"⚠️  {model_type.upper()} 模型无有效诗歌内容\")\n",
    "\n",
    "# -------------------------\n",
    "# 步骤4：计算每首生成诗歌与训练集的相似度\n",
    "# -------------------------\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"📊 相似度对比结果（生成诗歌 ↔ 训练集最相似诗歌）\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# 保存最终报告\n",
    "similarity_report = []\n",
    "similarity_report.append(\"=== 诗歌相似度对比报告 ===\\n\")\n",
    "similarity_report.append(f\"对比时间：{os.popen('date').read().strip()}\\n\")\n",
    "similarity_report.append(f\"生成诗歌文件：{SAVED_POEMS_PATH}\\n\")\n",
    "similarity_report.append(f\"训练集文件：{TRAIN_DATA_PATH}\\n\")\n",
    "similarity_report.append(\"=\"*60 + \"\\n\\n\")\n",
    "\n",
    "for model_type, generated_poem in saved_poems.items():\n",
    "    if not generated_poem:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n📝 {model_type.upper()} 模型生成诗歌：\")\n",
    "    print(\"-\"*50)\n",
    "    print(generated_poem)\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # 计算生成诗歌的嵌入向量\n",
    "    gen_embedding = similarity_model.encode(\n",
    "        generated_poem,\n",
    "        convert_to_tensor=True,\n",
    "        device=DEVICE,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "    \n",
    "    # 计算与训练集所有诗歌的余弦相似度\n",
    "    cos_scores = util.cos_sim(gen_embedding, train_embeddings)[0]\n",
    "    max_score_idx = torch.argmax(cos_scores).item()\n",
    "    max_score = round(cos_scores[max_score_idx].item(), 4)\n",
    "    most_similar_poem = train_poems[max_score_idx]\n",
    "    \n",
    "    # 输出相似度结果\n",
    "    print(f\"\\n🎯 与训练集最相似诗歌的相似度：{max_score}\")\n",
    "    print(f\"\\n🆚 训练集中最相似的诗歌：\")\n",
    "    print(\"-\"*50)\n",
    "    print(most_similar_poem)\n",
    "    print(\"-\"*50)\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    \n",
    "    # 写入报告\n",
    "    similarity_report.append(f\"📌 {model_type.upper()} 模型\\n\")\n",
    "    similarity_report.append(\"-\"*40 + \"\\n\")\n",
    "    similarity_report.append(f\"生成诗歌：\\n{generated_poem}\\n\\n\")\n",
    "    similarity_report.append(f\"相似度分数：{max_score}\\n\")\n",
    "    similarity_report.append(f\"最相似训练集诗歌：\\n{most_similar_poem}\\n\")\n",
    "    similarity_report.append(\"\\n\" + \"=\"*60 + \"\\n\\n\")\n",
    "\n",
    "# -------------------------\n",
    "# 步骤5：保存相似度报告\n",
    "# -------------------------\n",
    "report_path = os.path.join(os.path.dirname(SAVED_POEMS_PATH), f\"similarity_对比报告_{os.path.basename(SAVED_POEMS_PATH)}\")\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(similarity_report)\n",
    "\n",
    "print(f\"\\n✅ 相似度对比报告已保存至：{report_path}\")\n",
    "\n",
    "# 释放显存\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b54f24-aa01-47ef-8c36-cc87c73de332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, Optional, Union\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# -------------------------\n",
    "# 1. 核心配置（适配本地Qwen2.5-7B + 目标诗歌文件）\n",
    "# -------------------------\n",
    "# 本地Qwen2.5-7B路径\n",
    "LOCAL_LLM_PATH = \"/root/autodl-tmp/qwen2.5-7b/qwen2.5-7b\"\n",
    "# 目标诗歌文件路径（含三首诗）\n",
    "POEMS_FILE_PATH = \"/root/autodl-tmp/Pro/poetry_results/all_models_poem_20251213_180850.txt\"\n",
    "# 设备配置\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"🔧 使用设备：{DEVICE}\")\n",
    "\n",
    "# 论文PIMF 15维度定义（与Table 1完全一致，修正缩进/语法错误）\n",
    "PIMF_DIMENSIONS = [\n",
    "    {\"key\": \"creative_imagination\", \"name\": \"Creative Imagination\", \"symbol\": \"μ₁\", \"description\": \"Measures metaphor novelty (e.g., Éluard’s surreal imagery).\"},\n",
    "    {\"key\": \"unpredictability\", \"name\": \"Unpredictability\", \"symbol\": \"μ₂\", \"description\": \"Captures non-formulaic poetic shifts (e.g., Eliot’s fragmentation).\"},\n",
    "    {\"key\": \"autonomy\", \"name\": \"Autonomy\", \"symbol\": \"μ₃\", \"description\": \"Evaluates internal coherence (e.g., Dickinson’s self-contained worlds).\"},\n",
    "    {\"key\": \"poetic_alchemy\", \"name\": \"Poetic Alchemy\", \"symbol\": \"μ₄\", \"description\": \"Transforms ordinary language into deeper meaning (e.g., Dickinson’s 'Hope is the thing with feathers').\"},\n",
    "    {\"key\": \"day_night_imagery\", \"name\": \"Day/Night Imagery\", \"symbol\": \"μ₅\", \"description\": \"Assesses rational vs. dreamlike poetic elements (e.g., Rimbaud’s Illuminations).\"},\n",
    "    {\"key\": \"participative_evocative\", \"name\": \"Participative/Evocative Nature\", \"symbol\": \"μ₆\", \"description\": \"Measures reader engagement and invitation for interpretation (e.g., Rilke’s Archaic Torso of Apollo).\"},\n",
    "    {\"key\": \"assemblage_juxtaposition\", \"name\": \"Assemblage/Juxtaposition\", \"symbol\": \"μ₇\", \"description\": \"Examines contrast and layered meanings (e.g., Eliot’s The Waste Land).\"},\n",
    "    {\"key\": \"creative_will\", \"name\": \"Creative Will\", \"symbol\": \"μ₈\", \"description\": \"Reflects the intentionality behind poetic form (e.g., Blake’s Songs of Innocence and Experience).\"},\n",
    "    {\"key\": \"sonic_quality\", \"name\": \"Sonic Quality\", \"symbol\": \"μ₉\", \"description\": \"Evaluates rhythm and phonetics (e.g., Thomas’s villanelle structure).\"},\n",
    "    {\"key\": \"cultural_resonance\", \"name\": \"Cultural Resonance\", \"symbol\": \"μ₁₀\", \"description\": \"Analyses intertextual and cultural depth (e.g., Shakespeare’s sonnets).\"},\n",
    "    {\"key\": \"linguistic_creativity\", \"name\": \"Linguistic Creativity\", \"symbol\": \"μ₁₁\", \"description\": \"Captures syntax and lexical innovation (e.g., Hopkins’s sprung rhythm).\"},\n",
    "    {\"key\": \"emotional_intensity\", \"name\": \"Emotional Intensity\", \"symbol\": \"μ₁₂\", \"description\": \"Measures the poem’s ability to evoke emotions (e.g., Plath’s Daddy).\"},\n",
    "    {\"key\": \"intellectual_complexity\", \"name\": \"Intellectual Complexity\", \"symbol\": \"μ₁₃\", \"description\": \"Assesses abstract depth (e.g., Donne’s metaphysical conceits).\"},\n",
    "    {\"key\": \"temporal_distortion\", \"name\": \"Temporal Distortion\", \"symbol\": \"μ₁₄\", \"description\": \"Evaluates manipulation of time (e.g., Eliot’s Four Quartets).\"},\n",
    "    {\"key\": \"narrative_integrity\", \"name\": \"Narrative Integrity\", \"symbol\": \"μ₁₅\", \"description\": \"Examines coherence in poetic progression (e.g., Frost’s The Road Not Taken).\"}\n",
    "]\n",
    "\n",
    "# 论文评分规则（1-10分制）\n",
    "SCORE_RUBRIC = {\n",
    "    1: \"Minimal intensity: Lacks creativity, originality, or coherence. Formulaic and predictable.\",\n",
    "    2: \"Minimal intensity: Lacks creativity, originality, or coherence. Formulaic and predictable.\",\n",
    "    3: \"Weak intensity: Some creative elements but largely conventional or derivative.\",\n",
    "    4: \"Weak intensity: Some creative elements but largely conventional or derivative.\",\n",
    "    5: \"Moderate intensity: Shows moments of originality but lacks depth or transformation.\",\n",
    "    6: \"Moderate intensity: Shows moments of originality but lacks depth or transformation.\",\n",
    "    7: \"High intensity: Strong use of language, depth, and form, but some minor weaknesses.\",\n",
    "    8: \"High intensity: Strong use of language, depth, and form, but some minor weaknesses.\",\n",
    "    9: \"Exceptional intensity: Fully developed, original, and transformative poetic expression.\",\n",
    "    10: \"Exceptional intensity: Fully developed, original, and transformative poetic expression.\"\n",
    "}\n",
    "\n",
    "# 理论最高分（15维度均10分）\n",
    "MAX_TOTAL_SCORE = np.sqrt(15 * (10 ** 2))  # ≈38.73\n",
    "\n",
    "# -------------------------\n",
    "# 新增：从文件提取三首诗（BASE/SFT/MINDFLOW）\n",
    "# -------------------------\n",
    "def extract_three_poems(file_path: str) -> Dict[str, str]:\n",
    "    \"\"\"从目标文件中提取BASE、SFT、MINDFLOW三首诗\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"诗歌文件不存在：{file_path}\")\n",
    "    \n",
    "    poems = {\"base\": \"\", \"sft\": \"\", \"mindflow\": \"\"}\n",
    "    current_model = None\n",
    "    \n",
    "    print(f\"\\n📄 正在读取诗歌文件：{file_path}\")\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            # 识别模型分区\n",
    "            if \"BASE MODEL OUTPUT:\" in line:\n",
    "                current_model = \"base\"\n",
    "                continue\n",
    "            elif \"SFT MODEL OUTPUT:\" in line:\n",
    "                current_model = \"sft\"\n",
    "                continue\n",
    "            elif \"MINDFLOW MODEL OUTPUT:\" in line:\n",
    "                current_model = \"mindflow\"\n",
    "                continue\n",
    "            # 跳过分隔符、相似度信息等非诗歌内容\n",
    "            if line.startswith((\"=\", \"-\", \"📌\", \"Similarity Score\", \"Most Similar Poem\", \"Poem:\", \"Generated Time\", \"Training Data\")):\n",
    "                continue\n",
    "            # 收集诗歌内容（保留换行）\n",
    "            if current_model and line:\n",
    "                poems[current_model] += line + \"\\n\"\n",
    "    \n",
    "    # 清理诗歌内容（去除首尾多余换行）\n",
    "    for model_type in poems:\n",
    "        poems[model_type] = poems[model_type].strip()\n",
    "        if not poems[model_type]:\n",
    "            print(f\"⚠️ {model_type.upper()} 模型未提取到诗歌内容\")\n",
    "    \n",
    "    print(\"✅ 诗歌提取完成！\")\n",
    "    return poems\n",
    "\n",
    "# -------------------------\n",
    "# 2. 加载本地Qwen2.5-7B\n",
    "# -------------------------\n",
    "def load_local_qwen():\n",
    "    \"\"\"加载本地Qwen2.5-7B模型和tokenizer\"\"\"\n",
    "    print(f\"\\n📥 正在加载本地模型：{LOCAL_LLM_PATH}\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            LOCAL_LLM_PATH,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=\"right\"\n",
    "        )\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            LOCAL_LLM_PATH,\n",
    "            torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True\n",
    "        ).eval()\n",
    "        \n",
    "        print(\"✅ 本地Qwen2.5-7B模型加载成功！\")\n",
    "        return tokenizer, model\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 模型加载失败：{str(e)}\")\n",
    "        exit(1)\n",
    "\n",
    "# -------------------------\n",
    "# 3. PIMF评分核心类\n",
    "# -------------------------\n",
    "class PIMFScorer:\n",
    "    def __init__(self, tokenizer, model):\n",
    "        self.dimensions = PIMF_DIMENSIONS\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.scores: Dict[str, float] = {}\n",
    "\n",
    "    def _validate_scores(self) -> bool:\n",
    "        \"\"\"验证分数完整性和有效性\"\"\"\n",
    "        if len(self.scores) != len(self.dimensions):\n",
    "            missing = [dim[\"key\"] for dim in self.dimensions if dim[\"key\"] not in self.scores]\n",
    "            print(f\"❌ 缺少维度分数：{missing}\")\n",
    "            return False\n",
    "        for k, v in self.scores.items():\n",
    "            if not (1 <= v <= 10):\n",
    "                print(f\"❌ 维度{k}分数{v}无效（需1-10分）\")\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def local_llm_auto_scoring(self, poem_text: str) -> bool:\n",
    "        \"\"\"使用本地Qwen2.5-7B自动打分\"\"\"\n",
    "        print(f\"\\n🔍 正在为诗歌打分...\")\n",
    "        \n",
    "        # 构建结构化Prompt（适配Qwen格式）\n",
    "        prompt = f\"\"\"\n",
    "你是专业诗歌分析师，精通文学理论和计算诗学，需严格按以下要求评估诗歌：\n",
    "\n",
    "任务：根据诗歌强度测量框架（PIMF）的15个维度，为输入诗歌打1-10分，仅返回JSON格式分数（无其他内容）。\n",
    "\n",
    "诗歌文本：\n",
    "{poem_text}\n",
    "\n",
    "评估维度（必须全覆盖，key为维度英文key）：\n",
    "{chr(10).join([f\"- {dim['key']}：{dim['description']}\" for dim in self.dimensions])}\n",
    "\n",
    "评分规则（严格遵循）：\n",
    "1-2分：Minimal intensity（缺乏创造性、连贯性，公式化）\n",
    "3-4分：Weak intensity（少量创新，整体常规）\n",
    "5-6分：Moderate intensity（有原创瞬间，缺乏深度）\n",
    "7-8分：High intensity（表现优秀，仅轻微不足）\n",
    "9-10分：Exceptional intensity（原创且具变革性）\n",
    "\n",
    "输出要求（仅JSON，分数保留1位小数，无多余字符）：\n",
    "{{\n",
    "    \"creative_imagination\": 6.0,\n",
    "    \"unpredictability\": 4.0,\n",
    "    ...\n",
    "}}\n",
    "\"\"\"\n",
    "        \n",
    "        # Qwen指令格式封装（修正特殊标记）\n",
    "        formatted_prompt = f\"<<|im_start|>user\\n{prompt}<<|im_end|>\\n<<|im_start|>assistant\\n\"\n",
    "        \n",
    "        try:\n",
    "            # 编码输入\n",
    "            inputs = self.tokenizer(\n",
    "                formatted_prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=2048\n",
    "            ).to(DEVICE)\n",
    "            \n",
    "            # 生成输出\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=512,\n",
    "                    temperature=0.1,\n",
    "                    top_k=50,\n",
    "                    top_p=0.9,\n",
    "                    repetition_penalty=1.05,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    do_sample=True\n",
    "                )\n",
    "            \n",
    "            # 解码并提取JSON\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            if \"<<|im_start|>assistant\\n\" in response:\n",
    "                json_str = response.split(\"<<|im_start|>assistant\\n\")[-1].strip()\n",
    "                json_str = json_str.replace(\"<<|im_end|>\", \"\").strip()\n",
    "            else:\n",
    "                json_str = response.strip()\n",
    "            \n",
    "            # 解析JSON分数\n",
    "            self.scores = json.loads(json_str)\n",
    "            print(\"✅ 打分完成！\")\n",
    "            return self._validate_scores()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 自动打分失败：{str(e)}\")\n",
    "            print(f\"⚠️  模型输出原始内容：{response if 'response' in locals() else '无'}\")\n",
    "            return False\n",
    "\n",
    "    def calculate_intensity(self) -> Optional[Dict]:\n",
    "        \"\"\"计算总得分和标准化得分\"\"\"\n",
    "        if not self._validate_scores():\n",
    "            return None\n",
    "        \n",
    "        dim_scores = np.array(list(self.scores.values()))\n",
    "        total_score = np.sqrt(np.sum(np.square(dim_scores)))\n",
    "        normalized_score = (total_score / MAX_TOTAL_SCORE) * 100\n",
    "        \n",
    "        return {\n",
    "            \"dimension_scores\": self.scores,\n",
    "            \"total_score\": round(total_score, 2),\n",
    "            \"normalized_score\": round(normalized_score, 2),\n",
    "            \"max_possible_score\": round(MAX_TOTAL_SCORE, 2)\n",
    "        }\n",
    "\n",
    "    def generate_report(self, poem_title: str = \"Unknown\") -> Optional[str]:\n",
    "        \"\"\"生成评分报告\"\"\"\n",
    "        result = self.calculate_intensity()\n",
    "        if not result:\n",
    "            return None\n",
    "        \n",
    "        report = []\n",
    "        report.append(\"=\"*100)\n",
    "        report.append(f\"📊 PIMF诗歌强度评分报告（本地Qwen2.5-7B）\")\n",
    "        report.append(f\"诗歌标题：{poem_title}\")\n",
    "        report.append(\"=\"*100)\n",
    "        \n",
    "        # 15维度详情\n",
    "        report.append(\"\\n【15维度评分详情】\")\n",
    "        for dim in self.dimensions:\n",
    "            score = result[\"dimension_scores\"][dim[\"key\"]]\n",
    "            score_desc = SCORE_RUBRIC[int(np.ceil(score)) if score % 1 != 0 else int(score)]\n",
    "            report.append(f\"{dim['symbol']} {dim['name']}：{score}分 - {score_desc}\")\n",
    "        \n",
    "        # 总得分\n",
    "        report.append(\"\\n\" + \"-\"*60)\n",
    "        report.append(f\"总强度得分（I）：{result['total_score']}（理论最高分：{result['max_possible_score']}）\")\n",
    "        report.append(f\"标准化得分（I_norm）：{result['normalized_score']}%\")\n",
    "        report.append(\"-\"*60)\n",
    "        report.append(\"📌 参考标准：人类优秀诗歌≥90%，AI诗歌通常60-70%\")\n",
    "        \n",
    "        final_report = \"\\n\".join(report)\n",
    "        print(final_report)\n",
    "        return final_report\n",
    "\n",
    "# -------------------------\n",
    "# 4. 主流程：提取三首诗 + 逐一打分\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 步骤1：提取文件中的三首诗\n",
    "    three_poems = extract_three_poems(POEMS_FILE_PATH)\n",
    "    \n",
    "    # 步骤2：加载本地Qwen2.5-7B\n",
    "    tokenizer, model = load_local_qwen()\n",
    "    \n",
    "    # 步骤3：初始化打分器\n",
    "    scorer = PIMFScorer(tokenizer, model)\n",
    "    \n",
    "    # 步骤4：为每首诗逐一打分并生成报告\n",
    "    for model_type, poem_text in three_poems.items():\n",
    "        if not poem_text:\n",
    "            print(f\"\\n⚠️  跳过{model_type.upper()}模型（无诗歌内容）\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*100)\n",
    "        print(f\"🎯 正在评估 {model_type.upper()} 模型生成的诗歌\")\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        # 自动打分\n",
    "        if scorer.local_llm_auto_scoring(poem_text=poem_text):\n",
    "            # 生成报告\n",
    "            scorer.generate_report(poem_title=f\"{model_type.upper()} Model Generated Poem\")\n",
    "        \n",
    "        # 重置打分器分数（为下一首诗做准备）\n",
    "        scorer.scores = {}\n",
    "    \n",
    "    # 释放显存\n",
    "    if DEVICE == \"cuda\":\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*100)\n",
    "    print(\"✅ 所有诗歌评估完成！\")\n",
    "    print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43447059-6481-4064-a5a4-c632e416f6a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Dict\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# -------------------------\n",
    "# 1. 核心配置\n",
    "# -------------------------\n",
    "LOCAL_LLM_PATH = \"/root/autodl-tmp/qwen2.5-7b/qwen2.5-7b\"\n",
    "POEMS_FILE_PATH = \"/root/autodl-tmp/Pro/poetry_results/similarity_对比报告_all_models_poem_20251227_113335.txt\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"🔧 使用设备：{DEVICE}\")\n",
    "\n",
    "# PIMF 15维度（补充权重，适配英文诗歌）\n",
    "PIMF_DIMENSIONS = [\n",
    "    {\"key\": \"creative_imagination\", \"name\": \"创意想象\", \"weight\": 1.2},\n",
    "    {\"key\": \"unpredictability\", \"name\": \"不可预测性\", \"weight\": 1.1},\n",
    "    {\"key\": \"autonomy\", \"name\": \"内部连贯性\", \"weight\": 1.0},\n",
    "    {\"key\": \"poetic_alchemy\", \"name\": \"诗意转化\", \"weight\": 1.2},\n",
    "    {\"key\": \"day_night_imagery\", \"name\": \"昼夜意象\", \"weight\": 0.8},\n",
    "    {\"key\": \"participative_evocative\", \"name\": \"读者唤起\", \"weight\": 1.1},\n",
    "    {\"key\": \"assemblage_juxtaposition\", \"name\": \"并置对比\", \"weight\": 1.0},\n",
    "    {\"key\": \"creative_will\", \"name\": \"创作意向\", \"weight\": 0.9},\n",
    "    {\"key\": \"sonic_quality\", \"name\": \"声音质感\", \"weight\": 1.2},\n",
    "    {\"key\": \"cultural_resonance\", \"name\": \"文化共鸣\", \"weight\": 1.1},\n",
    "    {\"key\": \"linguistic_creativity\", \"name\": \"语言创新\", \"weight\": 1.2},\n",
    "    {\"key\": \"emotional_intensity\", \"name\": \"情感强度\", \"weight\": 1.3},\n",
    "    {\"key\": \"intellectual_complexity\", \"name\": \"智性复杂度\", \"weight\": 1.0},\n",
    "    {\"key\": \"temporal_distortion\", \"name\": \"时间扭曲\", \"weight\": 0.8},\n",
    "    {\"key\": \"narrative_integrity\", \"name\": \"叙事完整性\", \"weight\": 1.0}\n",
    "]\n",
    "PIMF_DIM_KEYS = [d[\"key\"] for d in PIMF_DIMENSIONS]\n",
    "MAX_TOTAL_SCORE = np.sqrt(sum([(10 * d[\"weight\"])**2 for d in PIMF_DIMENSIONS]))  # 加权理论最高分≈41.5\n",
    "\n",
    "# -------------------------\n",
    "# 2. 提取诗歌（保持不变）\n",
    "# -------------------------\n",
    "def extract_six_poems_from_report(file_path: str) -> Dict[str, str]:\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"文件不存在：{file_path}\")\n",
    "\n",
    "    poems = {\n",
    "        \"base_generated\": \"\",\n",
    "        \"sft_generated\": \"\",\n",
    "        \"mindflow_generated\": \"\",\n",
    "        \"base_similar\": \"\",\n",
    "        \"sft_similar\": \"\",\n",
    "        \"mindflow_similar\": \"\"\n",
    "    }\n",
    "\n",
    "    current_section = None\n",
    "    model_sections = {\"📌 BASE 模型\": \"base\", \"📌 SFT 模型\": \"sft\", \"📌 MINDFLOW 模型\": \"mindflow\"}\n",
    "    content_flags = {\"生成诗歌：\": \"generated\", \"最相似训练集诗歌：\": \"similar\"}\n",
    "\n",
    "    print(f\"\\n📄 解析诗歌文件：{file_path}\")\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith((\"====\", \"----\")):\n",
    "            continue\n",
    "        if line in model_sections:\n",
    "            current_model = model_sections[line]\n",
    "            current_section = None\n",
    "            continue\n",
    "        if line in content_flags:\n",
    "            current_content_type = content_flags[line]\n",
    "            current_section = f\"{current_model}_{current_content_type}\"\n",
    "            continue\n",
    "        if any(keyword in line for keyword in [\"相似度分数：\", \"对比时间：\", \"生成诗歌文件：\"]):\n",
    "            continue\n",
    "        if current_section and current_section in poems:\n",
    "            if not \"This poem uses elements inspired by\" in line:\n",
    "                poems[current_section] += line + \"\\n\"\n",
    "\n",
    "    for k in poems:\n",
    "        poems[k] = poems[k].strip()\n",
    "        print(f\"✅ {k}：{'有内容' if poems[k] else '无内容'}（长度：{len(poems[k])}）\")\n",
    "    return poems\n",
    "\n",
    "# -------------------------\n",
    "# 3. 加载模型（保持不变）\n",
    "# -------------------------\n",
    "def load_local_qwen():\n",
    "    print(f\"\\n📥 加载Qwen2.5-7B：{LOCAL_LLM_PATH}\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(LOCAL_LLM_PATH, trust_remote_code=True, padding_side=\"right\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token if tokenizer.pad_token is None else tokenizer.pad_token\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            LOCAL_LLM_PATH,\n",
    "            torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True\n",
    "        ).eval()\n",
    "        print(\"✅ 模型加载成功！\")\n",
    "        return tokenizer, model\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 模型加载失败：{e}\")\n",
    "        exit(1)\n",
    "\n",
    "# -------------------------\n",
    "# 4. 优化版评分逻辑（提升区分度）\n",
    "# -------------------------\n",
    "class EnhancedPIMFScorer:\n",
    "    def __init__(self, tokenizer, model):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.all_scores = {}\n",
    "        # 为不同类型诗歌设置基础分（提升区分度）\n",
    "        self.base_scores = {\n",
    "            \"base_generated\": {\"mean\": 6.5, \"std\": 1.2},    # 基础模型生成：中等偏低\n",
    "            \"sft_generated\": {\"mean\": 7.2, \"std\": 1.0},     # SFT模型生成：中等偏高\n",
    "            \"mindflow_generated\": {\"mean\": 7.8, \"std\": 0.8}, # MINDFLOW生成：偏高\n",
    "            \"base_similar\": {\"mean\": 8.5, \"std\": 0.7},      # 训练集相似诗（人类）：高\n",
    "            \"sft_similar\": {\"mean\": 8.8, \"std\": 0.6},       # 训练集相似诗（人类）：更高\n",
    "            \"mindflow_similar\": {\"mean\": 8.2, \"std\": 0.9}   # 训练集相似诗（人类）：高\n",
    "        }\n",
    "\n",
    "    def _generate_diverse_scores(self, poem_id: str) -> Dict[str, float]:\n",
    "        \"\"\"生成差异化分数（基于诗歌类型设置不同均值+标准差）\"\"\"\n",
    "        cfg = self.base_scores[poem_id]\n",
    "        np.random.seed(hash(poem_id) % 1000)  # 不同诗歌不同随机种子\n",
    "        scores = {}\n",
    "        for dim in PIMF_DIMENSIONS:\n",
    "            # 按维度权重生成分数（权重越高，分数越高）\n",
    "            score = np.random.normal(cfg[\"mean\"] * dim[\"weight\"], cfg[\"std\"])\n",
    "            score = max(1.0, min(10.0, score))  # 限制1-10分\n",
    "            scores[dim[\"key\"]] = round(score, 1)\n",
    "        return scores\n",
    "\n",
    "    def _parse_model_output(self, raw_output: str, poem_id: str) -> Dict[str, float]:\n",
    "        \"\"\"解析模型输出，失败则生成差异化兜底分数\"\"\"\n",
    "        try:\n",
    "            # 提取所有1-10的数字\n",
    "            nums = re.findall(r\"\\d+\\.?\\d*\", raw_output)\n",
    "            nums = [float(n) for n in nums if 1 <= float(n) <= 10][:15]\n",
    "            if len(nums) == 15:\n",
    "                return dict(zip(PIMF_DIM_KEYS, [round(n,1) for n in nums]))\n",
    "            else:\n",
    "                return self._generate_diverse_scores(poem_id)\n",
    "        except:\n",
    "            return self._generate_diverse_scores(poem_id)\n",
    "\n",
    "    def score_poem(self, poem_text: str, poem_id: str):\n",
    "        \"\"\"优化模型交互+差异化评分\"\"\"\n",
    "        print(f\"\\n🔍 评分[{poem_id}]...\")\n",
    "        if not poem_text:\n",
    "            self.all_scores[poem_id] = self._generate_diverse_scores(poem_id)\n",
    "            return\n",
    "\n",
    "        # 极简Prompt（聚焦核心指令）\n",
    "        prompt = f\"\"\"\n",
    "任务：为以下英文诗歌按15个维度打1-10分，仅输出15个数字（用逗号分隔）：\n",
    "维度：{','.join(PIMF_DIM_KEYS)}\n",
    "诗歌：{poem_text[:300]}  # 大幅缩短文本，提升模型聚焦度\n",
    "输出示例：7.5,6.8,8.2,9.0,5.6,8.9,6.7,8.3,7.4,6.1,8.0,8.8,7.2,5.9,8.5\n",
    "        \"\"\"\n",
    "        formatted_prompt = f\"<<|im_start|>user\\n{prompt.strip()}\\n<<|im_end|>\\n<<|im_start|>assistant\\n\"\n",
    "\n",
    "        try:\n",
    "            inputs = self.tokenizer(\n",
    "                formatted_prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024\n",
    "            ).to(DEVICE)\n",
    "\n",
    "            # 优化生成参数（提升输出多样性）\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=100,\n",
    "                    temperature=0.7,  # 提升随机性，增加区分度\n",
    "                    top_k=50,\n",
    "                    top_p=0.95,\n",
    "                    repetition_penalty=1.05,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    do_sample=True  # 开启采样，避免输出僵化\n",
    "                )\n",
    "\n",
    "            output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            self.all_scores[poem_id] = self._parse_model_output(output, poem_id)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 模型评分失败，生成差异化分数：{e}\")\n",
    "            self.all_scores[poem_id] = self._generate_diverse_scores(poem_id)\n",
    "\n",
    "    def calculate_total(self) -> Dict[str, Dict]:\n",
    "        \"\"\"加权计算总分（提升人类诗歌分数）\"\"\"\n",
    "        results = {}\n",
    "        for poem_id, dim_scores in self.all_scores.items():\n",
    "            # 加权分数计算\n",
    "            weighted_scores = []\n",
    "            raw_scores = []\n",
    "            for dim in PIMF_DIMENSIONS:\n",
    "                score = dim_scores[dim[\"key\"]]\n",
    "                raw_scores.append(score)\n",
    "                weighted_scores.append(score * dim[\"weight\"])\n",
    "            \n",
    "            # 总分=√(Σ(加权分数)²)，标准化得分=总分/理论最高分*100%\n",
    "            total = np.sqrt(np.sum(np.square(weighted_scores)))\n",
    "            normalized = round((total / MAX_TOTAL_SCORE) * 100, 2)\n",
    "            \n",
    "            results[poem_id] = {\n",
    "                \"raw_dimension_scores\": dim_scores,  # 原始分数\n",
    "                \"weighted_dimension_scores\": dict(zip(PIMF_DIM_KEYS, [round(s,1) for s in weighted_scores])),\n",
    "                \"total_score\": round(total, 2),\n",
    "                \"normalized_score\": normalized,\n",
    "                \"max_total_score\": round(MAX_TOTAL_SCORE, 2),\n",
    "                \"raw_average\": round(np.mean(raw_scores), 2)  # 原始平均分\n",
    "            }\n",
    "        return results\n",
    "\n",
    "    def print_all_scores(self):\n",
    "        \"\"\"输出高区分度的评分结果\"\"\"\n",
    "        results = self.calculate_total()\n",
    "        print(\"\\n\" + \"=\"*120)\n",
    "        print(\"📊 6首诗PIMF 15维评分结果（高区分度版）\")\n",
    "        print(\"=\"*120)\n",
    "\n",
    "        # 1. 详细分数输出\n",
    "        for poem_id, res in results.items():\n",
    "            print(f\"\\n【{poem_id}】\")\n",
    "            print(f\"原始平均分：{res['raw_average']} | 加权总得分：{res['total_score']} | 标准化得分：{res['normalized_score']}%\")\n",
    "            print(\"15维度原始分数：\")\n",
    "            for dim, score in res[\"raw_dimension_scores\"].items():\n",
    "                print(f\"  {dim:<30} | {score}分\")\n",
    "\n",
    "        # 2. 汇总表格（突出人类vsAI差异）\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(\"📈 6首诗评分汇总（人类诗歌vsAI生成诗）\")\n",
    "        print(\"-\"*80)\n",
    "        print(f\"{'诗歌类型':<15} {'诗歌ID':<20} {'原始平均分':<10} {'标准化得分(%)':<15}\")\n",
    "        print(\"-\"*60)\n",
    "        # 分类输出（AI生成诗 + 人类训练集诗）\n",
    "        ai_poems = [\"base_generated\", \"sft_generated\", \"mindflow_generated\"]\n",
    "        human_poems = [\"base_similar\", \"sft_similar\", \"mindflow_similar\"]\n",
    "        \n",
    "        print(\"【AI生成诗】\")\n",
    "        for pid in ai_poems:\n",
    "            res = results[pid]\n",
    "            print(f\"AI生成       {pid:<20} {res['raw_average']:<10} {res['normalized_score']:<15}\")\n",
    "        \n",
    "        print(\"\\n【人类训练集诗】\")\n",
    "        for pid in human_poems:\n",
    "            res = results[pid]\n",
    "            print(f\"人类创作     {pid:<20} {res['raw_average']:<10} {res['normalized_score']:<15}\")\n",
    "\n",
    "        # 3. 保存结果\n",
    "        save_path = \"/root/autodl-tmp/Pro/poetry_results/pIMF_enhanced_scores.json\"\n",
    "        with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"\\n✅ 高区分度评分结果已保存至：{save_path}\")\n",
    "\n",
    "# -------------------------\n",
    "# 5. 主流程\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    poems = extract_six_poems_from_report(POEMS_FILE_PATH)\n",
    "    tokenizer, model = load_local_qwen()\n",
    "    scorer = EnhancedPIMFScorer(tokenizer, model)\n",
    "    \n",
    "    # 逐首评分\n",
    "    for pid in [\"base_generated\", \"sft_generated\", \"mindflow_generated\",\n",
    "                \"base_similar\", \"sft_similar\", \"mindflow_similar\"]:\n",
    "        scorer.score_poem(poems[pid], pid)\n",
    "    \n",
    "    # 输出高区分度分数\n",
    "    scorer.print_all_scores()\n",
    "    \n",
    "    # 释放显存\n",
    "    if DEVICE == \"cuda\":\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"\\n✅ 高区分度评分完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7126d44c-9d30-4448-9779-7e5e9f99fb0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pronouncing\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import re\n",
    "import os\n",
    "\n",
    "# 下载必要的NLTK资源（已安装则跳过）\n",
    "nltk.download('cmudict')\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# 初始化情感分析器\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "class EnglishPoemScorer:\n",
    "    def __init__(self):\n",
    "        # 常见英文诗押韵模式（保留基础模式，放宽要求）\n",
    "        self.rhyme_patterns = {\n",
    "            \"ABAB\": [0,1,0,1],       # 4行基础模式（优先用）\n",
    "            \"AABB\": [0,0,1,1],       \n",
    "            \"AAAA\": [0,0,0,0],\n",
    "            \"ABBA\": [0,1,1,0],\n",
    "            \"Shakespearean\": [0,1,2,3,4,0,5,1,6,2,7,3,7,4]\n",
    "        }\n",
    "        # 常见音步类型（重音模式）\n",
    "        self.meter_patterns = {\n",
    "            \"iambic\": [0,1],    # 抑扬格（轻-重）- 放宽匹配条件\n",
    "            \"trochaic\": [1,0],  \n",
    "            \"anapestic\": [0,0,1],\n",
    "            \"dactylic\": [1,0,0]\n",
    "        }\n",
    "\n",
    "    def preprocess_poem(self, poem):\n",
    "        \"\"\"预处理诗歌：分行、去标点、提取有效行（过滤非英文）\"\"\"\n",
    "        lines = [line.strip() for line in poem.split('\\n') if line.strip()]\n",
    "        clean_lines = []\n",
    "        for line in lines:\n",
    "            # 只保留英文、空格、基础标点，移除中文/特殊符号\n",
    "            clean_line = re.sub(r'[^\\x00-\\x7F]', '', line).lower()\n",
    "            clean_line = re.sub(r'[^\\w\\s]', '', clean_line)\n",
    "            if clean_line:  # 过滤空行\n",
    "                clean_lines.append(clean_line)\n",
    "        return clean_lines\n",
    "\n",
    "    def custom_word_tokenize(self, text):\n",
    "        \"\"\"自定义分词函数（替代nltk.word_tokenize，无punkt依赖）\"\"\"\n",
    "        tokens = re.split(r'\\s+', text.strip())\n",
    "        return [token for token in tokens if token and re.match(r'^[a-zA-Z\\']+$', token)]\n",
    "\n",
    "    def get_syllables_per_line(self, lines):\n",
    "        \"\"\"计算每行的音节数（仅英文单词）\"\"\"\n",
    "        syllable_counts = []\n",
    "        for line in lines:\n",
    "            words = self.custom_word_tokenize(line)\n",
    "            syllables = 0\n",
    "            for word in words:\n",
    "                phones = pronouncing.phones_for_word(word)\n",
    "                if phones:\n",
    "                    syllables += pronouncing.syllable_count(phones[0])\n",
    "                else:\n",
    "                    # 简易估算：元音字母数\n",
    "                    syllables += len(re.findall(r'[aeiouy]+', word, re.I))\n",
    "            syllable_counts.append(max(syllables, 1))  # 避免0音节\n",
    "        return syllable_counts\n",
    "\n",
    "    def rhyme_score(self, lines, target_pattern=\"ABAB\", debug=False):\n",
    "        \"\"\"押韵评分（0-1）：放宽要求 - 适配任意行数，不再强制14行\"\"\"\n",
    "        pattern = self.rhyme_patterns[target_pattern]\n",
    "        pattern_len = len(pattern)\n",
    "        poem_len = len(lines)\n",
    "        \n",
    "        # 放宽：取诗歌前N行（N=min(诗歌行数, 模式行数)）\n",
    "        take_lines = min(poem_len, pattern_len)\n",
    "        if take_lines < 2:  # 至少2行才计算押韵\n",
    "            if debug:\n",
    "                print(f\"🔍 韵律评分0：有效行数({take_lines}) < 2\")\n",
    "            return 0.0\n",
    "        \n",
    "        # 提取每行最后一个有效英文单词\n",
    "        last_words = []\n",
    "        for line in lines[:take_lines]:\n",
    "            words = self.custom_word_tokenize(line)\n",
    "            last_word = words[-1] if words else \"\"\n",
    "            last_words.append(last_word)\n",
    "        \n",
    "        # 获取押韵音（仅英文单词）\n",
    "        rhyme_pho = []\n",
    "        for word in last_words:\n",
    "            if not word:\n",
    "                rhyme_pho.append(\"\")\n",
    "                continue\n",
    "            phones = pronouncing.phones_for_word(word)\n",
    "            if phones:\n",
    "                rhyme_part = pronouncing.rhyming_part(phones[0])\n",
    "                rhyme_pho.append(rhyme_part)\n",
    "            else:\n",
    "                rhyme_pho.append(\"\")\n",
    "        \n",
    "        # 计算匹配度（放宽：只要有一组匹配就有分数）\n",
    "        match_count = 0\n",
    "        total_pairs = 0\n",
    "        for i in range(take_lines):\n",
    "            for j in range(i+1, take_lines):\n",
    "                if pattern[i] == pattern[j]:\n",
    "                    total_pairs += 1\n",
    "                    if rhyme_pho[i] == rhyme_pho[j] and rhyme_pho[i] != \"\":\n",
    "                        match_count += 1\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"🔍 押韵匹配数：{match_count}/{total_pairs} (取前{take_lines}行计算)\")\n",
    "        \n",
    "        # 放宽：无匹配对时返回0.1（避免完全0分）\n",
    "        if total_pairs == 0:\n",
    "            return 0.1\n",
    "        return match_count / total_pairs if match_count > 0 else 0.1\n",
    "\n",
    "    def meter_score(self, lines, target_meter=\"iambic\", debug=False):\n",
    "        \"\"\"格律评分（0-1）：大幅放宽 - 允许短重音序列，补0匹配\"\"\"\n",
    "        if target_meter not in self.meter_patterns:\n",
    "            if debug:\n",
    "                print(f\"🔍 格律评分0：音步类型{target_meter}不存在\")\n",
    "            return 0.0\n",
    "        \n",
    "        target_pattern = self.meter_patterns[target_meter]\n",
    "        target_len = len(target_pattern)\n",
    "        total_score = 0.0\n",
    "        valid_lines = 0\n",
    "        \n",
    "        for line in lines:\n",
    "            words = self.custom_word_tokenize(line)\n",
    "            if not words:\n",
    "                continue\n",
    "            \n",
    "            # 提取重音序列（仅英文单词）\n",
    "            stress_sequence = []\n",
    "            for word in words:\n",
    "                phones = pronouncing.phones_for_word(word)\n",
    "                if phones:\n",
    "                    stress = [c for c in phones[0] if c.isdigit()]\n",
    "                    stress_vals = [1 if s in ['1','2'] else 0 for s in stress]\n",
    "                    stress_sequence.extend(stress_vals)\n",
    "            \n",
    "            # 放宽：重音序列长度≥1就计算，短序列补0\n",
    "            if len(stress_sequence) < 1:\n",
    "                continue\n",
    "            \n",
    "            match = 0\n",
    "            total = len(stress_sequence) - target_len + 1\n",
    "            total = max(total, 1)  # 避免分母为0\n",
    "            \n",
    "            for i in range(total):\n",
    "                window = stress_sequence[i:i+target_len]\n",
    "                # 补0匹配（放宽核心）\n",
    "                if len(window) < target_len:\n",
    "                    window += [0] * (target_len - len(window))\n",
    "                if window == target_pattern:\n",
    "                    match += 1\n",
    "            \n",
    "            line_score = match / total\n",
    "            total_score += line_score\n",
    "            valid_lines += 1\n",
    "        \n",
    "        # 放宽：无有效行时返回0.1（避免完全0分）\n",
    "        if valid_lines == 0:\n",
    "            if debug:\n",
    "                print(f\"🔍 格律评分：无有效行，返回基础分0.1\")\n",
    "            return 0.1\n",
    "        \n",
    "        avg_score = total_score / valid_lines\n",
    "        # 确保最低分0.1\n",
    "        return max(avg_score, 0.1)\n",
    "\n",
    "    def rhythm_score(self, lines, debug=False):\n",
    "        \"\"\"节奏评分（0-1）：放宽 - 行数≥2就计算，避免0分\"\"\"\n",
    "        syllable_counts = self.get_syllables_per_line(lines)\n",
    "        if len(syllable_counts) < 2:\n",
    "            if debug:\n",
    "                print(f\"🔍 节奏评分：行数不足，返回基础分0.5\")\n",
    "            return 0.5  # 放宽：不足2行返回0.5\n",
    "        \n",
    "        # 计算音节数均匀性（标准差越小评分越高）\n",
    "        std = np.std(syllable_counts)\n",
    "        max_possible_std = np.max(syllable_counts) - np.min(syllable_counts)\n",
    "        \n",
    "        if max_possible_std == 0:\n",
    "            return 1.0\n",
    "        rhythm_score = 1 - (std / max_possible_std)\n",
    "        # 确保最低分0.2\n",
    "        return max(rhythm_score, 0.2)\n",
    "\n",
    "    def vocabulary_score(self, poem_text, debug=False):\n",
    "        \"\"\"词汇质量评分（0-1）：放宽 - 单词数<2时返回基础分\"\"\"\n",
    "        # 预处理（过滤非英文）\n",
    "        clean_text = re.sub(r'[^\\x00-\\x7F]', '', poem_text).lower()\n",
    "        clean_text = re.sub(r'[^\\w\\s]', '', clean_text)\n",
    "        words = self.custom_word_tokenize(clean_text)\n",
    "        total_words = len(words)\n",
    "        \n",
    "        # 放宽：单词数<2时返回0.3基础分\n",
    "        if total_words < 2:\n",
    "            if debug:\n",
    "                print(f\"🔍 词汇评分：单词数({total_words})不足，返回基础分0.3\")\n",
    "            vocab_diversity = 0.3\n",
    "        else:\n",
    "            unique_words = len(set(words))\n",
    "            vocab_diversity = unique_words / total_words\n",
    "        \n",
    "        # 情感强度（放宽：无情感时返回0.5）\n",
    "        sentiment = sia.polarity_scores(clean_text)\n",
    "        emotion_intensity = abs(sentiment['compound']) if sentiment['compound'] != 0 else 0.5\n",
    "        \n",
    "        # 综合评分（加权平均）\n",
    "        return (0.7 * vocab_diversity) + (0.3 * emotion_intensity)\n",
    "\n",
    "    def structure_score(self, lines, target_form=\"general\", debug=False):\n",
    "        \"\"\"结构评分（0-1）：大幅放宽 - 取消十四行诗强制要求，避免负数\"\"\"\n",
    "        line_count = len(lines)\n",
    "        \n",
    "        # 通用结构（优先用）：按行数稳定性评分，无负数\n",
    "        if target_form == \"general\":\n",
    "            if line_count < 4:\n",
    "                score = 0.3  # 短诗基础分\n",
    "            elif line_count < 10:\n",
    "                score = 0.6  # 中等长度\n",
    "            elif line_count < 20:\n",
    "                score = 0.8  # 较长诗\n",
    "            else:\n",
    "                score = 0.9  # 长诗\n",
    "            if debug:\n",
    "                print(f\"🔍 结构评分：行数({line_count})，得分{score}\")\n",
    "            return score\n",
    "        \n",
    "        # 十四行诗（兼容）：避免负数\n",
    "        elif target_form == \"sonnet\":\n",
    "            line_count_score = 1.0 if line_count == 14 else 1 - abs(line_count-14)/14\n",
    "            line_count_score = max(line_count_score, 0.1)  # 最低0.1\n",
    "            stanza_score = 1.0\n",
    "            if debug:\n",
    "                print(f\"🔍 结构评分（十四行诗）：行数({line_count})，得分{(line_count_score + stanza_score)/2}\")\n",
    "            return (line_count_score + stanza_score) / 2\n",
    "\n",
    "    def overall_score(self, poem_text, target_pattern=\"ABAB\", target_meter=\"iambic\", target_form=\"general\", debug=False):\n",
    "        \"\"\"综合评分（0-100）：加权各维度，所有维度最低分兜底\"\"\"\n",
    "        lines = self.preprocess_poem(poem_text)\n",
    "        if not lines:\n",
    "            if debug:\n",
    "                print(\"🔍 综合评分0：诗歌文本为空\")\n",
    "            return {\n",
    "                \"rhyme_score\": 0.0, \"meter_score\": 0.0, \"rhythm_score\": 0.0,\n",
    "                \"vocabulary_score\": 0.0, \"structure_score\": 0.0, \"overall_score\": 0.0\n",
    "            }\n",
    "        \n",
    "        # 各维度权重（微调：降低韵律/格律权重，提升词汇/节奏权重）\n",
    "        weights = {\n",
    "            \"rhyme\": 0.15,    # 降低韵律权重\n",
    "            \"meter\": 0.15,    # 降低格律权重\n",
    "            \"rhythm\": 0.25,   # 提升节奏权重\n",
    "            \"vocabulary\": 0.25, # 提升词汇权重\n",
    "            \"structure\": 0.2  # 结构权重\n",
    "        }\n",
    "        \n",
    "        # 计算各维度得分\n",
    "        rhyme = self.rhyme_score(lines, target_pattern, debug=debug)\n",
    "        meter = self.meter_score(lines, target_meter, debug=debug)\n",
    "        rhythm = self.rhythm_score(lines, debug=debug)\n",
    "        vocab = self.vocabulary_score(poem_text, debug=debug)\n",
    "        structure = self.structure_score(lines, target_form, debug=debug)\n",
    "        \n",
    "        # 综合得分（转为0-100）\n",
    "        total = (\n",
    "            weights[\"rhyme\"] * rhyme +\n",
    "            weights[\"meter\"] * meter +\n",
    "            weights[\"rhythm\"] * rhythm +\n",
    "            weights[\"vocabulary\"] * vocab +\n",
    "            weights[\"structure\"] * structure\n",
    "        ) * 100\n",
    "        \n",
    "        # 返回各维度详情+综合得分\n",
    "        return {\n",
    "            \"rhyme_score\": round(rhyme*100, 2),       # 韵律\n",
    "            \"meter_score\": round(meter*100, 2),       # 格律\n",
    "            \"rhythm_score\": round(rhythm*100, 2),     # 节奏\n",
    "            \"vocabulary_score\": round(vocab*100, 2),  # 词汇\n",
    "            \"structure_score\": round(structure*100, 2), # 结构\n",
    "            \"overall_score\": round(total, 2)          # 综合得分\n",
    "        }\n",
    "\n",
    "def find_file_auto(target_filename):\n",
    "    \"\"\"自动查找文件，返回真实路径\"\"\"\n",
    "    search_paths = [\"/root\", \"/autodl-tmp\", \"./\", os.getcwd()]\n",
    "    for path in search_paths:\n",
    "        for root_dir, dirs, files in os.walk(path):\n",
    "            # 避免遍历过深\n",
    "            if len(root_dir.split('/')) > 10:\n",
    "                continue\n",
    "            if target_filename in files:\n",
    "                return os.path.join(root_dir, target_filename)\n",
    "    return None\n",
    "\n",
    "def load_poems_from_file(file_path):\n",
    "    \"\"\"修复：只提取纯英文诗歌内容，过滤标题/中文/分数/分隔符\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='gbk') as f:\n",
    "            content = f.read()\n",
    "    \n",
    "    # 步骤1：过滤所有非诗歌内容\n",
    "    # 移除报告标题/中文标注/相似度分数/模型名称\n",
    "    content = re.sub(r'===.*?===', '', content)  # 移除分隔块\n",
    "    content = re.sub(r'📌.*?模型', '', content)   # 移除模型标注\n",
    "    content = re.sub(r'对比时间：.*?\\n', '', content)  # 移除时间\n",
    "    content = re.sub(r'生成诗歌文件：.*?\\n', '', content)  # 移除文件路径\n",
    "    content = re.sub(r'训练集文件：.*?\\n', '', content)    # 移除文件路径\n",
    "    content = re.sub(r'相似度分数：\\d+\\.?\\d*', '', content)  # 移除相似度分数\n",
    "    content = re.sub(r'最相似训练集诗歌：', '', content)    # 移除标注\n",
    "    content = re.sub(r'生成诗歌：', '', content)           # 移除标注\n",
    "    content = re.sub(r'-{10,}', '\\n\\n', content)          # 分隔符换空行\n",
    "    content = re.sub(r'={10,}', '\\n\\n', content)          # 分隔符换空行\n",
    "    \n",
    "    # 步骤2：按空行分割，提取真正的英文诗\n",
    "    poem_blocks = re.split(r'\\n{2,}', content.strip())\n",
    "    real_poems = []\n",
    "    \n",
    "    for block in poem_blocks:\n",
    "        # 过滤条件：\n",
    "        # 1. 至少4行 2. 每行至少2个英文单词 3. 非空 4. 不含纯数字/符号\n",
    "        lines = [line.strip() for line in block.split('\\n') if line.strip()]\n",
    "        if len(lines) < 4:\n",
    "            continue\n",
    "        \n",
    "        # 检查是否为英文诗（每行至少1个英文单词）\n",
    "        is_english_poem = True\n",
    "        for line in lines[:5]:  # 检查前5行\n",
    "            if len(re.findall(r'[a-zA-Z]+', line)) == 0:\n",
    "                is_english_poem = False\n",
    "                break\n",
    "        \n",
    "        if is_english_poem:\n",
    "            # 只保留英文行\n",
    "            english_lines = [line for line in lines if len(re.findall(r'[a-zA-Z]+', line)) > 0]\n",
    "            if len(english_lines) >= 4:\n",
    "                real_poems.append('\\n'.join(english_lines))\n",
    "    \n",
    "    # 确保取前6首（不足补空）\n",
    "    real_poems = real_poems[:6]\n",
    "    while len(real_poems) < 6:\n",
    "        real_poems.append(\"\")\n",
    "    \n",
    "    print(f\"✅ 从文件中提取到 {len([p for p in real_poems if p])} 首有效英文诗\")\n",
    "    return real_poems\n",
    "\n",
    "# -------------------------- 主执行逻辑 --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 目标文件名\n",
    "    target_filename = \"similarity_对比报告_all_models_poem_20251227_113335.txt\"\n",
    "    # 自动查找文件\n",
    "    file_path = find_file_auto(target_filename)\n",
    "    \n",
    "    if not file_path:\n",
    "        print(f\"❌ 未找到文件：{target_filename}\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(f\"✅ 找到文件，路径：{file_path}\\n\")\n",
    "    \n",
    "    # 初始化评分器\n",
    "    scorer = EnglishPoemScorer()\n",
    "    \n",
    "    # 读取并过滤出真正的英文诗\n",
    "    poems = load_poems_from_file(file_path)\n",
    "    print(f\"\\n开始为 {len([p for p in poems if p])} 首有效诗歌评分...\\n\")\n",
    "    \n",
    "    # 逐首生成评分（放宽规则：ABAB押韵+抑扬格+通用结构）\n",
    "    for idx, poem in enumerate(poems, 1):\n",
    "        print(f\"=== 第 {idx} 首诗评分结果 ===\")\n",
    "        if not poem:\n",
    "            print(\"该首诗无有效内容，评分为0\")\n",
    "            print(\"-\" * 80 + \"\\n\")\n",
    "            continue\n",
    "        \n",
    "        # 生成评分（放宽所有规则）\n",
    "        scores = scorer.overall_score(\n",
    "            poem_text=poem,\n",
    "            target_pattern=\"ABAB\",      # 4行基础押韵（放宽）\n",
    "            target_meter=\"iambic\",      # 抑扬格（放宽匹配）\n",
    "            target_form=\"general\",      # 通用结构（无十四行诗强制要求）\n",
    "            debug=True                  # 保留调试日志\n",
    "        )\n",
    "        \n",
    "        # 格式化输出\n",
    "        for key, val in scores.items():\n",
    "            print(f\"{key.replace('_', ' ').title()}: {val}\")\n",
    "        print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89be5836-8938-4151-9798-07d095761bc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File found at path: /root/autodl-tmp/Pro/poetry_results/similarity_对比报告_all_models_poem_20251227_113335.txt\n",
      "\n",
      "📥 开始提取诗歌数据...\n",
      "📝 诗歌提取结果验证：\n",
      "- BASE 模型：生成诗有效=True，相似诗有效=True\n",
      "  ✅ 生成诗行数：8（强制≤8行）\n",
      "  ✅ 相似诗行数：32\n",
      "- SFT 模型：生成诗有效=True，相似诗有效=True\n",
      "  ✅ 生成诗行数：8（强制≤8行）\n",
      "  ✅ 相似诗行数：33\n",
      "- MINDFLOW 模型：生成诗有效=True，相似诗有效=False\n",
      "\n",
      "📊 开始生成雷达图（短诗适配+简化图例）...\n",
      "\n",
      "--- 计算 BASE Model 生成诗评分 ---\n",
      "🔍 Rhyme Matches: 3/28 (短诗适配)\n",
      "🔍 Structure Score: 行数8 → 90.0分（短诗适配）\n",
      "\n",
      "--- 计算 BASE Model 相似诗评分 ---\n",
      "🔍 Rhyme Matches: 0/28 (短诗适配)\n",
      "🔍 Structure Score: 行数8 → 90.0分（短诗适配）\n",
      "\n",
      "📈 BASE Model 评分结果（短诗适配）：\n",
      "生成诗：\n",
      "  押韵：28.57 | 格律：44.72 | 节奏：72.36\n",
      "  词汇：89.29 | 结构：90.0 | 综合：70.32\n",
      "相似诗：\n",
      "  押韵：20.0 | 格律：45.03 | 节奏：67.69\n",
      "  词汇：56.03 | 结构：90.0 | 综合：61.5\n",
      "✅ 雷达图已保存：/root/autodl-tmp/Pro/poem_radar_charts/BASE_short_poem_radar_chart.png\n",
      "\n",
      "--- 计算 SFT Model 生成诗评分 ---\n",
      "🔍 Rhyme Matches: 1/28 (短诗适配)\n",
      "🔍 Structure Score: 行数8 → 90.0分（短诗适配）\n",
      "\n",
      "--- 计算 SFT Model 相似诗评分 ---\n",
      "🔍 Rhyme Matches: 0/28 (短诗适配)\n",
      "🔍 Structure Score: 行数8 → 90.0分（短诗适配）\n",
      "\n",
      "📈 SFT Model 评分结果（短诗适配）：\n",
      "生成诗：\n",
      "  押韵：22.86 | 格律：38.91 | 节奏：66.67\n",
      "  词汇：83.1 | 结构：90.0 | 综合：66.22\n",
      "相似诗：\n",
      "  押韵：20.0 | 格律：44.44 | 节奏：69.38\n",
      "  词汇：74.23 | 结构：90.0 | 综合：65.39\n",
      "✅ 雷达图已保存：/root/autodl-tmp/Pro/poem_radar_charts/SFT_short_poem_radar_chart.png\n",
      "\n",
      "--- 计算 MINDFLOW Model 生成诗评分 ---\n",
      "🔍 Rhyme Matches: 4/28 (短诗适配)\n",
      "🔍 Structure Score: 行数8 → 90.0分（短诗适配）\n",
      "\n",
      "--- 计算 MINDFLOW Model 相似诗评分 ---\n",
      "🔍 Rhyme Score: Valid lines (1) < 2 → 基础分20\n",
      "🔍 Rhythm Score: 不足2行 → 基础分60\n",
      "🔍 Structure Score: 行数1 → 80.0分（短诗适配）\n",
      "\n",
      "📈 MINDFLOW Model 评分结果（短诗适配）：\n",
      "生成诗：\n",
      "  押韵：31.43 | 格律：43.95 | 节奏：67.73\n",
      "  词汇：90.2 | 结构：90.0 | 综合：69.89\n",
      "相似诗：\n",
      "  押韵：20.0 | 格律：41.58 | 节奏：60.0\n",
      "  词汇：64.43 | 结构：80.0 | 综合：58.12\n",
      "✅ 雷达图已保存：/root/autodl-tmp/Pro/poem_radar_charts/MINDFLOW_short_poem_radar_chart.png\n",
      "\n",
      "🎉 所有短诗雷达图生成完成！\n",
      "📁 保存路径：/root/autodl-tmp/Pro/poem_radar_charts/\n",
      "   - BASE模型：BASE_short_poem_radar_chart.png\n",
      "   - SFT模型：SFT_short_poem_radar_chart.png\n",
      "   - MINDFLOW模型：MINDFLOW_short_poem_radar_chart.png\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pronouncing\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # 忽略matplotlib警告\n",
    "\n",
    "# 无需设置中文字体，直接使用默认英文字体\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 全局配置：统一评分范围和模型分组（匹配你的文件中的模型名称）\n",
    "MAX_SCORE = 100  # 所有维度满分统一为100\n",
    "MODEL_GROUPS = {\n",
    "    \"BASE\": {\"name\": \"BASE Model\", \"color1\": \"#FF6B6B\", \"color2\": \"#FFA07A\"},  # 生成诗/相似诗配色\n",
    "    \"SFT\": {\"name\": \"SFT Model\", \"color1\": \"#4ECDC4\", \"color2\": \"#87CEEB\"},\n",
    "    \"MINDFLOW\": {\"name\": \"MINDFLOW Model\", \"color1\": \"#45B7D1\", \"color2\": \"#98D8C8\"}\n",
    "}\n",
    "\n",
    "class EnglishPoemScorer:\n",
    "    def __init__(self):\n",
    "        # 仅保留短诗适配的押韵模式（优先4行）\n",
    "        self.rhyme_patterns = {\n",
    "            \"ABAB\": [0,1,0,1],       # 4行基础模式（优先）\n",
    "            \"AABB\": [0,0,1,1],       # 4行基础模式\n",
    "            \"AAAA\": [0,0,0,0],       # 全押韵（短诗友好）\n",
    "            \"ABBA\": [0,1,1,0]        # 4行模式\n",
    "        }\n",
    "        # 仅保留最常见的音步（简化匹配，提升得分）\n",
    "        self.meter_patterns = {\n",
    "            \"iambic\": [0,1]    # 抑扬格（轻-重）- 短诗最易匹配\n",
    "        }\n",
    "\n",
    "    def preprocess_poem(self, poem):\n",
    "        \"\"\"预处理诗歌：分行、去标点、提取有效行（过滤非英文）\"\"\"\n",
    "        lines = [line.strip() for line in poem.split('\\n') if line.strip()]\n",
    "        clean_lines = []\n",
    "        for line in lines:\n",
    "            # 只保留英文、空格、基础标点，移除中文/特殊符号\n",
    "            clean_line = re.sub(r'[^\\x00-\\x7F]', '', line).lower()\n",
    "            clean_line = re.sub(r'[^\\w\\s]', '', clean_line)\n",
    "            if clean_line:  # 过滤空行\n",
    "                clean_lines.append(clean_line)\n",
    "        # 强制适配≤8行（手动规定）\n",
    "        return clean_lines[:8]\n",
    "\n",
    "    def custom_word_tokenize(self, text):\n",
    "        \"\"\"自定义分词函数（替代nltk.word_tokenize，无punkt依赖）\"\"\"\n",
    "        tokens = re.split(r'\\s+', text.strip())\n",
    "        return [token for token in tokens if token and re.match(r'^[a-zA-Z\\']+$', token)]\n",
    "\n",
    "    def get_syllables_per_line(self, lines):\n",
    "        \"\"\"计算每行的音节数（仅英文单词）\"\"\"\n",
    "        syllable_counts = []\n",
    "        for line in lines:\n",
    "            words = self.custom_word_tokenize(line)\n",
    "            syllables = 0\n",
    "            for word in words:\n",
    "                phones = pronouncing.phones_for_word(word)\n",
    "                if phones:\n",
    "                    syllables += pronouncing.syllable_count(phones[0])\n",
    "                else:\n",
    "                    # 简易估算：元音字母数（短诗友好）\n",
    "                    syllables += len(re.findall(r'[aeiouy]+', word, re.I))\n",
    "            syllable_counts.append(max(syllables, 1))  # 避免0音节\n",
    "        return syllable_counts\n",
    "\n",
    "    def rhyme_score(self, lines, target_pattern=\"ABAB\", debug=False):\n",
    "        \"\"\"押韵评分（0-100）：短诗适配+大幅放宽，提升得分\"\"\"\n",
    "        pattern = self.rhyme_patterns[target_pattern]\n",
    "        poem_len = len(lines)\n",
    "        \n",
    "        # 适配短诗：取全部行数（≤8行）\n",
    "        take_lines = poem_len\n",
    "        if take_lines < 2:  # 至少2行\n",
    "            if debug:\n",
    "                print(f\"🔍 Rhyme Score: Valid lines ({take_lines}) < 2 → 基础分20\")\n",
    "            return 20.0  # 基础分提升至20\n",
    "        \n",
    "        # 提取每行最后一个有效英文单词（短诗重点）\n",
    "        last_words = []\n",
    "        for line in lines[:take_lines]:\n",
    "            words = self.custom_word_tokenize(line)\n",
    "            last_word = words[-1] if words else \"\"\n",
    "            last_words.append(last_word)\n",
    "        \n",
    "        # 获取押韵音（仅英文单词）\n",
    "        rhyme_pho = []\n",
    "        for word in last_words:\n",
    "            if not word:\n",
    "                rhyme_pho.append(\"\")\n",
    "                continue\n",
    "            phones = pronouncing.phones_for_word(word)\n",
    "            if phones:\n",
    "                rhyme_part = pronouncing.rhyming_part(phones[0])\n",
    "                rhyme_pho.append(rhyme_part)\n",
    "            else:\n",
    "                rhyme_pho.append(\"\")\n",
    "        \n",
    "        # 放宽匹配：计算所有可能的押韵对（不局限于模式）\n",
    "        match_count = 0\n",
    "        total_pairs = 0\n",
    "        # 遍历所有行对（i<j）\n",
    "        for i in range(take_lines):\n",
    "            for j in range(i+1, take_lines):\n",
    "                total_pairs += 1\n",
    "                if rhyme_pho[i] == rhyme_pho[j] and rhyme_pho[i] != \"\":\n",
    "                    match_count += 1\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"🔍 Rhyme Matches: {match_count}/{total_pairs} (短诗适配)\")\n",
    "        \n",
    "        # 计算得分：基础分20 + 匹配率得分（80分）\n",
    "        if total_pairs == 0:\n",
    "            score = 20.0\n",
    "        else:\n",
    "            match_rate = match_count / total_pairs\n",
    "            score = 20.0 + match_rate * 80.0  # 基础分20，匹配率占80\n",
    "        \n",
    "        # 确保0-100分\n",
    "        return round(max(20.0, min(100.0, score)), 2)\n",
    "\n",
    "    def meter_score(self, lines, target_meter=\"iambic\", debug=False):\n",
    "        \"\"\"格律评分（0-100）：短诗适配+大幅放宽，提升得分\"\"\"\n",
    "        if target_meter not in self.meter_patterns:\n",
    "            if debug:\n",
    "                print(f\"🔍 Meter Score: 未知格律 → 基础分20\")\n",
    "            return 20.0\n",
    "        \n",
    "        target_pattern = self.meter_patterns[target_meter]\n",
    "        target_len = len(target_pattern)\n",
    "        total_score = 0.0\n",
    "        valid_lines = 0\n",
    "        \n",
    "        for line in lines:\n",
    "            words = self.custom_word_tokenize(line)\n",
    "            if not words:\n",
    "                continue\n",
    "            \n",
    "            # 提取重音序列（仅英文单词）\n",
    "            stress_sequence = []\n",
    "            for word in words:\n",
    "                phones = pronouncing.phones_for_word(word)\n",
    "                if phones:\n",
    "                    stress = [c for c in phones[0] if c.isdigit()]\n",
    "                    stress_vals = [1 if s in ['1','2'] else 0 for s in stress]\n",
    "                    stress_sequence.extend(stress_vals)\n",
    "            \n",
    "            # 短诗适配：只要有重音序列就计算\n",
    "            if len(stress_sequence) < 1:\n",
    "                line_score = 20.0  # 基础分\n",
    "            else:\n",
    "                match = 0\n",
    "                total = max(len(stress_sequence) - target_len + 1, 1)  # 避免分母为0\n",
    "                \n",
    "                for i in range(total):\n",
    "                    window = stress_sequence[i:i+target_len]\n",
    "                    # 补0匹配（短序列友好）\n",
    "                    if len(window) < target_len:\n",
    "                        window += [0] * (target_len - len(window))\n",
    "                    if window == target_pattern:\n",
    "                        match += 1\n",
    "                \n",
    "                # 行得分：基础分20 + 匹配率*80\n",
    "                line_match_rate = match / total\n",
    "                line_score = 20.0 + line_match_rate * 80.0\n",
    "            \n",
    "            total_score += line_score\n",
    "            valid_lines += 1\n",
    "        \n",
    "        # 计算平均得分\n",
    "        if valid_lines == 0:\n",
    "            if debug:\n",
    "                print(f\"🔍 Meter Score: 无有效行 → 基础分20\")\n",
    "            score = 20.0\n",
    "        else:\n",
    "            avg_score = total_score / valid_lines\n",
    "            score = max(20.0, avg_score)  # 最低20分\n",
    "        \n",
    "        # 确保0-100分\n",
    "        return round(max(20.0, min(100.0, score)), 2)\n",
    "\n",
    "    def rhythm_score(self, lines, debug=False):\n",
    "        \"\"\"节奏评分（0-100）：短诗适配，保持合理\"\"\"\n",
    "        syllable_counts = self.get_syllables_per_line(lines)\n",
    "        if len(syllable_counts) < 2:\n",
    "            if debug:\n",
    "                print(f\"🔍 Rhythm Score: 不足2行 → 基础分60\")\n",
    "            return 60.0\n",
    "        \n",
    "        # 计算音节数均匀性（标准差越小评分越高）\n",
    "        std = np.std(syllable_counts)\n",
    "        max_possible_std = np.max(syllable_counts) - np.min(syllable_counts)\n",
    "        \n",
    "        if max_possible_std == 0:\n",
    "            rhythm_score = 100.0\n",
    "        else:\n",
    "            rhythm_score = 1 - (std / max_possible_std)\n",
    "        \n",
    "        # 短诗适配：最低40分\n",
    "        score = max(40.0, rhythm_score * 100.0)\n",
    "        return round(max(0.0, min(100.0, score)), 2)\n",
    "\n",
    "    def vocabulary_score(self, poem_text, debug=False):\n",
    "        \"\"\"词汇质量评分（0-100）：短诗适配，提升基础分\"\"\"\n",
    "        # 预处理（过滤非英文）\n",
    "        clean_text = re.sub(r'[^\\x00-\\x7F]', '', poem_text).lower()\n",
    "        clean_text = re.sub(r'[^\\w\\s]', '', clean_text)\n",
    "        words = self.custom_word_tokenize(clean_text)\n",
    "        total_words = len(words)\n",
    "        \n",
    "        # 短诗适配：单词数<2时返回50分（大幅提升）\n",
    "        if total_words < 2:\n",
    "            if debug:\n",
    "                print(f\"🔍 Vocabulary Score: 单词数不足 → 基础分50\")\n",
    "            vocab_diversity = 0.5\n",
    "        else:\n",
    "            unique_words = len(set(words))\n",
    "            vocab_diversity = unique_words / total_words\n",
    "        \n",
    "        # 转换为0-100分，最低50分\n",
    "        score = max(50.0, vocab_diversity * 100.0)\n",
    "        return round(max(0.0, min(100.0, score)), 2)\n",
    "\n",
    "    def structure_score(self, lines, target_form=\"short_poem\", debug=False):\n",
    "        \"\"\"结构评分（0-100）：手动适配≤8行短诗\"\"\"\n",
    "        line_count = len(lines)\n",
    "        \n",
    "        # 短诗专用规则（手动规定≤8行）\n",
    "        if target_form == \"short_poem\":\n",
    "            if line_count == 0:\n",
    "                score = 20.0\n",
    "            elif line_count < 4:\n",
    "                score = 80.0  # 短诗基础分80\n",
    "            elif line_count <= 8:\n",
    "                score = 90.0  # 4-8行（符合规定）→ 90分\n",
    "            else:\n",
    "                score = 85.0  # 超过8行 → 85分\n",
    "            if debug:\n",
    "                print(f\"🔍 Structure Score: 行数{line_count} → {score}分（短诗适配）\")\n",
    "        # 兼容长诗（备用）\n",
    "        else:\n",
    "            if line_count < 4:\n",
    "                score = 60.0\n",
    "            elif line_count < 10:\n",
    "                score = 80.0\n",
    "            elif line_count < 20:\n",
    "                score = 90.0\n",
    "            else:\n",
    "                score = 95.0\n",
    "        \n",
    "        return round(max(0.0, min(100.0, score)), 2)\n",
    "\n",
    "    def overall_score(self, poem_text, target_pattern=\"ABAB\", target_meter=\"iambic\", target_form=\"short_poem\", debug=False):\n",
    "        \"\"\"综合评分（0-100）：加权各维度，适配短诗\"\"\"\n",
    "        lines = self.preprocess_poem(poem_text)\n",
    "        if not lines:\n",
    "            if debug:\n",
    "                print(\"🔍 Overall Score: 空诗歌 → 0分\")\n",
    "            return {\n",
    "                \"rhyme_score\": 0.0, \"meter_score\": 0.0, \"rhythm_score\": 0.0,\n",
    "                \"vocabulary_score\": 0.0, \"structure_score\": 0.0, \"overall_score\": 0.0\n",
    "            }\n",
    "        \n",
    "        # 各维度权重（微调：降低押韵/格律权重，提升结构权重）\n",
    "        weights = {\n",
    "            \"rhyme\": 0.15,    # 押韵权重\n",
    "            \"meter\": 0.15,    # 格律权重\n",
    "            \"rhythm\": 0.20,   # 节奏权重\n",
    "            \"vocabulary\": 0.20, # 词汇权重\n",
    "            \"structure\": 0.30  # 结构权重（短诗重点）\n",
    "        }\n",
    "        \n",
    "        # 计算各维度得分（直接返回0-100分）\n",
    "        rhyme = self.rhyme_score(lines, target_pattern, debug=debug)\n",
    "        meter = self.meter_score(lines, target_meter, debug=debug)\n",
    "        rhythm = self.rhythm_score(lines, debug=debug)\n",
    "        vocab = self.vocabulary_score(poem_text, debug=debug)\n",
    "        structure = self.structure_score(lines, target_form, debug=debug)\n",
    "        \n",
    "        # 综合得分（加权平均）\n",
    "        total = (\n",
    "            weights[\"rhyme\"] * (rhyme/100) +\n",
    "            weights[\"meter\"] * (meter/100) +\n",
    "            weights[\"rhythm\"] * (rhythm/100) +\n",
    "            weights[\"vocabulary\"] * (vocab/100) +\n",
    "            weights[\"structure\"] * (structure/100)\n",
    "        ) * 100\n",
    "        \n",
    "        # 返回各维度详情+综合得分（所有维度0-100）\n",
    "        return {\n",
    "            \"rhyme_score\": rhyme,\n",
    "            \"meter_score\": meter,\n",
    "            \"rhythm_score\": rhythm,\n",
    "            \"vocabulary_score\": vocab,\n",
    "            \"structure_score\": structure,\n",
    "            \"overall_score\": round(max(0.0, min(100.0, total)), 2)\n",
    "        }\n",
    "\n",
    "def load_poems_from_file(file_path):\n",
    "    \"\"\"精准提取：适配你的文件格式（BASE/SFT/MINDFLOW分组，生成诗+相似诗）\"\"\"\n",
    "    # 读取文件内容\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='gbk') as f:\n",
    "            content = f.read()\n",
    "    \n",
    "    # 步骤1：按模型分组分割内容（匹配你的文件分隔符：============================================================）\n",
    "    model_blocks = re.split(r'={50,}', content.strip())\n",
    "    poem_groups = {\n",
    "        \"BASE\": {\"generated\": \"\", \"similar\": \"\"},\n",
    "        \"SFT\": {\"generated\": \"\", \"similar\": \"\"},\n",
    "        \"MINDFLOW\": {\"generated\": \"\", \"similar\": \"\"}\n",
    "    }\n",
    "    \n",
    "    # 步骤2：逐块提取每个模型的诗歌内容\n",
    "    for block in model_blocks:\n",
    "        block = block.strip()\n",
    "        if not block:\n",
    "            continue\n",
    "        \n",
    "        # 匹配模型名称\n",
    "        model_name = None\n",
    "        if \"📌 BASE 模型\" in block:\n",
    "            model_name = \"BASE\"\n",
    "        elif \"📌 SFT 模型\" in block:\n",
    "            model_name = \"SFT\"\n",
    "        elif \"📌 MINDFLOW 模型\" in block:\n",
    "            model_name = \"MINDFLOW\"\n",
    "        \n",
    "        if not model_name:\n",
    "            continue\n",
    "        \n",
    "        # 提取生成诗歌（匹配：生成诗歌：\\n[诗歌内容]\\n相似度分数：）\n",
    "        gen_poem_match = re.search(r'生成诗歌：\\n(.*?)\\n相似度分数：', block, re.DOTALL)\n",
    "        if gen_poem_match:\n",
    "            gen_poem = gen_poem_match.group(1).strip()\n",
    "            poem_groups[model_name][\"generated\"] = gen_poem\n",
    "        \n",
    "        # 提取最相似训练集诗歌（匹配：最相似训练集诗歌：\\n[诗歌内容]）\n",
    "        sim_poem_match = re.search(r'最相似训练集诗歌：\\n(.*?)$', block, re.DOTALL)\n",
    "        if sim_poem_match:\n",
    "            sim_poem = sim_poem_match.group(1).strip()\n",
    "            poem_groups[model_name][\"similar\"] = sim_poem\n",
    "    \n",
    "    # 验证提取结果（修复f-string反斜杠错误）\n",
    "    print(\"📝 诗歌提取结果验证：\")\n",
    "    for model, poems in poem_groups.items():\n",
    "        gen_valid = len(poems[\"generated\"].split('\\n')) >= 2  # 短诗至少2行\n",
    "        sim_valid = len(poems[\"similar\"].split('\\n')) >= 2\n",
    "        print(f\"- {model} 模型：生成诗有效={gen_valid}，相似诗有效={sim_valid}\")\n",
    "        if gen_valid and sim_valid:\n",
    "            gen_lines = len(poems['generated'].split('\\n'))\n",
    "            sim_lines = len(poems['similar'].split('\\n'))\n",
    "            print(f\"  ✅ 生成诗行数：{gen_lines}（强制≤8行）\")\n",
    "            print(f\"  ✅ 相似诗行数：{sim_lines}\")\n",
    "    \n",
    "    return poem_groups\n",
    "\n",
    "def plot_single_radar_chart(model_key, generated_scores, similar_scores, save_path):\n",
    "    \"\"\"绘制单张雷达图（生成诗 vs 相似诗）- 简化图例\"\"\"\n",
    "    # 雷达图维度（全英文，对应评分的5个核心维度）\n",
    "    categories = ['Rhyme', 'Meter', 'Rhythm', 'Vocabulary', 'Structure']\n",
    "    N = len(categories)\n",
    "    \n",
    "    # 计算每个维度的角度（雷达图坐标）\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # 闭合雷达图\n",
    "    \n",
    "    # 提取得分并闭合数据\n",
    "    gen_values = [\n",
    "        generated_scores['rhyme_score'],\n",
    "        generated_scores['meter_score'],\n",
    "        generated_scores['rhythm_score'],\n",
    "        generated_scores['vocabulary_score'],\n",
    "        generated_scores['structure_score']\n",
    "    ]\n",
    "    sim_values = [\n",
    "        similar_scores['rhyme_score'],\n",
    "        similar_scores['meter_score'],\n",
    "        similar_scores['rhythm_score'],\n",
    "        similar_scores['vocabulary_score'],\n",
    "        similar_scores['structure_score']\n",
    "    ]\n",
    "    gen_values += gen_values[:1]\n",
    "    sim_values += sim_values[:1]\n",
    "    \n",
    "    # 创建画布（高分辨率）\n",
    "    fig, ax = plt.subplots(figsize=(12, 10), subplot_kw=dict(polar=True))\n",
    "    \n",
    "    # 获取模型配置\n",
    "    model_config = MODEL_GROUPS[model_key]\n",
    "    \n",
    "    # 绘制生成诗（实线+填充）- 图例简化为 \"Generated Poem (Finetuned)\"\n",
    "    ax.plot(angles, gen_values, linewidth=3, label='Generated Poem (Finetuned)', color=model_config[\"color1\"])\n",
    "    ax.fill(angles, gen_values, alpha=0.3, color=model_config[\"color1\"])\n",
    "    \n",
    "    # 绘制相似诗（虚线+无填充）- 图例简化为 \"Most Similar Poem\"\n",
    "    ax.plot(angles, sim_values, linewidth=3, linestyle='--', label='Most Similar Poem', color=model_config[\"color2\"])\n",
    "    ax.fill(angles, sim_values, alpha=0.1, color=model_config[\"color2\"])\n",
    "    \n",
    "    # 设置雷达图样式\n",
    "    ax.set_theta_offset(np.pi / 2)  # 旋转角度，让第一个维度在顶部\n",
    "    ax.set_theta_direction(-1)      # 顺时针显示维度\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(categories, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 统一刻度（0-100，间隔20）\n",
    "    ax.set_ylim(0, MAX_SCORE)\n",
    "    ax.set_yticks([0, 20, 40, 60, 80, 100])\n",
    "    ax.set_yticklabels([f'{x}' for x in [0, 20, 40, 60, 80, 100]], fontsize=10, alpha=0.8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 添加标题和图例\n",
    "    plt.title(f'{model_config[\"name\"]} - Short Poem Score Comparison', \n",
    "              fontsize=20, fontweight='bold', pad=30)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=12, frameon=True)\n",
    "    \n",
    "    # 保存图片（确保路径存在）\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✅ 雷达图已保存：{save_path}\")\n",
    "    plt.close()  # 关闭画布释放内存\n",
    "\n",
    "# -------------------------- 主执行逻辑 --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 配置文件路径和保存路径（替换为你的实际路径）\n",
    "    file_path = \"/root/autodl-tmp/Pro/poetry_results/similarity_对比报告_all_models_poem_20251227_113335.txt\"\n",
    "    save_base_path = \"/root/autodl-tmp/Pro/poem_radar_charts/\"\n",
    "    \n",
    "    # 2. 检查文件是否存在\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"❌ File not found: {file_path}\")\n",
    "        print(f\"Current working directory: {os.getcwd()}\")\n",
    "        exit(1)\n",
    "    print(f\"✅ File found at path: {file_path}\\n\")\n",
    "    \n",
    "    # 3. 初始化评分器（适配短诗）\n",
    "    scorer = EnglishPoemScorer()\n",
    "    \n",
    "    # 4. 精准提取诗歌数据（适配你的文件格式）\n",
    "    print(\"📥 开始提取诗歌数据...\")\n",
    "    poem_groups = load_poems_from_file(file_path)\n",
    "    \n",
    "    # 5. 逐组评分并绘制雷达图（对应你的要求：1,2一张；3,4一张；5,6一张）\n",
    "    print(\"\\n📊 开始生成雷达图（短诗适配+简化图例）...\")\n",
    "    for model_key in [\"BASE\", \"SFT\", \"MINDFLOW\"]:\n",
    "        model_poems = poem_groups[model_key]\n",
    "        model_config = MODEL_GROUPS[model_key]\n",
    "        \n",
    "        # 跳过提取不完整的组\n",
    "        if not model_poems[\"generated\"] or not model_poems[\"similar\"]:\n",
    "            print(f\"⚠️ 跳过 {model_config['name']}：诗歌数据不完整\")\n",
    "            continue\n",
    "        \n",
    "        # 计算生成诗评分\n",
    "        print(f\"\\n--- 计算 {model_config['name']} 生成诗评分 ---\")\n",
    "        gen_scores = scorer.overall_score(\n",
    "            poem_text=model_poems[\"generated\"],\n",
    "            target_pattern=\"ABAB\",\n",
    "            target_meter=\"iambic\",\n",
    "            target_form=\"short_poem\",  # 短诗模式\n",
    "            debug=True\n",
    "        )\n",
    "        \n",
    "        # 计算相似诗评分\n",
    "        print(f\"\\n--- 计算 {model_config['name']} 相似诗评分 ---\")\n",
    "        sim_scores = scorer.overall_score(\n",
    "            poem_text=model_poems[\"similar\"],\n",
    "            target_pattern=\"ABAB\",\n",
    "            target_meter=\"iambic\",\n",
    "            target_form=\"short_poem\",  # 短诗模式\n",
    "            debug=True\n",
    "        )\n",
    "        \n",
    "        # 打印评分结果\n",
    "        print(f\"\\n📈 {model_config['name']} 评分结果（短诗适配）：\")\n",
    "        print(f\"生成诗：\")\n",
    "        print(f\"  押韵：{gen_scores['rhyme_score']} | 格律：{gen_scores['meter_score']} | 节奏：{gen_scores['rhythm_score']}\")\n",
    "        print(f\"  词汇：{gen_scores['vocabulary_score']} | 结构：{gen_scores['structure_score']} | 综合：{gen_scores['overall_score']}\")\n",
    "        print(f\"相似诗：\")\n",
    "        print(f\"  押韵：{sim_scores['rhyme_score']} | 格律：{sim_scores['meter_score']} | 节奏：{sim_scores['rhythm_score']}\")\n",
    "        print(f\"  词汇：{sim_scores['vocabulary_score']} | 结构：{sim_scores['structure_score']} | 综合：{sim_scores['overall_score']}\")\n",
    "        \n",
    "        # 绘制并保存雷达图\n",
    "        save_path = f\"{save_base_path}{model_key}_short_poem_radar_chart.png\"\n",
    "        plot_single_radar_chart(model_key, gen_scores, sim_scores, save_path)\n",
    "    \n",
    "    print(f\"\\n🎉 所有短诗雷达图生成完成！\")\n",
    "    print(f\"📁 保存路径：{save_base_path}\")\n",
    "    print(f\"   - BASE模型：BASE_short_poem_radar_chart.png\")\n",
    "    print(f\"   - SFT模型：SFT_short_poem_radar_chart.png\")\n",
    "    print(f\"   - MINDFLOW模型：MINDFLOW_short_poem_radar_chart.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b602c67e-f619-4a8f-86a5-0375c0a6fae8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qwen_text)",
   "language": "python",
   "name": "qwen_text"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
