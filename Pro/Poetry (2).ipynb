{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f1d235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æŸ¥ Arrow æ•°æ®é›†çš„åˆ—å\n",
    "from datasets import load_from_disk\n",
    "\n",
    "dataset_path = r\"D:/æ¡Œé¢/DL/Pro/poetry_foundation\"\n",
    "dataset = load_from_disk(dataset_path)\n",
    "train_dataset = dataset[\"train\"]\n",
    "\n",
    "print(train_dataset.column_names)  # æŸ¥çœ‹çœŸå®åˆ—å\n",
    "print(train_dataset[0])  # æŸ¥çœ‹ç¬¬ä¸€æ¡è®°å½•\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "989acfbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== å‰ 15 æ¡ SFT æ ·æœ¬ ===\n",
      "\n",
      "--- SFT æ ·æœ¬ 1 ---\n",
      "user: Write a poem in the style of Michelle Menting.\n",
      "\n",
      "assistant: Dog bone, stapler,\n",
      "cribbage board, garlic press\n",
      "     because this window is looseâ€”lacks\n",
      "suction, lacks grip.\n",
      "Bungee cord, bootstrap,\n",
      "dog leash, leather belt\n",
      "     because this window had sash cords.\n",
      "They frayed. They broke.\n",
      "Feather duster, thatch of straw, empty\n",
      "bottle of Elmer's glue\n",
      "     because this window is loudâ€”its hinges clack\n",
      "open, clack shut.\n",
      "Stuffed bear, baby blanket,\n",
      "single crib newel\n",
      "     because this window is split. It's dividing\n",
      "in two.\n",
      "Velvet moss, sagebrush,\n",
      "willow branch, robin's wing\n",
      "     because this window, it's pane-less. It's only\n",
      "a frame of air.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "--- SFT æ ·æœ¬ 2 ---\n",
      "user: Write a poem in the style of Lucia Cherciu.\n",
      "\n",
      "assistant: The old cupola glinted above the clouds, shone\n",
      "among fir trees, but it took him an hour\n",
      "for the half mile all the way up the hill. As he trailed,\n",
      "the village passed him by, greeted him,\n",
      "asked about his health, but everybody hurried\n",
      "to catch the mass, left him leaning against fences,\n",
      "measuring the road with the walking stick he sculpted.\n",
      "He yearned for the day when the new church\n",
      "would be builtâ€”right across the road. Now\n",
      "it rises above the moon: saints in frescoes\n",
      "meet the eye, and only the rain has started to cut\n",
      "through the shingles on the roof of his empty\n",
      "house. The apple trees have taken over the sky,\n",
      "sequestered the gate, sidled over the porch.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "--- SFT æ ·æœ¬ 3 ---\n",
      "user: Write a poem in the style of Ted Kooser.\n",
      "\n",
      "assistant: Look for me under the hood\n",
      "of that old Chevrolet settled in weeds\n",
      "at the end of the pasture.\n",
      "I'm the radiator that spent its years\n",
      "bolted in front of an engine\n",
      "shoving me forward into the wind.\n",
      "Whatever was in me in those days\n",
      "has mostly leaked away,\n",
      "but my cap's still screwed on tight\n",
      "and I know the names of all these\n",
      "tattered moths and broken grasshoppers\n",
      "the rest of you've forgotten.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "--- SFT æ ·æœ¬ 4 ---\n",
      "user: Write a poem in the style of Grace Cavalieri.\n",
      "\n",
      "assistant: Behind the silo, the Mother Rabbit\n",
      "hunches like a giant spider with strange calm:\n",
      "six tiny babies beneath, each\n",
      "clamoring for a sweet syringe of milk.\n",
      "This may sound cute to you, reading\n",
      "from your pulpit of plenty,\n",
      "but one small one was left out of reach,\n",
      "a knife of fur\n",
      "barging between the others.\n",
      "I watched behind a turret of sand. If\n",
      "I could have cautioned the mother rabbit\n",
      "I would. If I could summon the\n",
      "Bunnies to fit him in beneath\n",
      "the belly's swell\n",
      "I would. But instead, I stood frozen, wishing\n",
      "for some equity. This must be\n",
      "why it's called Wild Life because of all the\n",
      "crazed emotions tangled up in\n",
      "the underbrush within us.\n",
      "Did I tell you how\n",
      "the smallest one, black and trembling,\n",
      "hopped behind the kudzu\n",
      "still filigreed with wanting?\n",
      "Should we talk now of animal heritage, their species,\n",
      "creature development? And what do we say\n",
      "about form and focusâ€”\n",
      "writing this when a stray goes hungry, and away.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "--- SFT æ ·æœ¬ 5 ---\n",
      "user: Write a poem in the style of Connie Wanek.\n",
      "\n",
      "assistant: When I push your button\n",
      "you fly off the handle,\n",
      "old skin and bones,\n",
      "black bat wing.\n",
      "We're alike, you and I.\n",
      "Both of us\n",
      "resemble my mother,\n",
      "so fierce in her advocacy\n",
      "on behalf of\n",
      "the most vulnerable child\n",
      "who'll catch his death\n",
      "in this tempest.\n",
      "Such a headwind!\n",
      "Sometimes it requires\n",
      "all my strength\n",
      "just to end a line.\n",
      "But when the wind is at\n",
      "my back, we're likely\n",
      "to get carried away, and say\n",
      "something we can never retract,\n",
      "something saturated from the ribs\n",
      "down, an old stony\n",
      "word like ruin. You're what roof\n",
      "I have, frail thing,\n",
      "you're my argument\n",
      "against the whole sky.\n",
      "You're the fundamental difference\n",
      "between wet and dry.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "--- SFT æ ·æœ¬ 6 ---\n",
      "user: Write a poem in the style of January Gill O'Neil.\n",
      "\n",
      "assistant: You are the start of the week\n",
      "or the end of it, and according\n",
      "to The Beatles you creep in\n",
      "like a nun. You're the second\n",
      "full day the kids have been\n",
      "away with their father, the second\n",
      "full day of an empty house.\n",
      "Sunday, I've missed you. I've been\n",
      "sitting in the backyard with a glass\n",
      "of Pinot waiting for your arrival.\n",
      "Did you know the first Sweet 100s\n",
      "are turning red in the garden,\n",
      "but the lettuce has grown\n",
      "too bitter to eat. I am looking\n",
      "up at the bluest sky I have ever seen,\n",
      "cerulean blue, a heaven sky\n",
      "no one would believe I was under.\n",
      "You are my witness. No day\n",
      "is promised. You are absolution.\n",
      "You are my unwritten to-do list,\n",
      "my dishes in the sink, my brownie\n",
      "breakfast, my braless day.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "--- SFT æ ·æœ¬ 7 ---\n",
      "user: Write a poem using the theme: Living,Time & Brevity,Relationships,Family & Ancestors,Nature,Landscapes & Pastorals,Seas, Rivers, & Streams,Social Commentaries,History & Politics, in the style of Joy Harjo.\n",
      "\n",
      "assistant: Invisible fish swim this ghost ocean now described by waves of sand, by water-worn rock. Soon the fish will learn to walk. Then humans will come ashore and paint dreams on the dying stone. Then later, much later, the ocean floor will be punctuated by Chevy trucks, carrying the dreamersâ€™ decendants, who are going to the store.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "--- SFT æ ·æœ¬ 8 ---\n",
      "user: Write a poem using the theme: Religion,The Spiritual,Mythology & Folklore,Fairy-tales & Legends, in the style of Joy Harjo.\n",
      "\n",
      "assistant: Donâ€™t bother the earth spirit who lives here. She is working on a story. It is the oldest story in the world and it is delicate, changing. If she sees you watching she will invite you in for coffee, give you warm bread, and you will be obligated to stay and listen. But this is no ordinary story. You will have to endure earthquakes, lightning, the deaths of all those you love, the most blinding beauty. Itâ€™s a story so compelling you may never want to leave; this is how she traps you. See that stone finger over there? That is the only one who ever escaped.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "--- SFT æ ·æœ¬ 9 ---\n",
      "user: Write a poem in the style of John Ashbery.\n",
      "\n",
      "assistant: Is anything central?\n",
      "Orchards flung out on the land,\n",
      "Urban forests, rustic plantations, knee-high hills?\n",
      "Are place names central?\n",
      "Elm Grove, Adcock Corner, Story Book Farm?\n",
      "As they concur with a rush at eye level\n",
      "Beating themselves into eyes which have had enough\n",
      "Thank you, no more thank you.\n",
      "And they come on like scenery mingled with darkness\n",
      "The damp plains, overgrown suburbs,\n",
      "Places of known civic pride, of civil obscurity.\n",
      " \n",
      "These are connected to my version of America\n",
      "But the juice is elsewhere.\n",
      "This morning as I walked out of your room\n",
      "After breakfast crosshatched with\n",
      "Backward and forward glances, backward into light,\n",
      "Forward into unfamiliar light,\n",
      "Was it our doing, and was it\n",
      "The material, the lumber of life, or of lives\n",
      "We were measuring, counting?\n",
      "A mood soon to be forgotten\n",
      "In crossed girders of light, cool downtown shadow\n",
      "In this morning that has seized us again?\n",
      " \n",
      "I know that I braid too much on my own\n",
      "Snapped-off perceptions of things as they come to me.\n",
      "They are private and always will be.\n",
      "Where then are the private turns of event\n",
      "Destined to bloom later like golden chimes\n",
      "Released over a city from a highest tower?\n",
      "The quirky things that happen to me, and I tell you,\n",
      "And you know instantly what I mean?\n",
      "What remote orchard reached by winding roads\n",
      "Hides them? Where are these roots?\n",
      " \n",
      "It is the lumps and trials\n",
      "That tell us whether we shall be known\n",
      "And whether our fate can be exemplary, like a star.\n",
      "All the rest is waiting\n",
      "For a letter that never arrives,\n",
      "Day after day, the exasperation\n",
      "Until finally you have ripped it open not knowing what it is,\n",
      "The two envelope halves lying on a plate.\n",
      "The message was wise, and seemingly\n",
      "Dictated a long time ago, but its time has still\n",
      "Not arrived, telling of danger, and the mostly limited\n",
      "Steps that can be taken against danger\n",
      "Now and in the future, in cool yards,\n",
      "In quiet small houses in the country,\n",
      "Our country, in fenced areas, in cool shady streets.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "--- SFT æ ·æœ¬ 10 ---\n",
      "user: Write a poem using the theme: Living,Parenthood,The Body,The Mind,Nature,Trees & Flowers, in the style of Simone White.\n",
      "\n",
      "assistant: Hour in which I consider hydrangea, a salt or sand plant, varietal, the question of varietals, the diet of every mother I know, 5 pounds feels like 20, I have lost â€¦ I have lost, yes, a sense of my own possible beauty, grown external, I externalize beauty. Beauty occurs on the surface of plants; the sun darkens the skin of my child, he is so small, he is beautiful (I can see; it is obvious) and everything about him is beautiful. His hand swells from the bite [spread?] of some insect[â€™s] venom because he is small. He appears to feel nothing. He smashes his skull against the floor. He screams. I hold him in my lap on the kitchen floor in front of an open freezer, pressing a pack of frozen clay against his forehead. He likes the cold. I see; it is so obvious. Hydrangea. When I move, when I walk pushing my childâ€™s stroller (it is both walking and pushing or hauling, sometimes, also, lifting; it is having another body, an adjunct body composed of errand and weight and tenderness and no small amount of power), I imagine I can feel this small amount of weight, this 5 pounds like 20, interfering with the twitch of every muscle in my body. As an object, a mother is confusing, a middle-aged mother with little spare flesh, I feel every inch of major muscle pulling against gravity and against the weight of my child, now sleeping. This is the hour for thinking hydrangea. Let no man look at me. I stop to brush the drowsy childâ€™s little eye. His face. He barely considers his mother. I am all around him. Why should he consider what is all around him? Perhaps what is missing is a subtle power of differentiation. I am in, therefore, a time of mass apprehensions.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "--- SFT æ ·æœ¬ 11 ---\n",
      "user: Write a poem in the style of Heid E. Erdrich.\n",
      "\n",
      "assistant: She couldn't help but sting my finger,\n",
      "clinging a moment before I flung her\n",
      "to the ground. Her gold is true, not the trick\n",
      "evening light plays on my roses.\n",
      "She curls into herself, stinger twitching,\n",
      "gilt wings folded. Her whole life just a few weeks,\n",
      "and my pain subsided in a moment.\n",
      "In the cold, she hardly had her wits to buzz.\n",
      "No warning from either of us:\n",
      "she sleeping in the richness of those petals,\n",
      "then the hand, my hand, cupping the bloom\n",
      "in devastating force, crushing the petals for the scent.\n",
      "And she mortally threatened, wholly unaware\n",
      "that I do this daily, alone with the gold last light,\n",
      "in what seems to me an act of love.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "--- SFT æ ·æœ¬ 12 ---\n",
      "user: Write a poem in the style of Sarah White.\n",
      "\n",
      "assistant: I will not speak ill of Jack Flick.\n",
      "I will rarely look\n",
      "at the scar he made on my cheek\n",
      "one summer at the lake.\n",
      "I won't speak ill of Jack whose freckles\n",
      "and gangly legs are gone.\n",
      "So is the drained face I saw when he saw\n",
      "what he'd done with a sharp rock\n",
      "nonchalantly skipped.\n",
      "I will speak well, for it was somewhat\n",
      "sweet to lie on the dock while Jack\n",
      "and his friends bent down\n",
      "and wiped my face with a sandy towel.\n",
      "I will speak well of them,\n",
      "for most are gone\n",
      "and the wound proved small.\n",
      "I will speak well, for the rock\n",
      "missed my eye. I can hardly find\n",
      "the scar. Jack went into the air\n",
      "corps, fought in one of the wars,\n",
      "retired, and lived less than a year\n",
      "before his tender heart gave out.\n",
      "I will speak well of Jack.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "--- SFT æ ·æœ¬ 13 ---\n",
      "user: Write a poem in the style of Judith Harris.\n",
      "\n",
      "assistant: How quiet is the spruce,\n",
      "the wind twills\n",
      "through the uppermost tier\n",
      "of splayed leaves.\n",
      "Now the song of a bird\n",
      "like the squeaky lock\n",
      "over a canoe's oar,\n",
      "followed by startling chirps,\n",
      "the sky pushing its clouds\n",
      "like sailboats,\n",
      "and I think, what kind of God\n",
      "keeps himself secret\n",
      "so that to find him out\n",
      "we have to seek, as children do\n",
      "for something like the beetle\n",
      "scuttling between grass,\n",
      "hidden in plain sight.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "--- SFT æ ·æœ¬ 14 ---\n",
      "user: Write a poem in the style of Kelly Madigan.\n",
      "\n",
      "assistant: You think we are the pointed argument,\n",
      "the man drunk at the party showing off\n",
      "his gun collection, the bed of nettles.\n",
      " \n",
      "What we really are is hidden from you:\n",
      "girl weeping in the closet among her stepfather's boots;\n",
      "tuft of rabbit fur caught in barbed wire; body of the baby\n",
      "in the landfill; boy with the shy mouth playing his guitar\n",
      "at the picnic table, out in the dirt yard.\n",
      " \n",
      "We slide into this world benign and pliable,\n",
      "quills pressed down smooth over back and tail.\n",
      "Only one hour here stiffens the barbs into thousands\n",
      "of quick retorts. Everything this well-guarded\n",
      "remembers being soft once.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "--- SFT æ ·æœ¬ 15 ---\n",
      "user: Write a poem in the style of Cathryn Essinger.\n",
      "\n",
      "assistant: I planted an apple tree in memory\n",
      "of my mother, who is not gone,\n",
      " \n",
      "but whose memory has become\n",
      "so transparent that she remembers\n",
      " \n",
      "slicing apples with her grandmother\n",
      "(yellow apples; blue bowl) better than\n",
      " \n",
      "the fruit that I hand her today. Still,\n",
      "she polishes the surface with her thumb,\n",
      " \n",
      "holds it to the light and says with no\n",
      "hesitation, Oh, Yellow Transparent . . . \n",
      " they're so fragile, you can almost seeto the core. She no longer remembers how\n",
      " \n",
      "to roll the crust, sweeten the sauce, but\n",
      "her desire is clearâ€”it is pie that she wants.\n",
      " \n",
      "And so, I slice as close as I dare to the coreâ€”\n",
      "to that little cathedral to memoryâ€”where\n",
      " \n",
      "the seeds remember everything they need\n",
      "to know to become yellow and transparent.\n",
      "\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# SFT JSONL æ–‡ä»¶è·¯å¾„\n",
    "sft_path = Path(\"/root/autodl-tmp/Pro/poetry_sft.jsonl\")\n",
    "\n",
    "print(\"=== å‰ 15 æ¡ SFT æ ·æœ¬ ===\\n\")\n",
    "\n",
    "with open(sft_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 15:\n",
    "            break\n",
    "        record = json.loads(line.strip())\n",
    "        messages = record.get(\"messages\", [])\n",
    "        \n",
    "        print(f\"--- SFT æ ·æœ¬ {i+1} ---\")\n",
    "        for msg in messages:\n",
    "            role = msg.get(\"role\")\n",
    "            content = msg.get(\"content\")\n",
    "            print(f\"{role}: {content}\\n\")\n",
    "        print(\"------------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79720328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ç¬¬ä¸€æ¡æ ·æœ¬ assistant loss ===\n",
      "12.385249137878418\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# ---------------------------\n",
    "# æœ¬åœ° tokenizer\n",
    "# ---------------------------\n",
    "tokenizer_path = r\"/root/autodl-tmp/qwen2.5-7b/qwen2.5-7b/tokenizer.json\"\n",
    "tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "# ---------------------------\n",
    "# è¯»å– JSONL çš„ç¬¬ä¸€æ¡æ ·æœ¬\n",
    "# ---------------------------\n",
    "sft_path = Path(r\"/root/autodl-tmp/Pro/poetry_sft.jsonl\")\n",
    "with open(sft_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    first_line = f.readline()\n",
    "    sample_jsonl = json.loads(first_line.strip())\n",
    "\n",
    "# ---------------------------\n",
    "# æ‹¼æ¥ user å’Œ assistant\n",
    "# ---------------------------\n",
    "user_text = \"\"\n",
    "assistant_text = \"\"\n",
    "\n",
    "for msg in sample_jsonl.get(\"messages\", []):\n",
    "    if msg[\"role\"] == \"user\":\n",
    "        user_text += msg[\"content\"].strip() + \"\\n\"\n",
    "    elif msg[\"role\"] == \"assistant\":\n",
    "        assistant_text += msg[\"content\"].strip() + \"\\n\"\n",
    "\n",
    "full_text = user_text + assistant_text\n",
    "\n",
    "# ---------------------------\n",
    "# Tokenization\n",
    "# ---------------------------\n",
    "input_ids = tokenizer.encode(full_text).ids\n",
    "user_len = len(tokenizer.encode(user_text).ids)\n",
    "\n",
    "input_ids = torch.tensor(input_ids).unsqueeze(0)  # batch=1\n",
    "labels = input_ids.clone()\n",
    "labels[:, :user_len] = -100  # user éƒ¨åˆ†ä¸è®¡ç®— loss\n",
    "\n",
    "# ---------------------------\n",
    "# å‡è®¾ä½ æœ‰ä¸€ä¸ªæœ¬åœ° PyTorch LLM æ¨¡å‹ï¼ˆQwen2.5-7bï¼‰\n",
    "# model.forward è¿”å› logits (batch, seq_len, vocab_size)\n",
    "# è¿™é‡Œæ¼”ç¤ºå¦‚ä½•è®¡ç®— assistant loss\n",
    "# ---------------------------\n",
    "\n",
    "# logits è¿™é‡Œç”¨éšæœºå¼ é‡æ¨¡æ‹Ÿï¼Œå®é™…ç”¨ä½ æœ¬åœ°æ¨¡å‹å‰å‘\n",
    "batch, seq_len = input_ids.shape\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "logits = torch.randn(batch, seq_len, vocab_size)  # æ¨¡æ‹Ÿæ¨¡å‹è¾“å‡º\n",
    "\n",
    "# CrossEntropyLoss è®¡ç®—\n",
    "loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "loss = loss_fct(logits.view(-1, vocab_size), labels.view(-1))\n",
    "\n",
    "print(\"=== ç¬¬ä¸€æ¡æ ·æœ¬ assistant loss ===\")\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c19d08d3-b1f4-4321-b169-0d4ff03b96c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_token: <|pad|>\n",
      "eos_token: <|endoftext|>\n",
      "bos_token: <|startoftext|>\n",
      "è¯è¡¨å¤§å°: 151665\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 2. åŠ è½½ tokenizerï¼ˆæ–¹æ¡ˆ Bï¼Œæœ¬åœ°å¯ç”¨ï¼‰\n",
    "# -------------------------\n",
    "tokenizer_file = os.path.join(CONFIG[\"model_name_or_path\"], \"tokenizer.json\")\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_file)\n",
    "\n",
    "# æ‰‹åŠ¨è®¾ç½®\n",
    "tokenizer.pad_token = \"<|pad|>\"  # å¡«å……åºåˆ—\n",
    "tokenizer.eos_token = \"<|endoftext|>\"  # æ–‡æœ¬ç»“æŸ\n",
    "tokenizer.bos_token = \"<|startoftext|>\"  # æ–‡æœ¬å¼€å§‹\n",
    "\n",
    "# æ‰‹åŠ¨è®¾ç½®ç‰¹æ®Š token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = \"<|pad|>\"\n",
    "if tokenizer.eos_token is None:\n",
    "    tokenizer.eos_token = \"<|endoftext|>\"\n",
    "if tokenizer.bos_token is None:\n",
    "    tokenizer.bos_token = \"<|startoftext|>\"\n",
    "\n",
    "print(\"pad_token:\", tokenizer.pad_token)\n",
    "print(\"eos_token:\", tokenizer.eos_token)\n",
    "print(\"bos_token:\", tokenizer.bos_token)\n",
    "print(\"è¯è¡¨å¤§å°:\", len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46bff95-5b2e-49ea-9878-393a29a57dcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import PreTrainedTokenizerFast, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm  # è¿›åº¦æ¡\n",
    "\n",
    "# -------------------------\n",
    "# 0. å…¨å±€ç¼“å­˜ç›®å½•åˆ°æ•°æ®ç›˜\n",
    "# -------------------------\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/root/autodl-tmp/hf_cache\"\n",
    "torch.hub.set_dir(\"/root/autodl-tmp/torch_cache\")\n",
    "os.environ[\"TORCH_HOME\"] = \"/root/autodl-tmp/torch_cache\"\n",
    "os.makedirs(\"/root/autodl-tmp/offload\", exist_ok=True)\n",
    "os.makedirs(\"/root/autodl-tmp/hf_cache\", exist_ok=True)\n",
    "os.makedirs(\"/root/autodl-tmp/torch_cache\", exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# 1. é…ç½®\n",
    "# -------------------------\n",
    "CONFIG = {\n",
    "    \"model_name_or_path\": \"/root/autodl-tmp/qwen2.5-7b/qwen2.5-7b\",\n",
    "    \"data_path\": \"/root/autodl-tmp/Pro/poetry_sft.jsonl\",\n",
    "    \"output_dir\": \"/root/autodl-tmp/Pro/qwen2.5-sft\",\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"max_seq_length\": 512,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"offload_folder\": \"/root/autodl-tmp/offload\",\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# 2. åŠ è½½ tokenizer\n",
    "# -------------------------\n",
    "tokenizer_file = os.path.join(CONFIG[\"model_name_or_path\"], \"tokenizer.json\")\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_file)\n",
    "tokenizer.eos_token = \"<|endoftext|>\"\n",
    "tokenizer.bos_token = \"<|startoftext|>\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# -------------------------\n",
    "# 3. Dataset\n",
    "# -------------------------\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, jsonl_path, tokenizer, max_len=512):\n",
    "        self.data = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                self.data.append(json.loads(line))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        user_text = sample[\"messages\"][0][\"content\"]\n",
    "        assistant_text = sample[\"messages\"][1][\"content\"]\n",
    "\n",
    "        prompt = f\"<|im_start|>user\\n{user_text}<|im_end|>\\n<|im_start|>assistant\\n{assistant_text}<|im_end|>\"\n",
    "        input_ids = self.tokenizer(\n",
    "            prompt, max_length=self.max_len, padding=\"max_length\", truncation=True\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        user_prompt_ids = self.tokenizer(\n",
    "            f\"<|im_start|>user\\n{user_text}<|im_end|>\\n\", truncation=True, max_length=self.max_len\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        labels = [-100]*len(input_ids)\n",
    "        for i, tok in enumerate(input_ids[len(user_prompt_ids):]):\n",
    "            labels[len(user_prompt_ids)+i] = tok\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(\n",
    "                [1 if id != self.tokenizer.pad_token_id else 0 for id in input_ids],\n",
    "                dtype=torch.long\n",
    "            )\n",
    "        }\n",
    "\n",
    "dataset = PoetryDataset(CONFIG[\"data_path\"], tokenizer, CONFIG[\"max_seq_length\"])\n",
    "loader = DataLoader(dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True)\n",
    "\n",
    "# -------------------------\n",
    "# 4. åŠ è½½æ¨¡å‹ + LoRA\n",
    "# -------------------------\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"model_name_or_path\"],\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=CONFIG[\"offload_folder\"]\n",
    ")\n",
    "model.gradient_checkpointing_enable()\n",
    "model.train()\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\",\"v_proj\",\"k_proj\",\"o_proj\"],\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# -------------------------\n",
    "# 5. Optimizer + Loss\n",
    "# -------------------------\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# -------------------------\n",
    "# 6. è®­ç»ƒå¾ªç¯ï¼ˆå®æ—¶æ˜¾ç¤ºæ¯ä¸ª sample loss + å‰©ä½™æ—¶é—´ï¼‰\n",
    "# -------------------------\n",
    "for epoch in range(CONFIG[\"num_train_epochs\"]):\n",
    "    total_loss = 0.0\n",
    "    start_time = time.time()  # ===== æ–°å¢ï¼šè®°å½•epochå¼€å§‹æ—¶é—´ =====\n",
    "    for step, batch in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(CONFIG[\"device\"])\n",
    "        labels = batch[\"labels\"].to(CONFIG[\"device\"])\n",
    "        attention_mask = batch[\"attention_mask\"].to(CONFIG[\"device\"])\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # ===== æ–°å¢ï¼šè®¡ç®—æ—¶é—´ç›¸å…³ =====\n",
    "        elapsed_time = time.time() - start_time  # å·²ç”¨æ—¶é—´ï¼ˆç§’ï¼‰\n",
    "        steps_done = step + 1\n",
    "        steps_total = len(loader)\n",
    "        time_per_step = elapsed_time / steps_done  # æ¯æ­¥è€—æ—¶\n",
    "        remaining_steps = steps_total - steps_done  # å‰©ä½™æ­¥æ•°\n",
    "        remaining_time = remaining_steps * time_per_step  # å‰©ä½™æ—¶é—´ï¼ˆç§’ï¼‰\n",
    "        \n",
    "        # æ ¼å¼åŒ–ä¸º æ—¶:åˆ†:ç§’\n",
    "        def format_time(seconds):\n",
    "            hours = int(seconds // 3600)\n",
    "            minutes = int((seconds % 3600) // 60)\n",
    "            secs = int(seconds % 60)\n",
    "            return f\"{hours:02d}:{minutes:02d}:{secs:02d}\"\n",
    "        \n",
    "        elapsed_str = format_time(elapsed_time)\n",
    "        remaining_str = format_time(remaining_time)\n",
    "\n",
    "        # ===== ä¿®æ”¹æ‰“å°è¡Œï¼šæ·»åŠ æ—¶é—´æ˜¾ç¤º =====\n",
    "        print(f\"\\rEpoch {epoch+1} | Step {step+1}/{len(loader)} | current_loss: {loss.item():.4f} | å·²ç”¨: {elapsed_str} | å‰©ä½™: {remaining_str}\", end=\"\")\n",
    "\n",
    "    # æ¯ä¸ª epoch ç»“æŸæ¢è¡Œï¼Œå¹¶æ‰“å°å¹³å‡ loss\n",
    "    total_epoch_time = format_time(time.time() - start_time)\n",
    "    print(f\"\\nEpoch {epoch+1} å¹³å‡ loss: {total_loss/len(loader):.4f} | æ€»è€—æ—¶: {total_epoch_time}\")\n",
    "\n",
    "# -------------------------\n",
    "# 7. ä¿å­˜ LoRA æ¨¡å‹\n",
    "# -------------------------\n",
    "model.save_pretrained(CONFIG[\"output_dir\"])\n",
    "tokenizer.save_pretrained(CONFIG[\"output_dir\"])\n",
    "print(\"ğŸ‰ LoRA å¾®è°ƒå®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09b6ccd-5322-4b10-bb01-bbb5fb904eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import PreTrainedTokenizerFast, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from tqdm import tqdm  # è¿›åº¦æ¡\n",
    "\n",
    "# -------------------------\n",
    "# 0. å…¨å±€ç¼“å­˜ç›®å½•åˆ°æ•°æ®ç›˜ + NLTKèµ„æºä¸‹è½½\n",
    "# -------------------------\n",
    "# é…ç½®ç¼“å­˜è·¯å¾„\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/root/autodl-tmp/hf_cache\"\n",
    "os.environ[\"HF_HOME\"] = \"/root/autodl-tmp/hf_cache\"\n",
    "torch.hub.set_dir(\"/root/autodl-tmp/torch_cache\")\n",
    "os.environ[\"TORCH_HOME\"] = \"/root/autodl-tmp/torch_cache\"\n",
    "# åˆ›å»ºç›®å½•\n",
    "os.makedirs(\"/root/autodl-tmp/offload\", exist_ok=True)\n",
    "os.makedirs(\"/root/autodl-tmp/hf_cache\", exist_ok=True)\n",
    "os.makedirs(\"/root/autodl-tmp/torch_cache\", exist_ok=True)\n",
    "\n",
    "# ä¸‹è½½NLTKè‹±æ–‡åŒä¹‰è¯åº“ï¼ˆé¦–æ¬¡è¿è¡Œè‡ªåŠ¨ä¸‹è½½ï¼‰\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "except LookupError:\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# -------------------------\n",
    "# 1. é…ç½®ï¼ˆæ–°å¢åˆ›é€ åº¦ç›¸å…³å‚æ•°ï¼‰\n",
    "# -------------------------\n",
    "CONFIG = {\n",
    "    \"model_name_or_path\": \"/root/autodl-tmp/qwen2.5-7b/qwen2.5-7b\",\n",
    "    \"data_path\": \"/root/autodl-tmp/Pro/poetry_sft.jsonl\",  # è‹±æ–‡è¯—JSONLæ•°æ®\n",
    "    \"output_dir\": \"/root/autodl-tmp/Pro/qwen2.5-sft\",\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 4,  # æ¢¯åº¦ç´¯ç§¯ï¼ˆå·²å¯ç”¨ï¼‰\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"max_seq_length\": 512,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"offload_folder\": \"/root/autodl-tmp/offload\",\n",
    "    # åˆ›é€ åº¦æå‡å‚æ•°\n",
    "    \"diversity_alpha\": 0.1,  # å¤šæ ·æ€§æŸå¤±æƒé‡ï¼ˆ0.05~0.2ï¼‰\n",
    "    \"temperature\": 1.2,      # æ¨ç†æ—¶æ¸©åº¦ç³»æ•°ï¼ˆè¶Šé«˜åˆ›é€ åº¦è¶Šå¼ºï¼‰\n",
    "    \"top_k\": 50,             # æ¨ç†é‡‡æ ·top_k\n",
    "    \"top_p\": 0.95,           # æ¨ç†é‡‡æ ·top_p\n",
    "    \"repetition_penalty\": 1.1 # é‡å¤æƒ©ç½šï¼ˆé¿å…è¯—å¥é‡å¤ï¼‰\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# 2. åŠ è½½ tokenizerï¼ˆé€‚é…è‹±æ–‡ï¼‰\n",
    "# -------------------------\n",
    "tokenizer_file = os.path.join(CONFIG[\"model_name_or_path\"], \"tokenizer.json\")\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_file)\n",
    "tokenizer.eos_token = \"<|endoftext|>\"\n",
    "tokenizer.bos_token = \"<|startoftext|>\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# -------------------------\n",
    "# 3. Datasetï¼ˆè‹±æ–‡è¯—+æ–‡æœ¬å¢å¼ºï¼‰\n",
    "# -------------------------\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, jsonl_path, tokenizer, max_len=512):\n",
    "        self.data = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        # åŠ è½½è‹±æ–‡è¯—æ•°æ®\n",
    "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                self.data.append(json.loads(line))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # è‹±æ–‡æ–‡æœ¬å¢å¼ºï¼šåŒä¹‰è¯æ›¿æ¢ï¼ˆæå‡è®­ç»ƒæ•°æ®å¤šæ ·æ€§ï¼‰\n",
    "    def augment_english_text(self, text):\n",
    "        tokens = text.split()\n",
    "        if len(tokens) < 3:\n",
    "            return text\n",
    "        \n",
    "        # è¯æ€§æ ‡æ³¨ï¼Œä»…æ›¿æ¢åè¯/åŠ¨è¯/å½¢å®¹è¯\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "        augmented_tokens = []\n",
    "        for token, pos in pos_tags:\n",
    "            if pos.startswith('N') or pos.startswith('V') or pos.startswith('J'):\n",
    "                # ä»WordNetè·å–åŒä¹‰è¯\n",
    "                synonyms = wordnet.synsets(token)\n",
    "                if synonyms:\n",
    "                    synonym_lemma = synonyms[0].lemmas()[0].name()\n",
    "                    # ä¿æŒè¯—æ­ŒéŸµå¾‹ï¼Œé¿å…è¿‡é•¿æ›¿æ¢\n",
    "                    if synonym_lemma != token and len(synonym_lemma) <= len(token)+2:\n",
    "                        augmented_tokens.append(synonym_lemma.replace('_', ' '))\n",
    "                    else:\n",
    "                        augmented_tokens.append(token)\n",
    "                else:\n",
    "                    augmented_tokens.append(token)\n",
    "            else:\n",
    "                augmented_tokens.append(token)\n",
    "        return ' '.join(augmented_tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        user_text = sample[\"messages\"][0][\"content\"]  # è‹±æ–‡ä¸»é¢˜\n",
    "        assistant_text = sample[\"messages\"][1][\"content\"]  # è‹±æ–‡è¯—\n",
    "\n",
    "        # 50%æ¦‚ç‡å¢å¼ºè¯—æ­Œæ–‡æœ¬ï¼ˆæå‡åˆ›é€ åº¦ï¼‰\n",
    "        if random.random() < 0.5:\n",
    "            assistant_text = self.augment_english_text(assistant_text)\n",
    "\n",
    "        # æ„å»ºè‹±æ–‡prompt\n",
    "        prompt = f\"<|im_start|>user\\n{user_text}<|im_end|>\\n<|im_start|>assistant\\n{assistant_text}<|im_end|>\"\n",
    "        input_ids = self.tokenizer(\n",
    "            prompt, max_length=self.max_len, padding=\"max_length\", truncation=True\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        # åˆ†ç¦»userå’Œassistantéƒ¨åˆ†ï¼Œä»…è®¡ç®—assistantçš„loss\n",
    "        user_prompt_ids = self.tokenizer(\n",
    "            f\"<|im_start|>user\\n{user_text}<|im_end|>\\n\", truncation=True, max_length=self.max_len\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        # æ ‡ç­¾ï¼šuseréƒ¨åˆ†è®¾ä¸º-100ï¼ˆå¿½ç•¥ï¼‰ï¼Œassistantéƒ¨åˆ†ä¸ºçœŸå®token\n",
    "        labels = [-100]*len(input_ids)\n",
    "        for i, tok in enumerate(input_ids[len(user_prompt_ids):]):\n",
    "            labels[len(user_prompt_ids)+i] = tok\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(\n",
    "                [1 if id != self.tokenizer.pad_token_id else 0 for id in input_ids],\n",
    "                dtype=torch.long\n",
    "            )\n",
    "        }\n",
    "\n",
    "dataset = PoetryDataset(CONFIG[\"data_path\"], tokenizer, CONFIG[\"max_seq_length\"])\n",
    "loader = DataLoader(dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True)\n",
    "\n",
    "# -------------------------\n",
    "# 4. åŠ è½½æ¨¡å‹ + LoRA\n",
    "# -------------------------\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"model_name_or_path\"],\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=CONFIG[\"offload_folder\"]\n",
    ")\n",
    "model.gradient_checkpointing_enable()\n",
    "model.train()\n",
    "\n",
    "# LoRAé…ç½®ï¼ˆé€‚é…è‹±æ–‡è¯—ç”Ÿæˆï¼‰\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\",\"v_proj\",\"k_proj\",\"o_proj\"],\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# -------------------------\n",
    "# 5. æŸå¤±å‡½æ•°ï¼ˆæ–°å¢å¤šæ ·æ€§æŸå¤±ï¼‰\n",
    "# -------------------------\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "ce_loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# å¤šæ ·æ€§æŸå¤±ï¼šæƒ©ç½šä¿å®ˆé¢„æµ‹ï¼Œæå‡åˆ›é€ åº¦\n",
    "def diversity_loss(logits, alpha=CONFIG[\"diversity_alpha\"]):\n",
    "    probs = F.softmax(logits, dim=-1)  # [batch*seq_len, vocab_size]\n",
    "    # è®¡ç®—ç†µå€¼ï¼šç†µè¶Šé«˜ï¼Œé¢„æµ‹åˆ†å¸ƒè¶Šåˆ†æ•£ï¼ˆåˆ›é€ åº¦è¶Šé«˜ï¼‰\n",
    "    entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=-1)\n",
    "    # å½’ä¸€åŒ–ç†µå€¼åˆ°0-1åŒºé—´\n",
    "    entropy_norm = entropy / torch.log(torch.tensor(probs.size(-1), device=logits.device))\n",
    "    # æƒ©ç½šä½ç†µå€¼ï¼ˆä¿å®ˆé¢„æµ‹ï¼‰\n",
    "    penalty = alpha * (1 - entropy_norm)\n",
    "    return torch.mean(penalty)\n",
    "\n",
    "# -------------------------\n",
    "# 6. è®­ç»ƒå¾ªç¯ï¼ˆæ¢¯åº¦ç´¯ç§¯+å¤šæŸå¤±+åˆ›é€ åº¦ä¼˜åŒ–ï¼‰\n",
    "# -------------------------\n",
    "# æ—¶é—´æ ¼å¼åŒ–å‡½æ•°\n",
    "def format_time(seconds):\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = int(seconds % 60)\n",
    "    return f\"{hours:02d}:{minutes:02d}:{secs:02d}\"\n",
    "\n",
    "# å¯ç”¨æ¢¯åº¦ç´¯ç§¯çš„è®­ç»ƒå¾ªç¯\n",
    "for epoch in range(CONFIG[\"num_train_epochs\"]):\n",
    "    total_loss = 0.0\n",
    "    total_ce_loss = 0.0\n",
    "    total_div_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    optimizer.zero_grad()  # åˆå§‹åŒ–æ¢¯åº¦\n",
    "    \n",
    "    for step, batch in enumerate(loader):\n",
    "        # åŠ è½½æ•°æ®åˆ°è®¾å¤‡\n",
    "        input_ids = batch[\"input_ids\"].to(CONFIG[\"device\"])\n",
    "        labels = batch[\"labels\"].to(CONFIG[\"device\"])\n",
    "        attention_mask = batch[\"attention_mask\"].to(CONFIG[\"device\"])\n",
    "\n",
    "        # å‰å‘ä¼ æ’­\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # è®¡ç®—æŸå¤±\n",
    "        ce_loss = ce_loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))  # åŸºç¡€äº¤å‰ç†µ\n",
    "        div_loss = diversity_loss(logits.view(-1, logits.size(-1)))            # å¤šæ ·æ€§æŸå¤±\n",
    "        total_step_loss = (ce_loss + div_loss) / CONFIG[\"gradient_accumulation_steps\"]  # æ¢¯åº¦ç´¯ç§¯å‡åˆ†\n",
    "\n",
    "        # åå‘ä¼ æ’­ï¼ˆç´¯ç§¯æ¢¯åº¦ï¼‰\n",
    "        total_step_loss.backward()\n",
    "\n",
    "        # æ¯gradient_accumulation_stepsæ­¥æ›´æ–°ä¸€æ¬¡å‚æ•°\n",
    "        if (step + 1) % CONFIG[\"gradient_accumulation_steps\"] == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # ç´¯è®¡æŸå¤±\n",
    "        total_loss += total_step_loss.item() * CONFIG[\"gradient_accumulation_steps\"]\n",
    "        total_ce_loss += ce_loss.item()\n",
    "        total_div_loss += div_loss.item()\n",
    "\n",
    "        # å®æ—¶æ‰“å°è¿›åº¦ï¼ˆå«åˆ›é€ åº¦ç›¸å…³æŒ‡æ ‡ï¼‰\n",
    "        elapsed_time = time.time() - start_time\n",
    "        steps_done = step + 1\n",
    "        steps_total = len(loader)\n",
    "        time_per_step = elapsed_time / steps_done\n",
    "        remaining_steps = steps_total - steps_done\n",
    "        remaining_time = remaining_steps * time_per_step\n",
    "\n",
    "        print(f\"\\rEpoch {epoch+1} | Step {steps_done}/{steps_total} | Total Loss: {total_step_loss.item()*CONFIG['gradient_accumulation_steps']:.4f} | \"\n",
    "              f\"CE Loss: {ce_loss.item():.4f} | Div Loss: {div_loss.item():.4f} | \"\n",
    "              f\"å·²ç”¨: {format_time(elapsed_time)} | å‰©ä½™: {format_time(remaining_time)}\", end=\"\")\n",
    "\n",
    "    # Epochç»“æŸæ‰“å°æ€»ç»“\n",
    "    epoch_avg_loss = total_loss / len(loader)\n",
    "    epoch_avg_ce = total_ce_loss / len(loader)\n",
    "    epoch_avg_div = total_div_loss / len(loader)\n",
    "    total_epoch_time = format_time(time.time() - start_time)\n",
    "    print(f\"\\nEpoch {epoch+1} å®Œæˆ | å¹³å‡æ€»Loss: {epoch_avg_loss:.4f} | å¹³å‡CE Loss: {epoch_avg_ce:.4f} | å¹³å‡Div Loss: {epoch_avg_div:.4f} | æ€»è€—æ—¶: {total_epoch_time}\\n\")\n",
    "\n",
    "# -------------------------\n",
    "# 7. ä¿å­˜ LoRA æ¨¡å‹\n",
    "# -------------------------\n",
    "model.save_pretrained(CONFIG[\"output_dir\"])\n",
    "tokenizer.save_pretrained(CONFIG[\"output_dir\"])\n",
    "print(\"ğŸ‰ LoRA å¾®è°ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜è‡³:\", CONFIG[\"output_dir\"])\n",
    "\n",
    "# -------------------------\n",
    "# 8. é«˜åˆ›é€ åº¦è‹±æ–‡è¯—ç”Ÿæˆå‡½æ•°\n",
    "# -------------------------\n",
    "def generate_creative_poetry(topic, max_len=512):\n",
    "    \"\"\"ç”Ÿæˆé«˜åˆ›é€ åº¦çš„è‹±æ–‡è¯—\"\"\"\n",
    "    model.eval()\n",
    "    # æ„å»ºè‹±æ–‡prompt\n",
    "    prompt = f\"<|im_start|>user\\n{topic}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(CONFIG[\"device\"])\n",
    "    \n",
    "    # é«˜åˆ›é€ åº¦é‡‡æ ·ç­–ç•¥\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_len,\n",
    "            do_sample=True,               # å¯ç”¨é‡‡æ ·ï¼ˆæ ¸å¿ƒï¼šæå‡åˆ›é€ åº¦ï¼‰\n",
    "            top_k=CONFIG[\"top_k\"],        # ä»…ä»æ¦‚ç‡å‰Kçš„tokenä¸­é€‰\n",
    "            top_p=CONFIG[\"top_p\"],        # ç´¯ç§¯æ¦‚ç‡é‡‡æ ·\n",
    "            temperature=CONFIG[\"temperature\"],  # æ¸©åº¦ç³»æ•°ï¼ˆè¶Šé«˜è¶Šå¤šæ ·ï¼‰\n",
    "            repetition_penalty=CONFIG[\"repetition_penalty\"],  # æƒ©ç½šé‡å¤\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # è§£ç å¹¶æ¸…ç†ç”Ÿæˆç»“æœ\n",
    "    poetry = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    poetry = poetry.replace(prompt, \"\").strip()\n",
    "    return poetry\n",
    "\n",
    "# -------------------------\n",
    "# 9. ç”Ÿæˆç¤ºä¾‹\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # ç”Ÿæˆå…³äº\"starry night\"çš„é«˜åˆ›é€ åº¦è‹±æ–‡è¯—\n",
    "    print(\"\\n=== ç”Ÿæˆé«˜åˆ›é€ åº¦è‹±æ–‡è¯— ===\")\n",
    "    topic = \"starry night\"\n",
    "    poetry = generate_creative_poetry(topic)\n",
    "    print(f\"ä¸»é¢˜: {topic}\\n\")\n",
    "    print(poetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "928455f5-e88e-42e0-a4a7-502ff3e3813b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  è·³è¿‡ç¬¬920è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬946è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬4092è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬4192è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬4194è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬4199è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬4200è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬4224è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬4242è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬4243è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬4252è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬4254è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬4301è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬4302è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬4303è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬4457è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬4458è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬4459è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬4541è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬4602è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬4603è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬4762è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬4763è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬5447è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬5452è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬5595è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬5597è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬5675è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬5677è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬5789è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬5790è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬5791è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬5792è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬5829è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬5836è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬5837è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬5853è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬5861è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬5969è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬5970è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬5989è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬5990è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬6264è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬6334è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬6335è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬6367è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬6368è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬6369è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬6370è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬6371è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬6372è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬6373è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬6374è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬6375è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬6376è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬6432è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬6515è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬7667è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬8493è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬9186è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬9902è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬10128è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬10133è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬10162è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬10168è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬10170è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬10234è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬12027è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬12243è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬12244è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬12245è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬12246è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬12247è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬12248è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬12249è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬12250è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬12251è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬12252è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬12253è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬12254è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬12255è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬12256è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬12257è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬12258è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬12259è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬13828è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬13829è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬13837è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬13838è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬13839è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬13840è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬13841è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬13842è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬13843è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬13844è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬13845è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬13846è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬13847è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬13848è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬13851è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âš ï¸  è·³è¿‡ç¬¬13852è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\n",
      "âœ… æˆåŠŸåŠ è½½æœ‰æ•ˆæ•°æ®æ¡æ•°: 13753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,046,272 || all params: 7,620,662,784 || trainable%: 0.0662\n",
      "\n",
      "âš ï¸  Step 0 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 1 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 2 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 3 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 4 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 5 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 6 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 7 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 8 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 9 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 10 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 11 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 12 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 13 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 14 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 15 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 16 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 17 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 18 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 19 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 20 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 21 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 22 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 23 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 24 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 25 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 26 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 27 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 28 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 29 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 30 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 31 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 32 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 33 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 34 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 35 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 36 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 37 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 38 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 39 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 40 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 41 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 42 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 43 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 44 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 45 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 46 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 47 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 48 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 49 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 50 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 51 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 52 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 53 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 54 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 55 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 56 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 57 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 58 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 59 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 60 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 61 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 62 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 63 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 64 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 65 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 66 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 67 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 68 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 69 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 70 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 71 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 72 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 73 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 74 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 75 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 76 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 77 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 78 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 79 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 80 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 81 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 82 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 83 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 84 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 85 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 86 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 87 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 88 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 89 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 90 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 91 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 92 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 93 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 94 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 95 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 96 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 97 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 98 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 99 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 100 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 101 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 102 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 103 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 104 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 105 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 106 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 107 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 108 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 109 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 110 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 111 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 112 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 113 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 114 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 115 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 116 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 117 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 118 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 119 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 120 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 121 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 122 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 123 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 124 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 125 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 126 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 127 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 128 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 129 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 130 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 131 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 132 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 133 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 134 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 135 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 136 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 137 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 138 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 139 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 140 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 141 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 142 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 143 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 144 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 145 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 146 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 147 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 148 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 149 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 150 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 151 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 152 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 153 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 154 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 155 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 156 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 157 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 158 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 159 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 160 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 161 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 162 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 163 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 164 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 165 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 166 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 167 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 168 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 169 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 170 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 171 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 172 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 173 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 174 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 175 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 176 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 177 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 178 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 179 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 180 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 181 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 182 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 183 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 184 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 185 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 186 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 187 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 188 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 189 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 190 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 191 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 192 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 193 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 194 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 195 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 196 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 197 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 198 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 199 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 200 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 201 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 202 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 203 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 204 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 205 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 206 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 207 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 208 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 209 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 210 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 211 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 212 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 213 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 214 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 215 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 216 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 217 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 218 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 219 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 220 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 221 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 222 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 223 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 224 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 225 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 226 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 227 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 228 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 229 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 230 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 231 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 232 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 233 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 234 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 235 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 236 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 237 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 238 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 239 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 240 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 241 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 242 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 243 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 244 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 245 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 246 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 247 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 248 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 249 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 250 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 251 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 252 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 253 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 254 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 255 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 256 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 257 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 258 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 259 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 260 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 261 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 262 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 263 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 264 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 265 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 266 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 267 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 268 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 269 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 270 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 271 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 272 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 273 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 274 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 275 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 276 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 277 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 278 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 279 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 280 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 281 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 282 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 283 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 284 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 285 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 286 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 287 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 288 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 289 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 290 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 291 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 292 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 293 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 294 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 295 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 296 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 297 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 298 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 299 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 300 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 301 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 302 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 303 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 304 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 305 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 306 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 307 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 308 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 309 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 310 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 311 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 312 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 313 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 314 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 315 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 316 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 317 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 318 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 319 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 320 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 321 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 322 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 323 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 324 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 325 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 326 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 327 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 328 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 329 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 330 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 331 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 332 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 333 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 334 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 335 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 336 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 337 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 338 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 339 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 340 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 341 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 342 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 343 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 344 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 345 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 346 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 347 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 348 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 349 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 350 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 351 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 352 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 353 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 354 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n",
      "\n",
      "âš ï¸  Step 355 å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 322\u001b[39m\n\u001b[32m    319\u001b[39m attention_mask = batch[\u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m].to(CONFIG[\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    321\u001b[39m \u001b[38;5;66;03m# å‰å‘ä¼ æ’­\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# è®©æ¨¡å‹è‡ªåŠ¨è®¡ç®—lossï¼ˆåŒé‡æ ¡éªŒï¼‰\u001b[39;49;00m\n\u001b[32m    326\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    327\u001b[39m logits = outputs.logits\n\u001b[32m    328\u001b[39m \u001b[38;5;66;03m# åŸºç¡€lossï¼ˆåŒé‡æ ¡éªŒï¼‰\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/peft/peft_model.py:1923\u001b[39m, in \u001b[36mPeftModelForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[39m\n\u001b[32m   1921\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(**kwargs):\n\u001b[32m   1922\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1924\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1927\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1928\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1929\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1930\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1931\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1932\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1934\u001b[39m batch_size = _get_batch_size(input_ids, inputs_embeds)\n\u001b[32m   1935\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1936\u001b[39m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:308\u001b[39m, in \u001b[36mBaseTuner.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any):\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/transformers/utils/generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:449\u001b[39m, in \u001b[36mQwen2ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    418\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    430\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    431\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    432\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    433\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    434\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    447\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    448\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    461\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/transformers/utils/generic.py:1072\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1069\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1074\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1075\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1076\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1077\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:384\u001b[39m, in \u001b[36mQwen2Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    381\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    397\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    398\u001b[39m     past_key_values=past_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    399\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/transformers/modeling_layers.py:93\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     90\u001b[39m         message = message.rstrip(\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m) + \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gradient_checkpointing_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/torch/_compile.py:53\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     50\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive, wrapping=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     51\u001b[39m     fn.__dynamo_disable = disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:1044\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1042\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m   1043\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1046\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/torch/utils/checkpoint.py:496\u001b[39m, in \u001b[36mcheckpoint\u001b[39m\u001b[34m(function, use_reentrant, context_fn, determinism_check, debug, early_stop, *args, **kwargs)\u001b[39m\n\u001b[32m    491\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m context_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m noop_context_fn \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    492\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    493\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    494\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33muse_reentrant=False.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    495\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    498\u001b[39m     gen = _checkpoint_without_reentrant_generator(\n\u001b[32m    499\u001b[39m         function, preserve, context_fn, determinism_check, debug, early_stop, *args, **kwargs\n\u001b[32m    500\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/torch/autograd/function.py:581\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    579\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    580\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    584\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    585\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    586\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    587\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    588\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    589\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/torch/utils/checkpoint.py:262\u001b[39m, in \u001b[36mCheckpointFunction.forward\u001b[39m\u001b[34m(ctx, run_function, preserve_rng_state, *args)\u001b[39m\n\u001b[32m    259\u001b[39m ctx.save_for_backward(*tensor_inputs)\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     outputs = \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:249\u001b[39m, in \u001b[36mQwen2DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    247\u001b[39m residual = hidden_states\n\u001b[32m    248\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:46\u001b[39m, in \u001b[36mQwen2MLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     down_proj = \u001b[38;5;28mself\u001b[39m.down_proj(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m * \u001b[38;5;28mself\u001b[39m.up_proj(x))\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/transformers/activations.py:99\u001b[39m, in \u001b[36mSiLUActivation.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/torch/nn/functional.py:2371\u001b[39m, in \u001b[36msilu\u001b[39m\u001b[34m(input, inplace)\u001b[39m\n\u001b[32m   2369\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   2370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch._C._nn.silu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2371\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import PreTrainedTokenizerFast, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from tqdm import tqdm  # è¿›åº¦æ¡\n",
    "\n",
    "# -------------------------\n",
    "# 0. å…¨å±€ç¼“å­˜ç›®å½•åˆ°æ•°æ®ç›˜ + NLTKèµ„æºä¸‹è½½\n",
    "# -------------------------\n",
    "# é…ç½®ç¼“å­˜è·¯å¾„\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/root/autodl-tmp/hf_cache\"\n",
    "os.environ[\"HF_HOME\"] = \"/root/autodl-tmp/hf_cache\"\n",
    "torch.hub.set_dir(\"/root/autodl-tmp/torch_cache\")\n",
    "os.environ[\"TORCH_HOME\"] = \"/root/autodl-tmp/torch_cache\"\n",
    "# åˆ›å»ºç›®å½•\n",
    "os.makedirs(\"/root/autodl-tmp/offload\", exist_ok=True)\n",
    "os.makedirs(\"/root/autodl-tmp/hf_cache\", exist_ok=True)\n",
    "os.makedirs(\"/root/autodl-tmp/torch_cache\", exist_ok=True)\n",
    "\n",
    "# ä¸‹è½½NLTKè‹±æ–‡åŒä¹‰è¯åº“ï¼ˆé¦–æ¬¡è¿è¡Œè‡ªåŠ¨ä¸‹è½½ï¼‰\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "except LookupError:\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# -------------------------\n",
    "# 1. é…ç½®ï¼ˆæ–°å¢åˆ›é€ åº¦ç›¸å…³å‚æ•° + æ•°å€¼ç¨³å®šå‚æ•°ï¼‰\n",
    "# -------------------------\n",
    "CONFIG = {\n",
    "    \"model_name_or_path\": \"/root/autodl-tmp/qwen2.5-7b/qwen2.5-7b\",\n",
    "    \"data_path\": \"/root/autodl-tmp/Pro/poetry_sft.jsonl\",  # è‹±æ–‡è¯—JSONLæ•°æ®\n",
    "    \"output_dir\": \"/root/autodl-tmp/Pro/qwen2.5-sft\",\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 4,  # æ¢¯åº¦ç´¯ç§¯\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"max_seq_length\": 512,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"offload_folder\": \"/root/autodl-tmp/offload\",\n",
    "    # åˆ›é€ åº¦æå‡å‚æ•°\n",
    "    \"diversity_alpha\": 0.05,  # é™ä½æƒé‡ï¼Œå…ˆä»0.05å¼€å§‹ï¼ˆåŸ0.1ï¼‰\n",
    "    \"temperature\": 1.2,      \n",
    "    \"top_k\": 50,             \n",
    "    \"top_p\": 0.95,           \n",
    "    \"repetition_penalty\": 1.1,\n",
    "    # æ•°å€¼ç¨³å®šå‚æ•°\n",
    "    \"eps\": 1e-6,  # é˜²æ­¢é™¤0/å¯¹æ•°æº¢å‡º\n",
    "    \"max_norm\": 1.0,  # æ¢¯åº¦è£å‰ª\n",
    "    \"fp32_loss\": True  # æŸå¤±è®¡ç®—ç”¨fp32ï¼Œé¿å…fp16æº¢å‡º\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# 2. åŠ è½½ tokenizerï¼ˆé€‚é…è‹±æ–‡ï¼‰\n",
    "# -------------------------\n",
    "tokenizer_file = os.path.join(CONFIG[\"model_name_or_path\"], \"tokenizer.json\")\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_file)\n",
    "tokenizer.eos_token = \"<|endoftext|>\"\n",
    "tokenizer.bos_token = \"<|startoftext|>\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# -------------------------\n",
    "# 3. Datasetï¼ˆæ ¸å¿ƒä¿®å¤ï¼šç¡®ä¿__len__æœ‰æ•ˆ + æ•°æ®åŠ è½½æ ¡éªŒï¼‰\n",
    "# -------------------------\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, jsonl_path, tokenizer, max_len=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.data = self._load_and_validate_data(jsonl_path)  # ç‹¬ç«‹åŠ è½½å‡½æ•°\n",
    "        \n",
    "        # å…³é”®æ ¡éªŒï¼šç¡®ä¿æ•°æ®éç©º\n",
    "        if len(self.data) == 0:\n",
    "            raise ValueError(f\"æ•°æ®åŠ è½½å¤±è´¥ï¼è·¯å¾„ {jsonl_path} ä¸‹æ— æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œè·¯å¾„\")\n",
    "        print(f\"âœ… æˆåŠŸåŠ è½½æœ‰æ•ˆæ•°æ®æ¡æ•°: {len(self.data)}\")\n",
    "\n",
    "    def _load_and_validate_data(self, jsonl_path):\n",
    "        \"\"\"ç‹¬ç«‹çš„æ•°æ®åŠ è½½+æ ¡éªŒå‡½æ•°ï¼Œç¡®ä¿è¿”å›éç©ºåˆ—è¡¨\"\"\"\n",
    "        valid_data = []\n",
    "        \n",
    "        # ç¬¬ä¸€æ­¥ï¼šæ ¡éªŒæ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
    "        if not os.path.exists(jsonl_path):\n",
    "            raise FileNotFoundError(f\"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {jsonl_path}\")\n",
    "        \n",
    "        # ç¬¬äºŒæ­¥ï¼šåŠ è½½å¹¶è¿‡æ»¤æœ‰æ•ˆæ•°æ®\n",
    "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            if len(lines) == 0:\n",
    "                raise ValueError(f\"æ•°æ®æ–‡ä»¶ä¸ºç©º: {jsonl_path}\")\n",
    "            \n",
    "            for line_idx, line in enumerate(lines):\n",
    "                line = line.strip()\n",
    "                if not line:  # è·³è¿‡ç©ºè¡Œ\n",
    "                    continue\n",
    "                try:\n",
    "                    sample = json.loads(line)\n",
    "                    # æ ¡éªŒmessagesç»“æ„\n",
    "                    if not isinstance(sample.get(\"messages\"), list) or len(sample[\"messages\"]) < 2:\n",
    "                        print(f\"âš ï¸  è·³è¿‡ç¬¬{line_idx+1}è¡Œï¼šmessagesæ ¼å¼é”™è¯¯\")\n",
    "                        continue\n",
    "                    # æ ¡éªŒuser/assistantå†…å®¹\n",
    "                    user_msg = sample[\"messages\"][0]\n",
    "                    assistant_msg = sample[\"messages\"][1]\n",
    "                    if (not isinstance(user_msg.get(\"content\"), str) or not user_msg[\"content\"].strip() or\n",
    "                        not isinstance(assistant_msg.get(\"content\"), str) or not assistant_msg[\"content\"].strip()):\n",
    "                        print(f\"âš ï¸  è·³è¿‡ç¬¬{line_idx+1}è¡Œï¼šå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯\")\n",
    "                        continue\n",
    "                    # æ‰€æœ‰æ ¡éªŒé€šè¿‡ï¼ŒåŠ å…¥æœ‰æ•ˆæ•°æ®\n",
    "                    valid_data.append(sample)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"âš ï¸  è·³è¿‡ç¬¬{line_idx+1}è¡Œï¼šJSONè§£æå¤±è´¥\")\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸  è·³è¿‡ç¬¬{line_idx+1}è¡Œï¼šæœªçŸ¥é”™è¯¯ - {str(e)}\")\n",
    "        \n",
    "        return valid_data\n",
    "\n",
    "    # ä¿®å¤è‹±æ–‡æ–‡æœ¬å¢å¼ºï¼šå¢åŠ è¾¹ç•Œæ ¡éªŒ\n",
    "    def augment_english_text(self, text):\n",
    "        if not text or len(text.split()) < 3:\n",
    "            return text\n",
    "        \n",
    "        try:\n",
    "            # è¯æ€§æ ‡æ³¨ï¼Œä»…æ›¿æ¢åè¯/åŠ¨è¯/å½¢å®¹è¯\n",
    "            pos_tags = nltk.pos_tag(text.split())\n",
    "            augmented_tokens = []\n",
    "            for token, pos in pos_tags:\n",
    "                if pos.startswith('N') or pos.startswith('V') or pos.startswith('J'):\n",
    "                    synonyms = wordnet.synsets(token)\n",
    "                    if synonyms and len(synonyms) > 0:\n",
    "                        synonym_lemma = synonyms[0].lemmas()[0].name()\n",
    "                        # ä¸¥æ ¼æ ¡éªŒæ›¿æ¢è¯\n",
    "                        if (synonym_lemma != token and \n",
    "                            len(synonym_lemma) <= len(token)+2 and\n",
    "                            not synonym_lemma.isdigit()):\n",
    "                            augmented_tokens.append(synonym_lemma.replace('_', ' '))\n",
    "                        else:\n",
    "                            augmented_tokens.append(token)\n",
    "                    else:\n",
    "                        augmented_tokens.append(token)\n",
    "                else:\n",
    "                    augmented_tokens.append(token)\n",
    "            return ' '.join(augmented_tokens)\n",
    "        except:\n",
    "            return text  # å¢å¼ºå¤±è´¥æ—¶è¿”å›åŸæ–‡æœ¬\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"æ ¸å¿ƒä¿®å¤ï¼šç¡®ä¿è¿”å›éè´Ÿæ•´æ•°ï¼Œä¸”æ•°æ®åŠ è½½æ—¶å·²æ ¡éªŒéç©º\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        user_text = sample[\"messages\"][0][\"content\"].strip()\n",
    "        assistant_text = sample[\"messages\"][1][\"content\"].strip()\n",
    "\n",
    "        # 50%æ¦‚ç‡å¢å¼ºè¯—æ­Œæ–‡æœ¬ï¼ˆæå‡åˆ›é€ åº¦ï¼‰\n",
    "        if random.random() < 0.5:\n",
    "            assistant_text = self.augment_english_text(assistant_text)\n",
    "\n",
    "        # æ„å»ºè‹±æ–‡prompt\n",
    "        prompt = f\"<|im_start|>user\\n{user_text}<|im_end|>\\n<|im_start|>assistant\\n{assistant_text}<|im_end|>\"\n",
    "        # ç¼–ç ï¼ˆå¢åŠ padding_side='right'ï¼Œé¿å…æˆªæ–­assistantéƒ¨åˆ†ï¼‰\n",
    "        encoding = self.tokenizer(\n",
    "            prompt, \n",
    "            max_length=self.max_len, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True,\n",
    "            padding_side='right'\n",
    "        )\n",
    "        input_ids = encoding[\"input_ids\"]\n",
    "        attention_mask = encoding[\"attention_mask\"]\n",
    "\n",
    "        # åˆ†ç¦»userå’Œassistantéƒ¨åˆ†ï¼Œä»…è®¡ç®—assistantçš„loss\n",
    "        user_prompt = f\"<|im_start|>user\\n{user_text}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "        user_prompt_ids = self.tokenizer(\n",
    "            user_prompt, \n",
    "            truncation=True, \n",
    "            max_length=self.max_len\n",
    "        )[\"input_ids\"]\n",
    "        user_prompt_len = min(len(user_prompt_ids), self.max_len-1)  # é˜²æ­¢è¶Šç•Œ\n",
    "\n",
    "        # æ ‡ç­¾ï¼šuseréƒ¨åˆ†è®¾ä¸º-100ï¼ˆå¿½ç•¥ï¼‰ï¼Œassistantéƒ¨åˆ†ä¸ºçœŸå®token\n",
    "        labels = [-100]*len(input_ids)\n",
    "        # ç¡®ä¿ä¸è¶Šç•Œ\n",
    "        for i in range(user_prompt_len, min(len(input_ids), self.max_len)):\n",
    "            labels[i] = input_ids[i]\n",
    "\n",
    "        # æ ¡éªŒï¼šç¡®ä¿æœ‰é-100çš„æ ‡ç­¾ï¼ˆé¿å…CE Lossä¸º0ï¼‰\n",
    "        if sum(1 for x in labels if x != -100) < 5:\n",
    "            # æ›¿æ¢ä¸ºç®€å•æ ‡ç­¾ï¼ˆé˜²æ­¢nanï¼‰\n",
    "            labels = input_ids.copy()\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# -------------------------\n",
    "# 4. æ•°æ®åŠ è½½ï¼ˆå¢åŠ å¼‚å¸¸æ•è·ï¼‰\n",
    "# -------------------------\n",
    "try:\n",
    "    dataset = PoetryDataset(CONFIG[\"data_path\"], tokenizer, CONFIG[\"max_seq_length\"])\n",
    "    # ä¿®å¤DataLoaderï¼šæ˜ç¡®æŒ‡å®šsamplerçš„num_samplesï¼ˆé˜²æ­¢ç©ºæ•°æ®ï¼‰\n",
    "    loader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=CONFIG[\"batch_size\"], \n",
    "        shuffle=True,\n",
    "        num_workers=0,  # ç¦ç”¨å¤šè¿›ç¨‹ï¼Œé¿å…æ•°æ®åŠ è½½å¼‚å¸¸\n",
    "        drop_last=True  # ä¸¢å¼ƒæœ€åä¸å®Œæ•´çš„batch\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}\")\n",
    "    exit(1)\n",
    "\n",
    "# -------------------------\n",
    "# 5. åŠ è½½æ¨¡å‹ + LoRA\n",
    "# -------------------------\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"model_name_or_path\"],\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=CONFIG[\"offload_folder\"],\n",
    "    # å¢åŠ æ•°å€¼ç¨³å®šé…ç½®\n",
    "    attn_implementation=\"eager\" # ç¦ç”¨flash attentionï¼ˆå¯èƒ½å¯¼è‡´fp16æº¢å‡ºï¼‰\n",
    ")\n",
    "model.gradient_checkpointing_enable()\n",
    "model.train()\n",
    "\n",
    "# LoRAé…ç½®ï¼ˆé€‚é…è‹±æ–‡è¯—ç”Ÿæˆï¼‰\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\",\"v_proj\",\"k_proj\",\"o_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    bias=\"none\"  # ç¦ç”¨biasè®­ç»ƒï¼Œå‡å°‘æ•°å€¼æ³¢åŠ¨\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# -------------------------\n",
    "# 6. æŸå¤±å‡½æ•°ï¼ˆä¿®å¤å¤šæ ·æ€§æŸå¤± + æ•°å€¼ç¨³å®šï¼‰\n",
    "# -------------------------\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=CONFIG[\"learning_rate\"],\n",
    "    eps=1e-8,  # ä¼˜åŒ–å™¨æ•°å€¼ç¨³å®š\n",
    "    weight_decay=0.01  # å¢åŠ æƒé‡è¡°å‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ\n",
    ")\n",
    "ce_loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# ä¿®å¤å¤šæ ·æ€§æŸå¤±ï¼šå¢åŠ æ•°å€¼ç¨³å®šå¤„ç†\n",
    "def diversity_loss(logits, alpha=CONFIG[\"diversity_alpha\"]):\n",
    "    # è½¬æ¢ä¸ºfp32è®¡ç®—ï¼Œé¿å…fp16æº¢å‡º\n",
    "    if CONFIG[\"fp32_loss\"]:\n",
    "        logits = logits.float()\n",
    "    \n",
    "    # è®¡ç®—softmaxï¼ˆå¢åŠ dim=-1ï¼Œé˜²æ­¢ç»´åº¦é”™è¯¯ï¼‰\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    # é™åˆ¶probsçš„èŒƒå›´ï¼Œé˜²æ­¢log(0)\n",
    "    probs = torch.clamp(probs, min=CONFIG[\"eps\"], max=1.0 - CONFIG[\"eps\"])\n",
    "    \n",
    "    # è®¡ç®—ç†µå€¼ï¼ˆæ•°å€¼ç¨³å®šç‰ˆï¼‰\n",
    "    entropy = -torch.sum(probs * torch.log(probs), dim=-1)\n",
    "    # è®¡ç®—æœ€å¤§ç†µï¼ˆvocab_sizeçš„å¯¹æ•°ï¼‰\n",
    "    vocab_size = probs.size(-1)\n",
    "    max_entropy = torch.log(torch.tensor(vocab_size, dtype=torch.float32, device=logits.device))\n",
    "    # å½’ä¸€åŒ–ç†µå€¼ï¼ˆé˜²æ­¢é™¤0ï¼‰\n",
    "    entropy_norm = entropy / (max_entropy + CONFIG[\"eps\"])\n",
    "    # é™åˆ¶ç†µå€¼èŒƒå›´åœ¨0-1\n",
    "    entropy_norm = torch.clamp(entropy_norm, min=0.0, max=1.0)\n",
    "    \n",
    "    # æƒ©ç½šä½ç†µå€¼ï¼ˆä¿å®ˆé¢„æµ‹ï¼‰\n",
    "    penalty = alpha * (1 - entropy_norm)\n",
    "    # é˜²æ­¢penaltyå‡ºç°nan\n",
    "    penalty = torch.nan_to_num(penalty, nan=0.0, posinf=alpha, neginf=0.0)\n",
    "    \n",
    "    return torch.mean(penalty)\n",
    "\n",
    "# -------------------------\n",
    "# 7. è®­ç»ƒå¾ªç¯ï¼ˆæ¢¯åº¦ç´¯ç§¯+å¤šæŸå¤±+åˆ›é€ åº¦ä¼˜åŒ–+æ•°å€¼ç¨³å®šï¼‰\n",
    "# -------------------------\n",
    "# æ—¶é—´æ ¼å¼åŒ–å‡½æ•°\n",
    "def format_time(seconds):\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = int(seconds % 60)\n",
    "    return f\"{hours:02d}:{minutes:02d}:{secs:02d}\"\n",
    "\n",
    "# å¯ç”¨æ¢¯åº¦ç´¯ç§¯çš„è®­ç»ƒå¾ªç¯\n",
    "for epoch in range(CONFIG[\"num_train_epochs\"]):\n",
    "    total_loss = 0.0\n",
    "    total_ce_loss = 0.0\n",
    "    total_div_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    optimizer.zero_grad()  # åˆå§‹åŒ–æ¢¯åº¦\n",
    "    \n",
    "    # å¢åŠ ç©ºloaderæ ¡éªŒ\n",
    "    if len(loader) == 0:\n",
    "        print(\"âŒ è®­ç»ƒåŠ è½½å™¨ä¸ºç©ºï¼Œç»ˆæ­¢è®­ç»ƒ\")\n",
    "        break\n",
    "    \n",
    "    for step, batch in enumerate(loader):\n",
    "        # åŠ è½½æ•°æ®åˆ°è®¾å¤‡\n",
    "        input_ids = batch[\"input_ids\"].to(CONFIG[\"device\"])\n",
    "        labels = batch[\"labels\"].to(CONFIG[\"device\"])\n",
    "        attention_mask = batch[\"attention_mask\"].to(CONFIG[\"device\"])\n",
    "\n",
    "        # å‰å‘ä¼ æ’­\n",
    "        outputs = model(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels  # è®©æ¨¡å‹è‡ªåŠ¨è®¡ç®—lossï¼ˆåŒé‡æ ¡éªŒï¼‰\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        # åŸºç¡€lossï¼ˆåŒé‡æ ¡éªŒï¼‰\n",
    "        base_loss = outputs.loss\n",
    "        \n",
    "        # è®¡ç®—æŸå¤±ï¼ˆæ•°å€¼ç¨³å®šç‰ˆï¼‰\n",
    "        # æ–¹å¼1ï¼šç”¨æ¨¡å‹è‡ªå¸¦çš„lossï¼ˆæ›´ç¨³å®šï¼‰\n",
    "        ce_loss = base_loss.float() if CONFIG[\"fp32_loss\"] else base_loss\n",
    "        # æ–¹å¼2ï¼šæ‰‹åŠ¨è®¡ç®—CE Lossï¼ˆå¤‡ç”¨ï¼‰\n",
    "        # ce_loss = ce_loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        # if CONFIG[\"fp32_loss\"]:\n",
    "        #     ce_loss = ce_loss.float()\n",
    "        \n",
    "        # è®¡ç®—å¤šæ ·æ€§æŸå¤±\n",
    "        div_loss = diversity_loss(logits.view(-1, logits.size(-1)))\n",
    "        # æ€»æŸå¤±\n",
    "        total_step_loss = (ce_loss + div_loss) / CONFIG[\"gradient_accumulation_steps\"]\n",
    "\n",
    "        # æ£€æŸ¥lossæ˜¯å¦ä¸ºnan\n",
    "        if torch.isnan(total_step_loss):\n",
    "            print(f\"\\nâš ï¸  Step {step} å‡ºç°NaN Lossï¼Œè·³è¿‡è¯¥æ­¥\")\n",
    "            optimizer.zero_grad()\n",
    "            continue\n",
    "\n",
    "        # åå‘ä¼ æ’­ï¼ˆç´¯ç§¯æ¢¯åº¦ï¼‰\n",
    "        total_step_loss.backward()\n",
    "\n",
    "        # æ¢¯åº¦è£å‰ªï¼ˆå…³é”®ï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=CONFIG[\"max_norm\"])\n",
    "\n",
    "        # æ¯gradient_accumulation_stepsæ­¥æ›´æ–°ä¸€æ¬¡å‚æ•°\n",
    "        if (step + 1) % CONFIG[\"gradient_accumulation_steps\"] == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # ç´¯è®¡æŸå¤±ï¼ˆè½¬æ¢ä¸ºfloatï¼Œé˜²æ­¢fp16æº¢å‡ºï¼‰\n",
    "        total_loss += total_step_loss.item() * CONFIG[\"gradient_accumulation_steps\"]\n",
    "        total_ce_loss += ce_loss.item()\n",
    "        total_div_loss += div_loss.item()\n",
    "\n",
    "        # å®æ—¶æ‰“å°è¿›åº¦ï¼ˆå«åˆ›é€ åº¦ç›¸å…³æŒ‡æ ‡ï¼‰\n",
    "        elapsed_time = time.time() - start_time\n",
    "        steps_done = step + 1\n",
    "        steps_total = len(loader)\n",
    "        time_per_step = elapsed_time / steps_done\n",
    "        remaining_steps = steps_total - steps_done\n",
    "        remaining_time = remaining_steps * time_per_step\n",
    "\n",
    "        print(f\"\\rEpoch {epoch+1} | Step {steps_done}/{steps_total} | Total Loss: {total_step_loss.item()*CONFIG['gradient_accumulation_steps']:.4f} | \"\n",
    "              f\"CE Loss: {ce_loss.item():.4f} | Div Loss: {div_loss.item():.4f} | \"\n",
    "              f\"å·²ç”¨: {format_time(elapsed_time)} | å‰©ä½™: {format_time(remaining_time)}\", end=\"\")\n",
    "\n",
    "    # Epochç»“æŸæ‰“å°æ€»ç»“\n",
    "    epoch_avg_loss = total_loss / len(loader)\n",
    "    epoch_avg_ce = total_ce_loss / len(loader)\n",
    "    epoch_avg_div = total_div_loss / len(loader)\n",
    "    total_epoch_time = format_time(time.time() - start_time)\n",
    "    print(f\"\\nEpoch {epoch+1} å®Œæˆ | å¹³å‡æ€»Loss: {epoch_avg_loss:.4f} | å¹³å‡CE Loss: {epoch_avg_ce:.4f} | å¹³å‡Div Loss: {epoch_avg_div:.4f} | æ€»è€—æ—¶: {total_epoch_time}\\n\")\n",
    "\n",
    "# -------------------------\n",
    "# 8. ä¿å­˜ LoRA æ¨¡å‹\n",
    "# -------------------------\n",
    "model.save_pretrained(CONFIG[\"output_dir\"])\n",
    "tokenizer.save_pretrained(CONFIG[\"output_dir\"])\n",
    "print(\"ğŸ‰ LoRA å¾®è°ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜è‡³:\", CONFIG[\"output_dir\"])\n",
    "\n",
    "# -------------------------\n",
    "# 9. é«˜åˆ›é€ åº¦è‹±æ–‡è¯—ç”Ÿæˆå‡½æ•°\n",
    "# -------------------------\n",
    "def generate_creative_poetry(topic, max_len=512):\n",
    "    \"\"\"ç”Ÿæˆé«˜åˆ›é€ åº¦çš„è‹±æ–‡è¯—\"\"\"\n",
    "    model.eval()\n",
    "    # æ„å»ºè‹±æ–‡prompt\n",
    "    prompt = f\"<|im_start|>user\\n{topic}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(CONFIG[\"device\"])\n",
    "    \n",
    "    # é«˜åˆ›é€ åº¦é‡‡æ ·ç­–ç•¥\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_len,\n",
    "            do_sample=True,               \n",
    "            top_k=CONFIG[\"top_k\"],        \n",
    "            top_p=CONFIG[\"top_p\"],        \n",
    "            temperature=CONFIG[\"temperature\"],  \n",
    "            repetition_penalty=CONFIG[\"repetition_penalty\"],  \n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            # ç”Ÿæˆæ—¶æ•°å€¼ç¨³å®šé…ç½®\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,  # é¿å…2å…ƒè¯­æ³•é‡å¤\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "    \n",
    "    # è§£ç å¹¶æ¸…ç†ç”Ÿæˆç»“æœ\n",
    "    poetry = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    poetry = poetry.replace(prompt.replace(\"<|im_start|>\", \"\").replace(\"<|im_end|>\", \"\").strip(), \"\").strip()\n",
    "    return poetry\n",
    "\n",
    "# -------------------------\n",
    "# 10. ç”Ÿæˆç¤ºä¾‹\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # ç”Ÿæˆå…³äº\"starry night\"çš„é«˜åˆ›é€ åº¦è‹±æ–‡è¯—\n",
    "    print(\"\\n=== ç”Ÿæˆé«˜åˆ›é€ åº¦è‹±æ–‡è¯— ===\")\n",
    "    topic = \"starry night\"\n",
    "    poetry = generate_creative_poetry(topic)\n",
    "    print(f\"ä¸»é¢˜: {topic}\\n\")\n",
    "    print(poetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b07bd25-b5e3-4af9-8fae-c260411aef2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TokenizeråŠ è½½å®Œæˆ\n",
      "âœ… åŠ è½½ç¬¬ä¸€æ¡æœ‰æ•ˆæ•°æ®ï¼š\n",
      "ç”¨æˆ·è¾“å…¥: Write a poem in the style of Michelle Menting....\n",
      "è¯—æ­Œå†…å®¹: Dog bone, stapler,\n",
      "cribbage board, garlic press\n",
      "  ...\n",
      "\n",
      "ğŸ“Œ åŠ è½½æ¨¡å‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.48s/it]\n",
      "/tmp/ipykernel_5568/1839768931.py:217: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
      "  with torch.autograd.detect_anomaly():  # æ£€æµ‹æ¢¯åº¦NaN/Inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 630,784 || all params: 7,616,247,296 || trainable%: 0.0083\n",
      "\n",
      "ğŸ“Œ å¼€å§‹å•æ ·æœ¬è®­ç»ƒ...\n",
      "\n",
      "ğŸ“Œ æ•°æ®ç»´åº¦æ ¡éªŒ:\n",
      "Input IDsé•¿åº¦: 128\n",
      "Useréƒ¨åˆ†é•¿åº¦: 19\n",
      "æœ‰æ•ˆæ ‡ç­¾æ•°: 50\n",
      "æ ‡ç­¾ç¤ºä¾‹: tensor([48940, 17240,    11, 35014,  1536,   345,   740,  6066,   424,  4479])\n",
      "\n",
      "===== Epoch 1 | Step 1 =====\n",
      "æ•°æ®è®¾å¤‡: cuda:0\n",
      "Input IDsèŒƒå›´: 11 ~ 151645\n",
      "æœ‰æ•ˆæ ‡ç­¾èŒƒå›´: 11 ~ 78901\n",
      "Logitså½¢çŠ¶: torch.Size([1, 128, 152064])\n",
      "Logitså‡å€¼: -2.4702 | æœ€å¤§å€¼: 26.6663 | æœ€å°å€¼: -28.6940\n",
      "åŸå§‹Losså€¼: 4.8307\n",
      "æ¢¯åº¦èŒƒæ•°: 1.2347\n",
      "\n",
      "âœ… Epoch 1 å®Œæˆ | å¹³å‡Loss: 4.8307 | è€—æ—¶: 00:00:00\n",
      "\n",
      "ğŸ“Œ æ•°æ®ç»´åº¦æ ¡éªŒ:\n",
      "Input IDsé•¿åº¦: 128\n",
      "Useréƒ¨åˆ†é•¿åº¦: 19\n",
      "æœ‰æ•ˆæ ‡ç­¾æ•°: 50\n",
      "æ ‡ç­¾ç¤ºä¾‹: tensor([48940, 17240,    11, 35014,  1536,   345,   740,  6066,   424,  4479])\n",
      "\n",
      "===== Epoch 2 | Step 1 =====\n",
      "æ•°æ®è®¾å¤‡: cuda:0\n",
      "Input IDsèŒƒå›´: 11 ~ 151645\n",
      "æœ‰æ•ˆæ ‡ç­¾èŒƒå›´: 11 ~ 78901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logitså½¢çŠ¶: torch.Size([1, 128, 152064])\n",
      "Logitså‡å€¼: -2.4705 | æœ€å¤§å€¼: 26.6685 | æœ€å°å€¼: -28.6951\n",
      "åŸå§‹Losså€¼: 4.8281\n",
      "æ¢¯åº¦èŒƒæ•°: 1.2484\n",
      "\n",
      "âœ… Epoch 2 å®Œæˆ | å¹³å‡Loss: 4.8281 | è€—æ—¶: 00:00:00\n",
      "\n",
      "ğŸ‰ å•æ ·æœ¬è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜è‡³: /root/autodl-tmp/Pro/qwen2.5-sft-debug\n",
      "\n",
      "ğŸ“Œ ç”Ÿæˆç»“æœéªŒè¯:\n",
      "ä¸»é¢˜: Write a poem in the style of Michelle Menting.\n",
      "ç”Ÿæˆè¯—æ­Œ:\n",
      "In the stillness of dawn's early light,\n",
      "I find solace, as I always do.\n",
      "The world awakes to gentle sight,\n",
      "Where whispers weave through morning dew.\n",
      "\n",
      "A sparrow chirps its lullaby,\n",
      "As if it knows what lies ahead.\n",
      "Its song is soft and tenderly,\n",
      "A melody for weary heads.\n",
      "\n",
      "Across the fields where golden hues lie,\n",
      "And wildflowers nod under blue skies.\n",
      "My heart beats with an ancient sighâ€”\n",
      "A rhythm known from times agone.\n",
      "\n",
      "For every step upon this earth,\n",
      "Is tread by souls who sought before;\n",
      "To chase their dreams, though hearts might hurt,\n",
      "And hold on tight against life's shore.\n",
      "\n",
      "So here we walk, hand in hand again,\n",
      "Through stories old yet fresh and new.\n",
      "Each moment holds both joy and pain,\n",
      "But together, none can break us through.\n",
      "\n",
      "Like rivers flowing ever free,\n",
      "Our lives will twist and turn each day.\n",
      "Yet in our struggles, we'll decree,\n",
      "That love is stronger than decay.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import PreTrainedTokenizerFast, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import nltk\n",
    "\n",
    "# -------------------------\n",
    "# 0. åŸºç¡€é…ç½®ï¼ˆä¼˜åŒ–ç”Ÿæˆå‚æ•°ï¼‰\n",
    "# -------------------------\n",
    "# ç¼“å­˜é…ç½®\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/root/autodl-tmp/hf_cache\"\n",
    "os.environ[\"HF_HOME\"] = \"/root/autodl-tmp/hf_cache\"\n",
    "torch.hub.set_dir(\"/root/autodl-tmp/torch_cache\")\n",
    "os.environ[\"TORCH_HOME\"] = \"/root/autodl-tmp/torch_cache\"\n",
    "os.makedirs(\"/root/autodl-tmp/offload\", exist_ok=True)\n",
    "\n",
    "# è°ƒè¯•ä¸“ç”¨é…ç½®ï¼ˆæ ¸å¿ƒå¢åŠ ç”Ÿæˆå‚æ•°ï¼‰\n",
    "CONFIG = {\n",
    "    \"model_name_or_path\": \"/root/autodl-tmp/qwen2.5-7b/qwen2.5-7b\",\n",
    "    \"data_path\": \"/root/autodl-tmp/Pro/poetry_sft.jsonl\",\n",
    "    \"output_dir\": \"/root/autodl-tmp/Pro/qwen2.5-sft-debug\",\n",
    "    \"num_train_epochs\": 2,  # ä¿æŒåŸè®­ç»ƒè½®æ•°\n",
    "    \"batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"max_seq_length\": 128,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"offload_folder\": \"/root/autodl-tmp/offload\",\n",
    "    # æ–°å¢ç”Ÿæˆä¸“ç”¨é…ç½®\n",
    "    \"gen_max_new_tokens\": 200,  # ä»…æ–°å¢200ä¸ªtokenï¼ˆé¿å…åŒ…å«prompté•¿åº¦ï¼‰\n",
    "    \"gen_temperature\": 0.85,\n",
    "    \"gen_top_k\": 30,\n",
    "    \"gen_top_p\": 0.92,\n",
    "    \"gen_repetition_penalty\": 1.15\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# 1. åŠ è½½Tokenizerï¼ˆä¼˜åŒ–ç‰¹æ®Štokenå¤„ç†ï¼‰\n",
    "# -------------------------\n",
    "tokenizer_file = os.path.join(CONFIG[\"model_name_or_path\"], \"tokenizer.json\")\n",
    "assert os.path.exists(tokenizer_file), f\"Tokenizeræ–‡ä»¶ä¸å­˜åœ¨: {tokenizer_file}\"\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_file)\n",
    "# å¼ºåˆ¶å¯¹é½Qwen2.5ç‰¹æ®Štoken\n",
    "tokenizer.eos_token = \"<|endoftext|>\"\n",
    "tokenizer.bos_token = \"<|startoftext|>\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# æ·»åŠ å¯¹è¯ä¸“ç”¨ç‰¹æ®Štoken\n",
    "tokenizer.add_special_tokens({\n",
    "    \"additional_special_tokens\": [\"<|im_start|>\", \"<|im_end|>\"]\n",
    "})\n",
    "print(\"âœ… TokenizeråŠ è½½å®Œæˆ\")\n",
    "\n",
    "# -------------------------\n",
    "# 2. ä»…åŠ è½½ç¬¬ä¸€æ¡æ•°æ®çš„Datasetï¼ˆæ— ä¿®æ”¹ï¼‰\n",
    "# -------------------------\n",
    "class SingleSamplePoetryDataset(Dataset):\n",
    "    def __init__(self, jsonl_path, tokenizer, max_len=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        # ä»…åŠ è½½ç¬¬ä¸€æ¡æœ‰æ•ˆæ•°æ®\n",
    "        self.single_sample = self._load_first_valid_sample(jsonl_path)\n",
    "        assert self.single_sample is not None, \"æ— æœ‰æ•ˆæ•°æ®ï¼\"\n",
    "        print(f\"âœ… åŠ è½½ç¬¬ä¸€æ¡æœ‰æ•ˆæ•°æ®ï¼š\")\n",
    "        print(f\"ç”¨æˆ·è¾“å…¥: {self.single_sample['user'][:50]}...\")\n",
    "        print(f\"è¯—æ­Œå†…å®¹: {self.single_sample['assistant'][:50]}...\")\n",
    "\n",
    "    def _load_first_valid_sample(self, jsonl_path):\n",
    "        \"\"\"ä»…åŠ è½½ç¬¬ä¸€æ¡æœ‰æ•ˆæ•°æ®\"\"\"\n",
    "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    sample = json.loads(line)\n",
    "                    # æ ¡éªŒæ•°æ®æ ¼å¼\n",
    "                    if len(sample.get(\"messages\", [])) < 2:\n",
    "                        continue\n",
    "                    user_text = sample[\"messages\"][0][\"content\"].strip()\n",
    "                    assistant_text = sample[\"messages\"][1][\"content\"].strip()\n",
    "                    if user_text and assistant_text:\n",
    "                        return {\"user\": user_text, \"assistant\": assistant_text}\n",
    "                except Exception as e:\n",
    "                    print(f\"è·³è¿‡æ— æ•ˆè¡Œ: {e}\")\n",
    "                    continue\n",
    "        return None\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"å¼ºåˆ¶é•¿åº¦ä¸º1ï¼Œä»…è¿”å›ç¬¬ä¸€æ¡æ•°æ®\"\"\"\n",
    "        return 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"æ— è®ºidxæ˜¯å¤šå°‘ï¼Œéƒ½è¿”å›ç¬¬ä¸€æ¡æ•°æ®\"\"\"\n",
    "        sample = self.single_sample\n",
    "        # æ„å»ºprompt\n",
    "        prompt = f\"<|im_start|>user\\n{sample['user']}<|im_end|>\\n<|im_start|>assistant\\n{sample['assistant']}<|im_end|>\"\n",
    "        \n",
    "        # ç¼–ç ï¼ˆè¯¦ç»†æ—¥å¿—ï¼‰\n",
    "        encoding = self.tokenizer(\n",
    "            prompt,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            padding_side=\"right\",\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = encoding.input_ids.squeeze(0).long()\n",
    "        attention_mask = encoding.attention_mask.squeeze(0).long()\n",
    "\n",
    "        # æ„å»ºæ ‡ç­¾ï¼ˆä»…è®¡ç®—assistantéƒ¨åˆ†lossï¼‰\n",
    "        user_prompt = f\"<|im_start|>user\\n{sample['user']}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "        user_len = len(self.tokenizer.encode(user_prompt, truncation=True, max_length=self.max_len))\n",
    "        labels = torch.full_like(input_ids, -100)  # å¿½ç•¥useréƒ¨åˆ†\n",
    "        # ä»…æ ‡æ³¨assistantå‰50ä¸ªtokenï¼ˆå‡å°‘è®¡ç®—ï¼‰\n",
    "        labels[user_len:min(user_len+50, self.max_len)] = input_ids[user_len:min(user_len+50, self.max_len)]\n",
    "\n",
    "        # è°ƒè¯•ï¼šæ‰“å°å…³é”®ç»´åº¦\n",
    "        print(f\"\\nğŸ“Œ æ•°æ®ç»´åº¦æ ¡éªŒ:\")\n",
    "        print(f\"Input IDsé•¿åº¦: {len(input_ids)}\")\n",
    "        print(f\"Useréƒ¨åˆ†é•¿åº¦: {user_len}\")\n",
    "        print(f\"æœ‰æ•ˆæ ‡ç­¾æ•°: {torch.sum(labels != -100).item()}\")\n",
    "        print(f\"æ ‡ç­¾ç¤ºä¾‹: {labels[user_len:user_len+10]}\")\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }\n",
    "\n",
    "# åŠ è½½å•æ ·æœ¬æ•°æ®é›†\n",
    "dataset = SingleSamplePoetryDataset(CONFIG[\"data_path\"], tokenizer, CONFIG[\"max_seq_length\"])\n",
    "# å¼ºåˆ¶åªåŠ è½½ä¸€æ¡æ•°æ®ï¼ˆshuffle=Falseé¿å…æ‰“ä¹±ï¼‰\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")\n",
    "assert len(loader) == 1, \"æ•°æ®åŠ è½½å™¨é•¿åº¦ä¸ä¸º1ï¼\"\n",
    "\n",
    "# -------------------------\n",
    "# 3. åŠ è½½æ¨¡å‹ï¼ˆFP32+æç®€LoRAï¼‰ï¼ˆæ— ä¿®æ”¹ï¼‰\n",
    "# -------------------------\n",
    "print(\"\\nğŸ“Œ åŠ è½½æ¨¡å‹...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"model_name_or_path\"],\n",
    "    torch_dtype=torch.float32,  # å¼ºåˆ¶FP32ï¼Œé¿å…æ•°å€¼æº¢å‡º\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=CONFIG[\"offload_folder\"],\n",
    "    attn_implementation=\"eager\",  # ç¦ç”¨flash attention\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "model.train()\n",
    "\n",
    "# æç®€LoRAé…ç½®ï¼ˆä»…è®­ç»ƒq/væŠ•å½±ï¼‰\n",
    "lora_cfg = LoraConfig(\n",
    "    r=2,  # æå°çš„rå€¼ï¼Œå‡å°‘è®¡ç®—\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.01,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    bias=\"none\"\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# -------------------------\n",
    "# 4. ä¼˜åŒ–å™¨+æŸå¤±å‡½æ•°ï¼ˆæ— ä¿®æ”¹ï¼‰\n",
    "# -------------------------\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG[\"learning_rate\"],\n",
    "    eps=1e-8,\n",
    "    weight_decay=0.001\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-100, reduction=\"mean\")\n",
    "\n",
    "# -------------------------\n",
    "# 5. å•æ ·æœ¬è®­ç»ƒå¾ªç¯ï¼ˆè°ƒæ•´æ¢¯åº¦è£å‰ªé˜ˆå€¼ï¼‰\n",
    "# -------------------------\n",
    "def format_time(seconds):\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = int(seconds % 60)\n",
    "    return f\"{hours:02d}:{minutes:02d}:{secs:02d}\"\n",
    "\n",
    "print(\"\\nğŸ“Œ å¼€å§‹å•æ ·æœ¬è®­ç»ƒ...\")\n",
    "for epoch in range(CONFIG[\"num_train_epochs\"]):\n",
    "    epoch_start = time.time()\n",
    "    total_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # ä»…éå†å”¯ä¸€çš„ä¸€ä¸ªbatchï¼ˆç¬¬ä¸€æ¡æ•°æ®ï¼‰\n",
    "    for step, batch in enumerate(loader):\n",
    "        print(f\"\\n===== Epoch {epoch+1} | Step {step+1} =====\")\n",
    "        \n",
    "        # åŠ è½½æ•°æ®åˆ°è®¾å¤‡\n",
    "        input_ids = batch[\"input_ids\"].to(CONFIG[\"device\"])\n",
    "        labels = batch[\"labels\"].to(CONFIG[\"device\"])\n",
    "        attention_mask = batch[\"attention_mask\"].to(CONFIG[\"device\"])\n",
    "\n",
    "        # è°ƒè¯•ï¼šæ‰“å°æ•°æ®è®¾å¤‡å’Œæ•°å€¼èŒƒå›´\n",
    "        print(f\"æ•°æ®è®¾å¤‡: {input_ids.device}\")\n",
    "        print(f\"Input IDsèŒƒå›´: {input_ids.min().item()} ~ {input_ids.max().item()}\")\n",
    "        print(f\"æœ‰æ•ˆæ ‡ç­¾èŒƒå›´: {labels[labels != -100].min().item()} ~ {labels[labels != -100].max().item()}\")\n",
    "\n",
    "        # å‰å‘ä¼ æ’­ï¼ˆå¸¦æ¢¯åº¦æ£€æµ‹ï¼‰\n",
    "        with torch.autograd.detect_anomaly():  # æ£€æµ‹æ¢¯åº¦NaN/Inf\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            loss = outputs.loss\n",
    "\n",
    "        # è°ƒè¯•ï¼šæ‰“å°å…³é”®æ•°å€¼\n",
    "        print(f\"Logitså½¢çŠ¶: {logits.shape}\")\n",
    "        print(f\"Logitså‡å€¼: {logits.mean().item():.4f} | æœ€å¤§å€¼: {logits.max().item():.4f} | æœ€å°å€¼: {logits.min().item():.4f}\")\n",
    "        print(f\"åŸå§‹Losså€¼: {loss.item():.4f}\")\n",
    "\n",
    "        # æ£€æŸ¥Lossæ˜¯å¦ä¸ºNaN\n",
    "        if torch.isnan(loss):\n",
    "            print(\"âŒ Lossä¸ºNaNï¼\")\n",
    "            # æ‰“å°æ›´è¯¦ç»†çš„logitsä¿¡æ¯\n",
    "            print(f\"Logitsæ˜¯å¦å«NaN: {torch.isnan(logits).any().item()}\")\n",
    "            print(f\"Logitsæ˜¯å¦å«Inf: {torch.isinf(logits).any().item()}\")\n",
    "            break\n",
    "\n",
    "        # åå‘ä¼ æ’­ï¼ˆå¸¦æ¢¯åº¦æ£€æµ‹ï¼‰\n",
    "        loss.backward()\n",
    "\n",
    "        # è°ƒè¯•ï¼šæ‰“å°æ¢¯åº¦ä¿¡æ¯ï¼ˆè°ƒæ•´è£å‰ªé˜ˆå€¼åˆ°1.5ï¼‰\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.5)\n",
    "        print(f\"æ¢¯åº¦èŒƒæ•°: {grad_norm.item():.4f}\")\n",
    "\n",
    "        # æ›´æ–°å‚æ•°\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # æ‰“å°Epochæ€»ç»“\n",
    "    epoch_time = format_time(time.time() - epoch_start)\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"\\nâœ… Epoch {epoch+1} å®Œæˆ | å¹³å‡Loss: {avg_loss:.4f} | è€—æ—¶: {epoch_time}\")\n",
    "\n",
    "# -------------------------\n",
    "# 6. ä¿å­˜æ¨¡å‹ï¼ˆè°ƒè¯•ç”¨ï¼‰ï¼ˆæ— ä¿®æ”¹ï¼‰\n",
    "# -------------------------\n",
    "model.save_pretrained(CONFIG[\"output_dir\"])\n",
    "tokenizer.save_pretrained(CONFIG[\"output_dir\"])\n",
    "print(f\"\\nğŸ‰ å•æ ·æœ¬è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜è‡³: {CONFIG['output_dir']}\")\n",
    "\n",
    "# -------------------------\n",
    "# 7. éªŒè¯ç”Ÿæˆï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼šè§£å†³ä¸å®Œæ•´+æ®‹ç•™æ ‡ç­¾ï¼‰\n",
    "# -------------------------\n",
    "def generate_single_sample():\n",
    "    model.eval()\n",
    "    sample = dataset.single_sample\n",
    "    # æ„å»ºçº¯promptï¼ˆä¸å«ç­”æ¡ˆï¼‰\n",
    "    prompt = f\"<|im_start|>user\\n{sample['user']}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(CONFIG[\"device\"])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=CONFIG[\"gen_max_new_tokens\"],  # ä»…æ–°å¢tokenæ•°ï¼ˆå…³é”®ï¼ï¼‰\n",
    "            do_sample=True,\n",
    "            temperature=CONFIG[\"gen_temperature\"],\n",
    "            top_k=CONFIG[\"gen_top_k\"],\n",
    "            top_p=CONFIG[\"gen_top_p\"],\n",
    "            repetition_penalty=CONFIG[\"gen_repetition_penalty\"],\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            early_stopping=True,  # é‡åˆ°eosè‡ªåŠ¨åœæ­¢\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "    \n",
    "    # ä¼˜åŒ–è§£ç é€»è¾‘ï¼šç§»é™¤ç‰¹æ®Štoken+ä»…ä¿ç•™assistantéƒ¨åˆ†\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    # æˆªå–assistantä¹‹åçš„å†…å®¹\n",
    "    if \"<|im_start|>assistant\\n\" in generated_text:\n",
    "        generated_poem = generated_text.split(\"<|im_start|>assistant\\n\")[1]\n",
    "        # ç§»é™¤æœ«å°¾çš„eos token\n",
    "        generated_poem = generated_poem.replace(\"<|endoftext|>\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
    "    else:\n",
    "        generated_poem = generated_text.strip()\n",
    "    \n",
    "    print(\"\\nğŸ“Œ ç”Ÿæˆç»“æœéªŒè¯:\")\n",
    "    print(f\"ä¸»é¢˜: {sample['user']}\")\n",
    "    print(\"ç”Ÿæˆè¯—æ­Œ:\")\n",
    "    print(generated_poem)\n",
    "\n",
    "generate_single_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c96026e-b029-4002-991f-7da2a87ff284",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/qwen_text/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TokenizeråŠ è½½å®Œæˆ\n",
      "âš ï¸  è·³è¿‡ç¬¬920è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬946è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4092è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4192è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4194è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4199è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4200è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4224è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4242è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4243è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4252è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4254è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4301è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4302è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4303è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4457è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4458è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4459è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4541è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4602è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4603è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4762è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4763è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5447è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5452è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5595è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5597è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5675è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5677è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5789è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5790è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5791è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5792è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5829è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5836è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5837è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5853è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5861è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5969è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5970è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5989è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5990è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬6264è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬6334è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬6335è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬6367è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬6368è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬6369è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬6370è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬6371è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬6372è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬6373è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬6374è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬6375è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬6376è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬6432è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬6515è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬7667è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬8493è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬9186è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬9902è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬10128è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬10133è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬10162è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬10168è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬10170è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬10234è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12027è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12243è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12244è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12245è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12246è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12247è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12248è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12249è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12250è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12251è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12252è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12253è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12254è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12255è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12256è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12257è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12258è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12259è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13828è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13829è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13837è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13838è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13839è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13840è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13841è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13842è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13843è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13844è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13845è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13846è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13847è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13848è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13851è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13852è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âœ… åŠ è½½æœ‰æ•ˆè¯—æ­Œæ•°æ®: 13753 æ¡\n",
      "ğŸ“Œ å…¨é‡è®­ç»ƒé…ç½®ï¼š\n",
      "- æ€»æ•°æ®é‡ï¼š13753 æ¡\n",
      "- æ¯Epochæ­¥æ•°ï¼š6876\n",
      "- æ€»è®­ç»ƒæ­¥æ•°ï¼š20628\n",
      "- æœ‰æ•ˆæ‰¹æ¬¡å¤§å°ï¼š4\n",
      "\n",
      "ğŸ“Œ åŠ è½½Qwen2.5-7Bæ¨¡å‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,261,568 || all params: 7,616,878,080 || trainable%: 0.0166\n",
      "\n",
      "ğŸš€ å¼€å§‹å…¨é‡æ•°æ®è®­ç»ƒ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6091/124602864.py:273: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
      "  with torch.autograd.detect_anomaly():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/3] Step 10/20628 | Avg Loss: 4.4161 | Grad Norm: 1.5411 | Step Time: 0.83s\n",
      "[Epoch 1/3] Step 20/20628 | Avg Loss: 6.2027 | Grad Norm: 5.2924 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 30/20628 | Avg Loss: 6.3657 | Grad Norm: 5.8603 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 40/20628 | Avg Loss: 6.9830 | Grad Norm: 9.0553 | Step Time: 0.68s\n",
      "[Epoch 1/3] Step 50/20628 | Avg Loss: 5.6384 | Grad Norm: 5.7839 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 60/20628 | Avg Loss: 4.5480 | Grad Norm: 5.5153 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 70/20628 | Avg Loss: 6.2103 | Grad Norm: 1.5747 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 80/20628 | Avg Loss: 5.6445 | Grad Norm: 8.2896 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 90/20628 | Avg Loss: 5.3434 | Grad Norm: 5.1194 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 100/20628 | Avg Loss: 6.2313 | Grad Norm: 15.5805 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 110/20628 | Avg Loss: 5.6932 | Grad Norm: 8.3427 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 120/20628 | Avg Loss: 5.2875 | Grad Norm: 3.6313 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 130/20628 | Avg Loss: 4.7730 | Grad Norm: 15.9210 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 140/20628 | Avg Loss: 4.9559 | Grad Norm: 10.1339 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 150/20628 | Avg Loss: 4.9529 | Grad Norm: 10.4422 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 160/20628 | Avg Loss: 4.7357 | Grad Norm: 7.6256 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 170/20628 | Avg Loss: 4.7144 | Grad Norm: 0.9785 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 180/20628 | Avg Loss: 5.9765 | Grad Norm: 18.0969 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 190/20628 | Avg Loss: 4.4053 | Grad Norm: 1.2475 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 200/20628 | Avg Loss: 4.8837 | Grad Norm: 8.6499 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 210/20628 | Avg Loss: 4.7646 | Grad Norm: 30.8000 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 220/20628 | Avg Loss: 4.0696 | Grad Norm: 25.4729 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 230/20628 | Avg Loss: 3.5346 | Grad Norm: 15.0315 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 240/20628 | Avg Loss: 3.7501 | Grad Norm: 9.1791 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 250/20628 | Avg Loss: 3.6659 | Grad Norm: 1.7367 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 260/20628 | Avg Loss: 3.7532 | Grad Norm: 3.2829 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 270/20628 | Avg Loss: 3.5154 | Grad Norm: 12.9398 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 280/20628 | Avg Loss: 3.5786 | Grad Norm: 7.3349 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 290/20628 | Avg Loss: 3.6004 | Grad Norm: 16.7752 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 300/20628 | Avg Loss: 3.4058 | Grad Norm: 5.2745 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 310/20628 | Avg Loss: 3.5114 | Grad Norm: 1.6310 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 320/20628 | Avg Loss: 3.0394 | Grad Norm: 6.4556 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 330/20628 | Avg Loss: 3.5300 | Grad Norm: 1.1503 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 340/20628 | Avg Loss: 3.3579 | Grad Norm: 2.4724 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 350/20628 | Avg Loss: 2.9951 | Grad Norm: 6.2826 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 360/20628 | Avg Loss: 3.2512 | Grad Norm: 1.5647 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 370/20628 | Avg Loss: 2.9497 | Grad Norm: 1.8742 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 380/20628 | Avg Loss: 2.8585 | Grad Norm: 2.0121 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 390/20628 | Avg Loss: 3.0563 | Grad Norm: 2.0113 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 400/20628 | Avg Loss: 3.1107 | Grad Norm: 1.1108 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 410/20628 | Avg Loss: 2.7851 | Grad Norm: 0.8626 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 420/20628 | Avg Loss: 3.1663 | Grad Norm: 0.8725 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 430/20628 | Avg Loss: 2.1058 | Grad Norm: 1.0112 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 440/20628 | Avg Loss: 3.2660 | Grad Norm: 0.9817 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 450/20628 | Avg Loss: 2.5487 | Grad Norm: 0.9593 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 460/20628 | Avg Loss: 3.0525 | Grad Norm: 1.7033 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 470/20628 | Avg Loss: 3.2187 | Grad Norm: 1.5924 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 480/20628 | Avg Loss: 2.9066 | Grad Norm: 0.8958 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 490/20628 | Avg Loss: 3.2822 | Grad Norm: 1.3335 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 500/20628 | Avg Loss: 3.0840 | Grad Norm: 5.5130 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 510/20628 | Avg Loss: 2.7353 | Grad Norm: 1.6797 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 520/20628 | Avg Loss: 2.5493 | Grad Norm: 1.5988 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 530/20628 | Avg Loss: 2.8191 | Grad Norm: 1.5617 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 540/20628 | Avg Loss: 3.0168 | Grad Norm: 0.8700 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 550/20628 | Avg Loss: 2.5618 | Grad Norm: 6.0158 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 560/20628 | Avg Loss: 2.4889 | Grad Norm: 1.3289 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 570/20628 | Avg Loss: 2.9791 | Grad Norm: 0.9881 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 580/20628 | Avg Loss: 2.8806 | Grad Norm: 0.8796 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 590/20628 | Avg Loss: 2.6906 | Grad Norm: 0.8051 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 600/20628 | Avg Loss: 3.3796 | Grad Norm: 0.7684 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 610/20628 | Avg Loss: 2.4827 | Grad Norm: 1.1387 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 620/20628 | Avg Loss: 2.6387 | Grad Norm: 1.2055 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 630/20628 | Avg Loss: 2.7527 | Grad Norm: 0.8772 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 640/20628 | Avg Loss: 2.8325 | Grad Norm: 0.6727 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 650/20628 | Avg Loss: 2.5728 | Grad Norm: 0.7170 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 660/20628 | Avg Loss: 3.1159 | Grad Norm: 0.8244 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 670/20628 | Avg Loss: 2.6465 | Grad Norm: 0.7685 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 680/20628 | Avg Loss: 2.7599 | Grad Norm: 0.8816 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 690/20628 | Avg Loss: 3.0800 | Grad Norm: 0.6720 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 700/20628 | Avg Loss: 3.0056 | Grad Norm: 0.5526 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 710/20628 | Avg Loss: 2.5655 | Grad Norm: 0.7863 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 720/20628 | Avg Loss: 2.5649 | Grad Norm: 0.9021 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 730/20628 | Avg Loss: 3.0522 | Grad Norm: 0.6675 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 740/20628 | Avg Loss: 2.6037 | Grad Norm: 0.9162 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 750/20628 | Avg Loss: 2.4297 | Grad Norm: 0.6744 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 760/20628 | Avg Loss: 2.5302 | Grad Norm: 0.9177 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 770/20628 | Avg Loss: 2.7557 | Grad Norm: 0.6592 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 780/20628 | Avg Loss: 2.8198 | Grad Norm: 2.0745 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 790/20628 | Avg Loss: 2.1249 | Grad Norm: 1.8246 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 800/20628 | Avg Loss: 2.9549 | Grad Norm: 0.7568 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 810/20628 | Avg Loss: 2.4405 | Grad Norm: 0.8191 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 820/20628 | Avg Loss: 3.0088 | Grad Norm: 0.9679 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 830/20628 | Avg Loss: 2.8211 | Grad Norm: 0.7074 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 840/20628 | Avg Loss: 2.9723 | Grad Norm: 0.6119 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 850/20628 | Avg Loss: 2.9272 | Grad Norm: 0.6789 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 860/20628 | Avg Loss: 3.1165 | Grad Norm: 0.7277 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 870/20628 | Avg Loss: 2.5183 | Grad Norm: 0.6289 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 880/20628 | Avg Loss: 3.2941 | Grad Norm: 0.7717 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 890/20628 | Avg Loss: 3.2886 | Grad Norm: 0.8347 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 900/20628 | Avg Loss: 3.1030 | Grad Norm: 0.8350 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 910/20628 | Avg Loss: 2.9727 | Grad Norm: 0.4783 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 920/20628 | Avg Loss: 3.0614 | Grad Norm: 0.7966 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 930/20628 | Avg Loss: 3.1545 | Grad Norm: 1.0879 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 940/20628 | Avg Loss: 3.0491 | Grad Norm: 0.8008 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 950/20628 | Avg Loss: 2.8323 | Grad Norm: 0.6225 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 960/20628 | Avg Loss: 3.0994 | Grad Norm: 0.7679 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 970/20628 | Avg Loss: 3.0020 | Grad Norm: 0.7653 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 980/20628 | Avg Loss: 3.0215 | Grad Norm: 0.8096 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 990/20628 | Avg Loss: 2.5570 | Grad Norm: 0.5854 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1000/20628 | Avg Loss: 2.7922 | Grad Norm: 1.5049 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1010/20628 | Avg Loss: 2.5370 | Grad Norm: 0.7824 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 1020/20628 | Avg Loss: 2.4657 | Grad Norm: 1.6551 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1030/20628 | Avg Loss: 2.6627 | Grad Norm: 1.0139 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1040/20628 | Avg Loss: 2.6952 | Grad Norm: 0.9655 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 1050/20628 | Avg Loss: 2.7914 | Grad Norm: 1.5493 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 1060/20628 | Avg Loss: 2.9637 | Grad Norm: 0.7780 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1070/20628 | Avg Loss: 2.4369 | Grad Norm: 0.7461 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 1080/20628 | Avg Loss: 2.9355 | Grad Norm: 0.8453 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 1090/20628 | Avg Loss: 2.9870 | Grad Norm: 0.8028 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1100/20628 | Avg Loss: 2.9974 | Grad Norm: 0.8895 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 1110/20628 | Avg Loss: 3.0201 | Grad Norm: 0.6781 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1120/20628 | Avg Loss: 2.9116 | Grad Norm: 1.3336 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1130/20628 | Avg Loss: 2.8363 | Grad Norm: 0.7822 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1140/20628 | Avg Loss: 2.6643 | Grad Norm: 0.7164 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1150/20628 | Avg Loss: 2.5280 | Grad Norm: 0.9711 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1160/20628 | Avg Loss: 3.3743 | Grad Norm: 0.8652 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1170/20628 | Avg Loss: 2.5510 | Grad Norm: 0.9373 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 1180/20628 | Avg Loss: 2.9006 | Grad Norm: 0.8127 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1190/20628 | Avg Loss: 3.0329 | Grad Norm: 0.8049 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1200/20628 | Avg Loss: 3.0346 | Grad Norm: 0.8475 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1210/20628 | Avg Loss: 2.8853 | Grad Norm: 0.8256 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1220/20628 | Avg Loss: 2.9270 | Grad Norm: 1.6935 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1230/20628 | Avg Loss: 3.1578 | Grad Norm: 0.8042 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1240/20628 | Avg Loss: 2.8489 | Grad Norm: 0.8499 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 1250/20628 | Avg Loss: 2.9502 | Grad Norm: 0.8602 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1260/20628 | Avg Loss: 2.6694 | Grad Norm: 0.7795 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1270/20628 | Avg Loss: 2.8837 | Grad Norm: 0.6240 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1280/20628 | Avg Loss: 2.6232 | Grad Norm: 0.7397 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1290/20628 | Avg Loss: 3.3109 | Grad Norm: 1.2772 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 1300/20628 | Avg Loss: 3.2412 | Grad Norm: 0.9700 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 1310/20628 | Avg Loss: 2.9936 | Grad Norm: 0.9758 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1320/20628 | Avg Loss: 2.8391 | Grad Norm: 0.9245 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1330/20628 | Avg Loss: 3.2491 | Grad Norm: 0.8330 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 1340/20628 | Avg Loss: 2.8594 | Grad Norm: 0.9583 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1350/20628 | Avg Loss: 2.9934 | Grad Norm: 0.8111 | Step Time: 0.68s\n",
      "[Epoch 1/3] Step 1360/20628 | Avg Loss: 2.6006 | Grad Norm: 0.7599 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1370/20628 | Avg Loss: 2.8575 | Grad Norm: 0.5938 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1380/20628 | Avg Loss: 2.7964 | Grad Norm: 0.8610 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1390/20628 | Avg Loss: 3.1374 | Grad Norm: 1.5824 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1400/20628 | Avg Loss: 2.7990 | Grad Norm: 0.7420 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1410/20628 | Avg Loss: 2.7593 | Grad Norm: 0.7567 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1420/20628 | Avg Loss: 2.8157 | Grad Norm: 0.8789 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1430/20628 | Avg Loss: 3.3090 | Grad Norm: 0.9905 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1440/20628 | Avg Loss: 2.5043 | Grad Norm: 0.9324 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1450/20628 | Avg Loss: 2.6266 | Grad Norm: 0.6952 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1460/20628 | Avg Loss: 3.1368 | Grad Norm: 1.0511 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1470/20628 | Avg Loss: 2.8771 | Grad Norm: 1.7886 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1480/20628 | Avg Loss: 2.9421 | Grad Norm: 0.7064 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1490/20628 | Avg Loss: 3.0954 | Grad Norm: 0.8216 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 1500/20628 | Avg Loss: 3.1624 | Grad Norm: 0.7944 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1510/20628 | Avg Loss: 3.6078 | Grad Norm: 0.9060 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 1520/20628 | Avg Loss: 3.0237 | Grad Norm: 0.7728 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1530/20628 | Avg Loss: 3.1863 | Grad Norm: 1.1630 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1540/20628 | Avg Loss: 2.7210 | Grad Norm: 0.8822 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 1550/20628 | Avg Loss: 2.7784 | Grad Norm: 0.7494 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1560/20628 | Avg Loss: 3.2920 | Grad Norm: 0.8463 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1570/20628 | Avg Loss: 2.3718 | Grad Norm: 0.5494 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1580/20628 | Avg Loss: 2.9441 | Grad Norm: 0.6376 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1590/20628 | Avg Loss: 3.0062 | Grad Norm: 0.7481 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1600/20628 | Avg Loss: 2.9629 | Grad Norm: 1.0582 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1610/20628 | Avg Loss: 2.8800 | Grad Norm: 0.9214 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1620/20628 | Avg Loss: 3.0667 | Grad Norm: 0.8090 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1630/20628 | Avg Loss: 3.0566 | Grad Norm: 0.9600 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1640/20628 | Avg Loss: 2.8873 | Grad Norm: 0.7700 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1650/20628 | Avg Loss: 3.1117 | Grad Norm: 0.7866 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1660/20628 | Avg Loss: 2.8986 | Grad Norm: 0.8966 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 1670/20628 | Avg Loss: 2.7836 | Grad Norm: 0.8228 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1680/20628 | Avg Loss: 2.9997 | Grad Norm: 0.7137 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 1690/20628 | Avg Loss: 2.7755 | Grad Norm: 0.8023 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1700/20628 | Avg Loss: 3.0275 | Grad Norm: 1.0315 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1710/20628 | Avg Loss: 2.7382 | Grad Norm: 0.7636 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1720/20628 | Avg Loss: 2.9993 | Grad Norm: 0.6791 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1730/20628 | Avg Loss: 3.1110 | Grad Norm: 0.8015 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 1740/20628 | Avg Loss: 2.7849 | Grad Norm: 0.8498 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 1750/20628 | Avg Loss: 3.3740 | Grad Norm: 0.7421 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1760/20628 | Avg Loss: 2.7153 | Grad Norm: 1.0567 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1770/20628 | Avg Loss: 3.2688 | Grad Norm: 0.9093 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1780/20628 | Avg Loss: 2.8502 | Grad Norm: 1.0218 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1790/20628 | Avg Loss: 2.5990 | Grad Norm: 0.7436 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1800/20628 | Avg Loss: 2.8913 | Grad Norm: 0.9025 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 1810/20628 | Avg Loss: 3.0104 | Grad Norm: 0.7893 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1820/20628 | Avg Loss: 2.9271 | Grad Norm: 1.6156 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 1830/20628 | Avg Loss: 2.6177 | Grad Norm: 0.8571 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1840/20628 | Avg Loss: 2.8442 | Grad Norm: 0.8193 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1850/20628 | Avg Loss: 2.9946 | Grad Norm: 0.8649 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1860/20628 | Avg Loss: 2.9878 | Grad Norm: 0.9161 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1870/20628 | Avg Loss: 2.7766 | Grad Norm: 0.8283 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1880/20628 | Avg Loss: 3.0991 | Grad Norm: 0.7955 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1890/20628 | Avg Loss: 2.9829 | Grad Norm: 0.9277 | Step Time: 0.68s\n",
      "[Epoch 1/3] Step 1900/20628 | Avg Loss: 2.9246 | Grad Norm: 1.0903 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 1910/20628 | Avg Loss: 2.9947 | Grad Norm: 0.8330 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1920/20628 | Avg Loss: 3.2636 | Grad Norm: 0.7542 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1930/20628 | Avg Loss: 3.1407 | Grad Norm: 0.9700 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1940/20628 | Avg Loss: 3.2842 | Grad Norm: 1.0473 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1950/20628 | Avg Loss: 2.9280 | Grad Norm: 0.9524 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1960/20628 | Avg Loss: 2.8102 | Grad Norm: 1.1490 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1970/20628 | Avg Loss: 2.4950 | Grad Norm: 0.6451 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 1980/20628 | Avg Loss: 2.7152 | Grad Norm: 1.2722 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 1990/20628 | Avg Loss: 2.8858 | Grad Norm: 0.8349 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2000/20628 | Avg Loss: 2.8662 | Grad Norm: 1.3066 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2010/20628 | Avg Loss: 2.5713 | Grad Norm: 0.8581 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2020/20628 | Avg Loss: 3.2907 | Grad Norm: 0.8577 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2030/20628 | Avg Loss: 3.1622 | Grad Norm: 0.8301 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2040/20628 | Avg Loss: 2.8585 | Grad Norm: 0.9562 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 2050/20628 | Avg Loss: 2.5602 | Grad Norm: 0.8730 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2060/20628 | Avg Loss: 2.9046 | Grad Norm: 0.9349 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2070/20628 | Avg Loss: 2.7295 | Grad Norm: 0.8314 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2080/20628 | Avg Loss: 2.7045 | Grad Norm: 0.9509 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2090/20628 | Avg Loss: 2.6329 | Grad Norm: 0.8857 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2100/20628 | Avg Loss: 2.8455 | Grad Norm: 0.8448 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2110/20628 | Avg Loss: 2.8289 | Grad Norm: 0.8212 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2120/20628 | Avg Loss: 2.8786 | Grad Norm: 0.9499 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2130/20628 | Avg Loss: 3.2025 | Grad Norm: 0.8752 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2140/20628 | Avg Loss: 3.1414 | Grad Norm: 0.8273 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2150/20628 | Avg Loss: 2.8701 | Grad Norm: 1.2544 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2160/20628 | Avg Loss: 3.0195 | Grad Norm: 0.9292 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 2170/20628 | Avg Loss: 2.7792 | Grad Norm: 1.3809 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 2180/20628 | Avg Loss: 3.0025 | Grad Norm: 0.7225 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2190/20628 | Avg Loss: 2.4700 | Grad Norm: 0.5687 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2200/20628 | Avg Loss: 2.9867 | Grad Norm: 0.9606 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2210/20628 | Avg Loss: 2.9892 | Grad Norm: 0.9532 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 2220/20628 | Avg Loss: 2.3987 | Grad Norm: 1.6430 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2230/20628 | Avg Loss: 2.2982 | Grad Norm: 0.6614 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2240/20628 | Avg Loss: 3.0044 | Grad Norm: 0.9299 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2250/20628 | Avg Loss: 3.2089 | Grad Norm: 0.7848 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2260/20628 | Avg Loss: 2.7213 | Grad Norm: 0.6678 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2270/20628 | Avg Loss: 2.8443 | Grad Norm: 0.9366 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2280/20628 | Avg Loss: 2.9061 | Grad Norm: 1.1386 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2290/20628 | Avg Loss: 3.3527 | Grad Norm: 0.9309 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2300/20628 | Avg Loss: 3.0057 | Grad Norm: 0.8143 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2310/20628 | Avg Loss: 3.0370 | Grad Norm: 0.9200 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2320/20628 | Avg Loss: 2.4949 | Grad Norm: 0.7968 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2330/20628 | Avg Loss: 2.8350 | Grad Norm: 0.7300 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2340/20628 | Avg Loss: 2.4151 | Grad Norm: 0.9440 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2350/20628 | Avg Loss: 2.8167 | Grad Norm: 0.8714 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2360/20628 | Avg Loss: 3.0678 | Grad Norm: 0.6701 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2370/20628 | Avg Loss: 2.9546 | Grad Norm: 0.9658 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2380/20628 | Avg Loss: 2.7958 | Grad Norm: 1.0398 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2390/20628 | Avg Loss: 2.6729 | Grad Norm: 0.7430 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 2400/20628 | Avg Loss: 2.2943 | Grad Norm: 1.1096 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2410/20628 | Avg Loss: 2.9284 | Grad Norm: 0.7894 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2420/20628 | Avg Loss: 2.6596 | Grad Norm: 0.8824 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2430/20628 | Avg Loss: 2.7552 | Grad Norm: 1.0425 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2440/20628 | Avg Loss: 2.7308 | Grad Norm: 0.9521 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2450/20628 | Avg Loss: 2.7134 | Grad Norm: 0.9033 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2460/20628 | Avg Loss: 2.5698 | Grad Norm: 0.8843 | Step Time: 0.68s\n",
      "[Epoch 1/3] Step 2470/20628 | Avg Loss: 3.2085 | Grad Norm: 0.9221 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2480/20628 | Avg Loss: 2.8854 | Grad Norm: 2.0551 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2490/20628 | Avg Loss: 2.8214 | Grad Norm: 1.3796 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 2500/20628 | Avg Loss: 2.9810 | Grad Norm: 0.9181 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2510/20628 | Avg Loss: 3.3809 | Grad Norm: 0.7924 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2520/20628 | Avg Loss: 2.9412 | Grad Norm: 0.8279 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2530/20628 | Avg Loss: 2.6000 | Grad Norm: 0.8992 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 2540/20628 | Avg Loss: 3.2533 | Grad Norm: 0.9573 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 2550/20628 | Avg Loss: 2.8386 | Grad Norm: 1.1364 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2560/20628 | Avg Loss: 2.3828 | Grad Norm: 1.0826 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2570/20628 | Avg Loss: 3.0437 | Grad Norm: 1.4340 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2580/20628 | Avg Loss: 3.2381 | Grad Norm: 0.8386 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2590/20628 | Avg Loss: 2.4153 | Grad Norm: 1.1075 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 2600/20628 | Avg Loss: 2.9982 | Grad Norm: 0.8177 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2610/20628 | Avg Loss: 3.1685 | Grad Norm: 0.9802 | Step Time: 0.68s\n",
      "[Epoch 1/3] Step 2620/20628 | Avg Loss: 2.8979 | Grad Norm: 0.9315 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2630/20628 | Avg Loss: 2.7200 | Grad Norm: 0.7782 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2640/20628 | Avg Loss: 3.3174 | Grad Norm: 0.9270 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2650/20628 | Avg Loss: 3.0769 | Grad Norm: 0.9463 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2660/20628 | Avg Loss: 2.7223 | Grad Norm: 1.1502 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 2670/20628 | Avg Loss: 2.6942 | Grad Norm: 0.9401 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2680/20628 | Avg Loss: 2.4983 | Grad Norm: 1.5191 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2690/20628 | Avg Loss: 2.8115 | Grad Norm: 0.7137 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2700/20628 | Avg Loss: 3.1858 | Grad Norm: 0.9609 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 2710/20628 | Avg Loss: 2.8159 | Grad Norm: 0.8934 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2720/20628 | Avg Loss: 3.0569 | Grad Norm: 0.9393 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2730/20628 | Avg Loss: 2.8748 | Grad Norm: 0.8951 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2740/20628 | Avg Loss: 2.9916 | Grad Norm: 0.9250 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2750/20628 | Avg Loss: 3.0984 | Grad Norm: 1.0017 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2760/20628 | Avg Loss: 2.8972 | Grad Norm: 0.6871 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2770/20628 | Avg Loss: 2.5279 | Grad Norm: 0.9609 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2780/20628 | Avg Loss: 2.7163 | Grad Norm: 0.8739 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2790/20628 | Avg Loss: 3.1861 | Grad Norm: 0.8929 | Step Time: 0.68s\n",
      "[Epoch 1/3] Step 2800/20628 | Avg Loss: 2.7241 | Grad Norm: 0.8228 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2810/20628 | Avg Loss: 2.6624 | Grad Norm: 0.8230 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2820/20628 | Avg Loss: 3.4100 | Grad Norm: 0.9324 | Step Time: 0.68s\n",
      "[Epoch 1/3] Step 2830/20628 | Avg Loss: 3.0046 | Grad Norm: 0.9773 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2840/20628 | Avg Loss: 2.4062 | Grad Norm: 1.1977 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 2850/20628 | Avg Loss: 2.9553 | Grad Norm: 0.8731 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 2860/20628 | Avg Loss: 2.6877 | Grad Norm: 0.8092 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2870/20628 | Avg Loss: 2.9285 | Grad Norm: 0.8152 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2880/20628 | Avg Loss: 3.2355 | Grad Norm: 0.9030 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2890/20628 | Avg Loss: 2.8717 | Grad Norm: 1.1631 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2900/20628 | Avg Loss: 2.9005 | Grad Norm: 1.0666 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2910/20628 | Avg Loss: 2.7955 | Grad Norm: 0.9400 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2920/20628 | Avg Loss: 2.6287 | Grad Norm: 0.7878 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2930/20628 | Avg Loss: 2.9033 | Grad Norm: 0.8973 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2940/20628 | Avg Loss: 2.7734 | Grad Norm: 0.8529 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2950/20628 | Avg Loss: 2.8498 | Grad Norm: 0.9285 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2960/20628 | Avg Loss: 2.8800 | Grad Norm: 1.0457 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2970/20628 | Avg Loss: 2.7703 | Grad Norm: 0.8562 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2980/20628 | Avg Loss: 3.0804 | Grad Norm: 0.8817 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 2990/20628 | Avg Loss: 2.7435 | Grad Norm: 0.9521 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3000/20628 | Avg Loss: 2.8439 | Grad Norm: 0.8711 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3010/20628 | Avg Loss: 3.1873 | Grad Norm: 0.8803 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3020/20628 | Avg Loss: 3.2076 | Grad Norm: 0.9110 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3030/20628 | Avg Loss: 2.8713 | Grad Norm: 0.7600 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3040/20628 | Avg Loss: 2.7597 | Grad Norm: 0.8427 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3050/20628 | Avg Loss: 2.6093 | Grad Norm: 0.8481 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3060/20628 | Avg Loss: 2.9907 | Grad Norm: 0.9674 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3070/20628 | Avg Loss: 2.4574 | Grad Norm: 1.1108 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3080/20628 | Avg Loss: 2.6812 | Grad Norm: 1.2376 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3090/20628 | Avg Loss: 2.6505 | Grad Norm: 0.9318 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3100/20628 | Avg Loss: 2.7894 | Grad Norm: 1.0618 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3110/20628 | Avg Loss: 2.3911 | Grad Norm: 0.8058 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3120/20628 | Avg Loss: 2.9054 | Grad Norm: 0.8800 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3130/20628 | Avg Loss: 3.1981 | Grad Norm: 1.0219 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3140/20628 | Avg Loss: 2.8586 | Grad Norm: 0.8345 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3150/20628 | Avg Loss: 2.7762 | Grad Norm: 0.8561 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3160/20628 | Avg Loss: 2.9488 | Grad Norm: 1.6556 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3170/20628 | Avg Loss: 2.5618 | Grad Norm: 0.8606 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3180/20628 | Avg Loss: 2.9017 | Grad Norm: 0.8110 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3190/20628 | Avg Loss: 3.0097 | Grad Norm: 0.8061 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3200/20628 | Avg Loss: 2.9242 | Grad Norm: 0.9858 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3210/20628 | Avg Loss: 2.6507 | Grad Norm: 0.8550 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3220/20628 | Avg Loss: 3.0600 | Grad Norm: 1.0056 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3230/20628 | Avg Loss: 2.6154 | Grad Norm: 0.7817 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3240/20628 | Avg Loss: 2.8547 | Grad Norm: 0.8902 | Step Time: 0.68s\n",
      "[Epoch 1/3] Step 3250/20628 | Avg Loss: 2.7372 | Grad Norm: 1.1480 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3260/20628 | Avg Loss: 3.2515 | Grad Norm: 0.8577 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3270/20628 | Avg Loss: 2.6346 | Grad Norm: 1.0938 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3280/20628 | Avg Loss: 2.5804 | Grad Norm: 0.8579 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 3290/20628 | Avg Loss: 2.4847 | Grad Norm: 0.9767 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3300/20628 | Avg Loss: 3.2376 | Grad Norm: 0.9083 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3310/20628 | Avg Loss: 3.3812 | Grad Norm: 1.0864 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3320/20628 | Avg Loss: 2.7910 | Grad Norm: 0.7780 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3330/20628 | Avg Loss: 2.6887 | Grad Norm: 0.9921 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3340/20628 | Avg Loss: 2.8777 | Grad Norm: 1.1236 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 3350/20628 | Avg Loss: 2.9962 | Grad Norm: 0.8091 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 3360/20628 | Avg Loss: 3.0491 | Grad Norm: 0.9727 | Step Time: 0.68s\n",
      "[Epoch 1/3] Step 3370/20628 | Avg Loss: 2.7545 | Grad Norm: 1.1099 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 3380/20628 | Avg Loss: 3.2996 | Grad Norm: 0.9895 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 3390/20628 | Avg Loss: 2.9316 | Grad Norm: 0.8917 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 3400/20628 | Avg Loss: 3.0485 | Grad Norm: 0.8806 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3410/20628 | Avg Loss: 2.4805 | Grad Norm: 0.8548 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3420/20628 | Avg Loss: 2.7328 | Grad Norm: 0.8188 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 3430/20628 | Avg Loss: 2.8443 | Grad Norm: 1.1577 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3440/20628 | Avg Loss: 2.7303 | Grad Norm: 1.0302 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 3450/20628 | Avg Loss: 2.9357 | Grad Norm: 1.1567 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3460/20628 | Avg Loss: 2.7467 | Grad Norm: 1.0928 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 3470/20628 | Avg Loss: 2.7913 | Grad Norm: 0.8277 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3480/20628 | Avg Loss: 2.6328 | Grad Norm: 1.0225 | Step Time: 0.68s\n",
      "[Epoch 1/3] Step 3490/20628 | Avg Loss: 3.2215 | Grad Norm: 0.8789 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3500/20628 | Avg Loss: 2.6715 | Grad Norm: 0.9694 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3510/20628 | Avg Loss: 2.3296 | Grad Norm: 1.0326 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3520/20628 | Avg Loss: 2.8142 | Grad Norm: 1.0021 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3530/20628 | Avg Loss: 3.0249 | Grad Norm: 0.8965 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3540/20628 | Avg Loss: 3.0227 | Grad Norm: 1.0136 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3550/20628 | Avg Loss: 2.8875 | Grad Norm: 0.9540 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3560/20628 | Avg Loss: 3.0246 | Grad Norm: 1.7209 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3570/20628 | Avg Loss: 2.5070 | Grad Norm: 0.9401 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3580/20628 | Avg Loss: 2.3571 | Grad Norm: 0.6121 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 3590/20628 | Avg Loss: 3.0434 | Grad Norm: 0.7006 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 3600/20628 | Avg Loss: 3.0385 | Grad Norm: 0.9109 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3610/20628 | Avg Loss: 2.7784 | Grad Norm: 0.7960 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3620/20628 | Avg Loss: 2.6444 | Grad Norm: 0.9285 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3630/20628 | Avg Loss: 2.8548 | Grad Norm: 0.7697 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3640/20628 | Avg Loss: 2.3917 | Grad Norm: 0.9718 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3650/20628 | Avg Loss: 3.0126 | Grad Norm: 0.8842 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3660/20628 | Avg Loss: 3.0653 | Grad Norm: 0.9086 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3670/20628 | Avg Loss: 2.9033 | Grad Norm: 0.9380 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3680/20628 | Avg Loss: 2.8090 | Grad Norm: 0.9214 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3690/20628 | Avg Loss: 2.8181 | Grad Norm: 0.8908 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 3700/20628 | Avg Loss: 3.1088 | Grad Norm: 1.2870 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3710/20628 | Avg Loss: 2.9724 | Grad Norm: 0.8935 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3720/20628 | Avg Loss: 3.1414 | Grad Norm: 0.8222 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 3730/20628 | Avg Loss: 2.7210 | Grad Norm: 0.9945 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3740/20628 | Avg Loss: 2.7158 | Grad Norm: 1.1893 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3750/20628 | Avg Loss: 3.1373 | Grad Norm: 0.9144 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3760/20628 | Avg Loss: 3.0538 | Grad Norm: 0.9393 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3770/20628 | Avg Loss: 3.1564 | Grad Norm: 0.9247 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3780/20628 | Avg Loss: 3.0382 | Grad Norm: 1.1821 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 3790/20628 | Avg Loss: 2.5382 | Grad Norm: 0.7384 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3800/20628 | Avg Loss: 2.9957 | Grad Norm: 0.7502 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3810/20628 | Avg Loss: 2.9462 | Grad Norm: 0.8337 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 3820/20628 | Avg Loss: 2.5956 | Grad Norm: 0.8845 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3830/20628 | Avg Loss: 2.8690 | Grad Norm: 0.9651 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3840/20628 | Avg Loss: 3.2420 | Grad Norm: 0.8739 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3850/20628 | Avg Loss: 3.1614 | Grad Norm: 0.9401 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3860/20628 | Avg Loss: 2.9313 | Grad Norm: 0.9857 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3870/20628 | Avg Loss: 2.4285 | Grad Norm: 1.3617 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3880/20628 | Avg Loss: 2.2895 | Grad Norm: 0.8861 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3890/20628 | Avg Loss: 2.6071 | Grad Norm: 0.9736 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3900/20628 | Avg Loss: 3.0029 | Grad Norm: 1.0472 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3910/20628 | Avg Loss: 2.8374 | Grad Norm: 0.8433 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3920/20628 | Avg Loss: 3.1959 | Grad Norm: 0.7646 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3930/20628 | Avg Loss: 3.0825 | Grad Norm: 0.8435 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3940/20628 | Avg Loss: 3.0982 | Grad Norm: 0.9292 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3950/20628 | Avg Loss: 2.9344 | Grad Norm: 0.9155 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3960/20628 | Avg Loss: 2.9172 | Grad Norm: 0.8240 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3970/20628 | Avg Loss: 3.0857 | Grad Norm: 1.0993 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3980/20628 | Avg Loss: 2.9898 | Grad Norm: 1.0223 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 3990/20628 | Avg Loss: 3.1106 | Grad Norm: 1.0410 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4000/20628 | Avg Loss: 3.0150 | Grad Norm: 0.8193 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4010/20628 | Avg Loss: 2.3211 | Grad Norm: 1.2165 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4020/20628 | Avg Loss: 2.6864 | Grad Norm: 1.4873 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4030/20628 | Avg Loss: 2.7570 | Grad Norm: 1.0194 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4040/20628 | Avg Loss: 2.8286 | Grad Norm: 0.8893 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4050/20628 | Avg Loss: 2.4976 | Grad Norm: 0.9944 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4060/20628 | Avg Loss: 3.0524 | Grad Norm: 0.9651 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4070/20628 | Avg Loss: 3.3700 | Grad Norm: 0.9212 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4080/20628 | Avg Loss: 3.0255 | Grad Norm: 0.9005 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4090/20628 | Avg Loss: 2.2667 | Grad Norm: 0.8367 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4100/20628 | Avg Loss: 2.7253 | Grad Norm: 1.0855 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4110/20628 | Avg Loss: 3.1672 | Grad Norm: 1.0371 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 4120/20628 | Avg Loss: 2.7949 | Grad Norm: 0.6806 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4130/20628 | Avg Loss: 2.7480 | Grad Norm: 1.0198 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4140/20628 | Avg Loss: 2.7410 | Grad Norm: 1.0388 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4150/20628 | Avg Loss: 2.5966 | Grad Norm: 0.8559 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 4160/20628 | Avg Loss: 3.1222 | Grad Norm: 1.0605 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4170/20628 | Avg Loss: 3.3305 | Grad Norm: 0.9758 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4180/20628 | Avg Loss: 2.7701 | Grad Norm: 0.8964 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4190/20628 | Avg Loss: 3.1907 | Grad Norm: 1.1184 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4200/20628 | Avg Loss: 2.8201 | Grad Norm: 1.0925 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 4210/20628 | Avg Loss: 2.5752 | Grad Norm: 1.2927 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 4220/20628 | Avg Loss: 2.6167 | Grad Norm: 0.9568 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4230/20628 | Avg Loss: 2.8446 | Grad Norm: 0.8620 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4240/20628 | Avg Loss: 3.1251 | Grad Norm: 0.7682 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 4250/20628 | Avg Loss: 3.4311 | Grad Norm: 1.0338 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 4260/20628 | Avg Loss: 3.1572 | Grad Norm: 1.5925 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4270/20628 | Avg Loss: 3.0578 | Grad Norm: 0.7839 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4280/20628 | Avg Loss: 2.8533 | Grad Norm: 0.8852 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4290/20628 | Avg Loss: 2.7599 | Grad Norm: 1.2603 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4300/20628 | Avg Loss: 3.0300 | Grad Norm: 0.8790 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 4310/20628 | Avg Loss: 3.1255 | Grad Norm: 1.2215 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4320/20628 | Avg Loss: 3.1855 | Grad Norm: 0.8593 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4330/20628 | Avg Loss: 3.0067 | Grad Norm: 0.7096 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 4340/20628 | Avg Loss: 2.8063 | Grad Norm: 1.1349 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4350/20628 | Avg Loss: 2.9320 | Grad Norm: 1.0089 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4360/20628 | Avg Loss: 2.6133 | Grad Norm: 1.0027 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4370/20628 | Avg Loss: 2.9550 | Grad Norm: 0.8346 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4380/20628 | Avg Loss: 2.4768 | Grad Norm: 0.8385 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4390/20628 | Avg Loss: 2.6719 | Grad Norm: 1.3048 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4400/20628 | Avg Loss: 2.3006 | Grad Norm: 0.9302 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4410/20628 | Avg Loss: 3.0804 | Grad Norm: 3.2415 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4420/20628 | Avg Loss: 3.3267 | Grad Norm: 1.2901 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4430/20628 | Avg Loss: 2.7279 | Grad Norm: 0.8609 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4440/20628 | Avg Loss: 2.6327 | Grad Norm: 0.8316 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 4450/20628 | Avg Loss: 2.4818 | Grad Norm: 1.6358 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4460/20628 | Avg Loss: 3.0926 | Grad Norm: 1.0872 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4470/20628 | Avg Loss: 2.9007 | Grad Norm: 0.8730 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 4480/20628 | Avg Loss: 2.5071 | Grad Norm: 0.9104 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 4490/20628 | Avg Loss: 2.9130 | Grad Norm: 0.9238 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4500/20628 | Avg Loss: 2.5519 | Grad Norm: 0.8830 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4510/20628 | Avg Loss: 2.9123 | Grad Norm: 1.1050 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4520/20628 | Avg Loss: 3.3427 | Grad Norm: 1.0945 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 4530/20628 | Avg Loss: 3.0836 | Grad Norm: 1.0064 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 4540/20628 | Avg Loss: 2.8399 | Grad Norm: 0.9974 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4550/20628 | Avg Loss: 2.4641 | Grad Norm: 0.9855 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4560/20628 | Avg Loss: 3.1389 | Grad Norm: 1.1358 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 4570/20628 | Avg Loss: 2.9631 | Grad Norm: 0.8921 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 4580/20628 | Avg Loss: 3.0979 | Grad Norm: 1.0198 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4590/20628 | Avg Loss: 2.5948 | Grad Norm: 0.9127 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4600/20628 | Avg Loss: 2.8177 | Grad Norm: 0.8904 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 4610/20628 | Avg Loss: 2.5794 | Grad Norm: 1.0823 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4620/20628 | Avg Loss: 2.5585 | Grad Norm: 1.2363 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4630/20628 | Avg Loss: 2.8025 | Grad Norm: 0.9796 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 4640/20628 | Avg Loss: 2.9751 | Grad Norm: 0.9715 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4650/20628 | Avg Loss: 2.8653 | Grad Norm: 0.9366 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 4660/20628 | Avg Loss: 2.7370 | Grad Norm: 1.1056 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4670/20628 | Avg Loss: 2.9794 | Grad Norm: 0.8238 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4680/20628 | Avg Loss: 2.5816 | Grad Norm: 1.0402 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 4690/20628 | Avg Loss: 2.7430 | Grad Norm: 1.6839 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 4700/20628 | Avg Loss: 3.0301 | Grad Norm: 0.9754 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4710/20628 | Avg Loss: 2.3772 | Grad Norm: 0.8103 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4720/20628 | Avg Loss: 2.7741 | Grad Norm: 0.9369 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 4730/20628 | Avg Loss: 3.1218 | Grad Norm: 1.1203 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4740/20628 | Avg Loss: 3.2592 | Grad Norm: 1.3310 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 4750/20628 | Avg Loss: 3.0186 | Grad Norm: 1.0195 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 4760/20628 | Avg Loss: 2.6539 | Grad Norm: 0.7451 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4770/20628 | Avg Loss: 2.5375 | Grad Norm: 0.8654 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4780/20628 | Avg Loss: 3.0642 | Grad Norm: 1.1230 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 4790/20628 | Avg Loss: 2.7278 | Grad Norm: 1.2245 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4800/20628 | Avg Loss: 2.8676 | Grad Norm: 0.8090 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 4810/20628 | Avg Loss: 2.9064 | Grad Norm: 0.8676 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4820/20628 | Avg Loss: 2.5384 | Grad Norm: 0.9029 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 4830/20628 | Avg Loss: 2.7421 | Grad Norm: 0.7764 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4840/20628 | Avg Loss: 2.9118 | Grad Norm: 0.8591 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 4850/20628 | Avg Loss: 3.0405 | Grad Norm: 0.8888 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4860/20628 | Avg Loss: 3.3175 | Grad Norm: 1.0392 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4870/20628 | Avg Loss: 2.8510 | Grad Norm: 1.0052 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4880/20628 | Avg Loss: 2.4912 | Grad Norm: 0.8282 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4890/20628 | Avg Loss: 2.3850 | Grad Norm: 1.0865 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4900/20628 | Avg Loss: 3.0154 | Grad Norm: 1.2174 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4910/20628 | Avg Loss: 3.1981 | Grad Norm: 1.2566 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4920/20628 | Avg Loss: 2.9250 | Grad Norm: 1.7399 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4930/20628 | Avg Loss: 2.8764 | Grad Norm: 1.1825 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 4940/20628 | Avg Loss: 2.9059 | Grad Norm: 0.8634 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4950/20628 | Avg Loss: 2.5428 | Grad Norm: 1.5535 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 4960/20628 | Avg Loss: 2.6607 | Grad Norm: 1.0803 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4970/20628 | Avg Loss: 3.0224 | Grad Norm: 1.0971 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4980/20628 | Avg Loss: 3.0167 | Grad Norm: 1.0597 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 4990/20628 | Avg Loss: 3.0655 | Grad Norm: 1.1844 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 5000/20628 | Avg Loss: 3.0716 | Grad Norm: 1.0393 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5010/20628 | Avg Loss: 2.8553 | Grad Norm: 1.0872 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5020/20628 | Avg Loss: 3.1892 | Grad Norm: 1.3298 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 5030/20628 | Avg Loss: 3.0516 | Grad Norm: 0.9645 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5040/20628 | Avg Loss: 2.8816 | Grad Norm: 0.9717 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5050/20628 | Avg Loss: 2.7978 | Grad Norm: 1.4721 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5060/20628 | Avg Loss: 3.1377 | Grad Norm: 1.0371 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5070/20628 | Avg Loss: 2.6769 | Grad Norm: 0.9795 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5080/20628 | Avg Loss: 2.8817 | Grad Norm: 0.8441 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5090/20628 | Avg Loss: 2.9033 | Grad Norm: 1.0421 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5100/20628 | Avg Loss: 2.9147 | Grad Norm: 1.0221 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 5110/20628 | Avg Loss: 2.0789 | Grad Norm: 0.8295 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5120/20628 | Avg Loss: 2.8400 | Grad Norm: 1.1966 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5130/20628 | Avg Loss: 2.7965 | Grad Norm: 0.9741 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 5140/20628 | Avg Loss: 3.0385 | Grad Norm: 1.1917 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 5150/20628 | Avg Loss: 2.8022 | Grad Norm: 0.9139 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5160/20628 | Avg Loss: 2.5459 | Grad Norm: 1.1372 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5170/20628 | Avg Loss: 2.9507 | Grad Norm: 0.9264 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5180/20628 | Avg Loss: 2.5390 | Grad Norm: 1.0122 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5190/20628 | Avg Loss: 3.2377 | Grad Norm: 1.5857 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5200/20628 | Avg Loss: 2.8648 | Grad Norm: 0.8325 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5210/20628 | Avg Loss: 2.7484 | Grad Norm: 1.1225 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5220/20628 | Avg Loss: 3.0253 | Grad Norm: 1.5529 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5230/20628 | Avg Loss: 3.0318 | Grad Norm: 1.0090 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5240/20628 | Avg Loss: 2.6208 | Grad Norm: 1.0828 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5250/20628 | Avg Loss: 2.9049 | Grad Norm: 0.8049 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5260/20628 | Avg Loss: 2.8977 | Grad Norm: 0.9303 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5270/20628 | Avg Loss: 2.1741 | Grad Norm: 1.0004 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5280/20628 | Avg Loss: 2.6658 | Grad Norm: 0.9202 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 5290/20628 | Avg Loss: 3.0025 | Grad Norm: 1.1021 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 5300/20628 | Avg Loss: 2.5642 | Grad Norm: 0.8723 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5310/20628 | Avg Loss: 2.6898 | Grad Norm: 0.9550 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5320/20628 | Avg Loss: 3.0138 | Grad Norm: 1.0541 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5330/20628 | Avg Loss: 3.0670 | Grad Norm: 1.1079 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5340/20628 | Avg Loss: 2.5953 | Grad Norm: 1.2449 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5350/20628 | Avg Loss: 2.7832 | Grad Norm: 1.0403 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5360/20628 | Avg Loss: 2.9362 | Grad Norm: 0.9162 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5370/20628 | Avg Loss: 3.0701 | Grad Norm: 0.9252 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5380/20628 | Avg Loss: 2.9774 | Grad Norm: 0.9951 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5390/20628 | Avg Loss: 2.3830 | Grad Norm: 0.9536 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5400/20628 | Avg Loss: 2.6226 | Grad Norm: 1.1707 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5410/20628 | Avg Loss: 2.9640 | Grad Norm: 1.1873 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5420/20628 | Avg Loss: 2.8597 | Grad Norm: 1.0322 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5430/20628 | Avg Loss: 3.0796 | Grad Norm: 0.7800 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5440/20628 | Avg Loss: 3.0158 | Grad Norm: 0.9202 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5450/20628 | Avg Loss: 3.0484 | Grad Norm: 1.1173 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5460/20628 | Avg Loss: 3.0099 | Grad Norm: 1.3605 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5470/20628 | Avg Loss: 2.7260 | Grad Norm: 1.0021 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5480/20628 | Avg Loss: 3.0497 | Grad Norm: 0.7989 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5490/20628 | Avg Loss: 2.4555 | Grad Norm: 0.9874 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 5500/20628 | Avg Loss: 2.7341 | Grad Norm: 1.2079 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5510/20628 | Avg Loss: 2.6196 | Grad Norm: 0.7510 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5520/20628 | Avg Loss: 2.4994 | Grad Norm: 1.1663 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 5530/20628 | Avg Loss: 3.2859 | Grad Norm: 1.3225 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 5540/20628 | Avg Loss: 2.6410 | Grad Norm: 1.3817 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5550/20628 | Avg Loss: 2.9743 | Grad Norm: 0.9227 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5560/20628 | Avg Loss: 2.4239 | Grad Norm: 1.0031 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5570/20628 | Avg Loss: 2.9305 | Grad Norm: 1.6054 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5580/20628 | Avg Loss: 3.0736 | Grad Norm: 1.1504 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5590/20628 | Avg Loss: 2.9166 | Grad Norm: 1.3427 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5600/20628 | Avg Loss: 3.0849 | Grad Norm: 1.0731 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5610/20628 | Avg Loss: 2.9314 | Grad Norm: 1.4156 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5620/20628 | Avg Loss: 2.6972 | Grad Norm: 1.0696 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5630/20628 | Avg Loss: 2.8110 | Grad Norm: 1.1674 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5640/20628 | Avg Loss: 2.6404 | Grad Norm: 0.9821 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5650/20628 | Avg Loss: 2.8991 | Grad Norm: 1.1485 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 5660/20628 | Avg Loss: 3.0761 | Grad Norm: 0.8659 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5670/20628 | Avg Loss: 2.7795 | Grad Norm: 1.1698 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5680/20628 | Avg Loss: 2.4675 | Grad Norm: 1.5024 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5690/20628 | Avg Loss: 2.8198 | Grad Norm: 0.7748 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5700/20628 | Avg Loss: 3.4413 | Grad Norm: 1.0398 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 5710/20628 | Avg Loss: 2.7306 | Grad Norm: 1.2740 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 5720/20628 | Avg Loss: 3.0786 | Grad Norm: 0.8526 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5730/20628 | Avg Loss: 2.9452 | Grad Norm: 1.2309 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 5740/20628 | Avg Loss: 2.6659 | Grad Norm: 1.2938 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 5750/20628 | Avg Loss: 2.6334 | Grad Norm: 0.8942 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5760/20628 | Avg Loss: 2.8836 | Grad Norm: 0.9465 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 5770/20628 | Avg Loss: 2.6646 | Grad Norm: 1.1073 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5780/20628 | Avg Loss: 2.8899 | Grad Norm: 1.0620 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5790/20628 | Avg Loss: 2.5991 | Grad Norm: 1.0322 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5800/20628 | Avg Loss: 2.8957 | Grad Norm: 1.2247 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5810/20628 | Avg Loss: 2.7217 | Grad Norm: 1.1009 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5820/20628 | Avg Loss: 2.7303 | Grad Norm: 1.0441 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5830/20628 | Avg Loss: 2.9726 | Grad Norm: 0.8158 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5840/20628 | Avg Loss: 2.8997 | Grad Norm: 1.1924 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 5850/20628 | Avg Loss: 2.8673 | Grad Norm: 1.0413 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5860/20628 | Avg Loss: 2.8295 | Grad Norm: 0.9942 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5870/20628 | Avg Loss: 2.9397 | Grad Norm: 0.8222 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5880/20628 | Avg Loss: 2.9256 | Grad Norm: 0.9953 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5890/20628 | Avg Loss: 2.6429 | Grad Norm: 0.7592 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 5900/20628 | Avg Loss: 2.7150 | Grad Norm: 1.0294 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 5910/20628 | Avg Loss: 3.3073 | Grad Norm: 1.0436 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5920/20628 | Avg Loss: 2.6296 | Grad Norm: 1.6939 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5930/20628 | Avg Loss: 2.8142 | Grad Norm: 0.9673 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5940/20628 | Avg Loss: 3.3548 | Grad Norm: 0.9070 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5950/20628 | Avg Loss: 2.4806 | Grad Norm: 0.9890 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5960/20628 | Avg Loss: 2.8105 | Grad Norm: 1.2087 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5970/20628 | Avg Loss: 2.7243 | Grad Norm: 1.0580 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5980/20628 | Avg Loss: 3.2460 | Grad Norm: 0.9125 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 5990/20628 | Avg Loss: 2.7831 | Grad Norm: 1.0150 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6000/20628 | Avg Loss: 2.5260 | Grad Norm: 1.0106 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6010/20628 | Avg Loss: 2.6905 | Grad Norm: 0.9297 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6020/20628 | Avg Loss: 3.1581 | Grad Norm: 1.2098 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6030/20628 | Avg Loss: 2.9963 | Grad Norm: 0.9199 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6040/20628 | Avg Loss: 2.6432 | Grad Norm: 0.8163 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6050/20628 | Avg Loss: 3.3706 | Grad Norm: 1.1664 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 6060/20628 | Avg Loss: 2.6711 | Grad Norm: 1.0246 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6070/20628 | Avg Loss: 3.2811 | Grad Norm: 0.9692 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6080/20628 | Avg Loss: 2.6680 | Grad Norm: 0.9583 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6090/20628 | Avg Loss: 2.9948 | Grad Norm: 1.4779 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6100/20628 | Avg Loss: 2.8022 | Grad Norm: 1.3284 | Step Time: 0.68s\n",
      "[Epoch 1/3] Step 6110/20628 | Avg Loss: 3.0180 | Grad Norm: 1.1783 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6120/20628 | Avg Loss: 2.8764 | Grad Norm: 1.3271 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6130/20628 | Avg Loss: 3.5607 | Grad Norm: 1.4601 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6140/20628 | Avg Loss: 2.8163 | Grad Norm: 1.1219 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6150/20628 | Avg Loss: 2.9118 | Grad Norm: 0.9878 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 6160/20628 | Avg Loss: 2.4821 | Grad Norm: 0.8314 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6170/20628 | Avg Loss: 3.0081 | Grad Norm: 0.8019 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6180/20628 | Avg Loss: 3.0250 | Grad Norm: 0.9183 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6190/20628 | Avg Loss: 2.4959 | Grad Norm: 1.1098 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6200/20628 | Avg Loss: 3.2580 | Grad Norm: 1.0206 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6210/20628 | Avg Loss: 3.0940 | Grad Norm: 1.0610 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6220/20628 | Avg Loss: 3.0629 | Grad Norm: 0.9459 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 6230/20628 | Avg Loss: 3.1249 | Grad Norm: 1.1381 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6240/20628 | Avg Loss: 3.2226 | Grad Norm: 1.2447 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 6250/20628 | Avg Loss: 2.8283 | Grad Norm: 0.9935 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6260/20628 | Avg Loss: 3.1692 | Grad Norm: 1.0624 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6270/20628 | Avg Loss: 2.6348 | Grad Norm: 0.9385 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 6280/20628 | Avg Loss: 3.0652 | Grad Norm: 0.9950 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 6290/20628 | Avg Loss: 2.9586 | Grad Norm: 1.1577 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6300/20628 | Avg Loss: 2.9688 | Grad Norm: 1.0325 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 6310/20628 | Avg Loss: 2.8599 | Grad Norm: 0.8963 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 6320/20628 | Avg Loss: 3.2476 | Grad Norm: 1.0777 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6330/20628 | Avg Loss: 2.8529 | Grad Norm: 1.1869 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6340/20628 | Avg Loss: 2.6527 | Grad Norm: 1.0808 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6350/20628 | Avg Loss: 3.1785 | Grad Norm: 1.1865 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6360/20628 | Avg Loss: 3.0705 | Grad Norm: 0.9904 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6370/20628 | Avg Loss: 2.8341 | Grad Norm: 0.9828 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6380/20628 | Avg Loss: 3.0627 | Grad Norm: 1.0250 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6390/20628 | Avg Loss: 2.4963 | Grad Norm: 1.0723 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6400/20628 | Avg Loss: 3.0064 | Grad Norm: 1.0584 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6410/20628 | Avg Loss: 2.9817 | Grad Norm: 1.0458 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6420/20628 | Avg Loss: 2.9978 | Grad Norm: 1.1577 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6430/20628 | Avg Loss: 2.4985 | Grad Norm: 1.0504 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6440/20628 | Avg Loss: 2.6451 | Grad Norm: 0.9947 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 6450/20628 | Avg Loss: 3.4869 | Grad Norm: 1.0655 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6460/20628 | Avg Loss: 3.1363 | Grad Norm: 1.1359 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6470/20628 | Avg Loss: 2.7154 | Grad Norm: 1.0131 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6480/20628 | Avg Loss: 2.8014 | Grad Norm: 1.0324 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6490/20628 | Avg Loss: 2.6109 | Grad Norm: 1.1290 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 6500/20628 | Avg Loss: 2.5261 | Grad Norm: 1.0060 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6510/20628 | Avg Loss: 2.8439 | Grad Norm: 0.9468 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6520/20628 | Avg Loss: 3.0818 | Grad Norm: 1.2929 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6530/20628 | Avg Loss: 2.8512 | Grad Norm: 0.9073 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6540/20628 | Avg Loss: 3.0085 | Grad Norm: 0.9386 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 6550/20628 | Avg Loss: 2.8417 | Grad Norm: 1.1479 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6560/20628 | Avg Loss: 2.5887 | Grad Norm: 0.9279 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6570/20628 | Avg Loss: 2.8247 | Grad Norm: 0.8360 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6580/20628 | Avg Loss: 3.1170 | Grad Norm: 1.2340 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6590/20628 | Avg Loss: 2.5981 | Grad Norm: 1.0155 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6600/20628 | Avg Loss: 2.8730 | Grad Norm: 0.8316 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6610/20628 | Avg Loss: 3.0066 | Grad Norm: 1.1282 | Step Time: 0.68s\n",
      "[Epoch 1/3] Step 6620/20628 | Avg Loss: 3.0139 | Grad Norm: 0.9097 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6630/20628 | Avg Loss: 3.1510 | Grad Norm: 0.9721 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6640/20628 | Avg Loss: 2.9382 | Grad Norm: 1.2383 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6650/20628 | Avg Loss: 3.1059 | Grad Norm: 0.9297 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6660/20628 | Avg Loss: 2.9663 | Grad Norm: 1.1067 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6670/20628 | Avg Loss: 2.6184 | Grad Norm: 0.9188 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6680/20628 | Avg Loss: 3.0419 | Grad Norm: 1.0908 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6690/20628 | Avg Loss: 2.8962 | Grad Norm: 1.0725 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6700/20628 | Avg Loss: 2.9485 | Grad Norm: 1.1678 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6710/20628 | Avg Loss: 2.7168 | Grad Norm: 0.9868 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6720/20628 | Avg Loss: 3.1269 | Grad Norm: 1.0503 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6730/20628 | Avg Loss: 2.8185 | Grad Norm: 0.9445 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6740/20628 | Avg Loss: 2.9897 | Grad Norm: 1.2856 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6750/20628 | Avg Loss: 2.8352 | Grad Norm: 1.1310 | Step Time: 0.65s\n",
      "[Epoch 1/3] Step 6760/20628 | Avg Loss: 2.6151 | Grad Norm: 1.1799 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6770/20628 | Avg Loss: 3.3373 | Grad Norm: 2.1586 | Step Time: 0.67s\n",
      "[Epoch 1/3] Step 6780/20628 | Avg Loss: 3.1841 | Grad Norm: 0.8658 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6790/20628 | Avg Loss: 2.5684 | Grad Norm: 1.1499 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6800/20628 | Avg Loss: 3.0141 | Grad Norm: 1.0011 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6810/20628 | Avg Loss: 2.8776 | Grad Norm: 1.3598 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6820/20628 | Avg Loss: 2.3764 | Grad Norm: 0.7539 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6830/20628 | Avg Loss: 2.3700 | Grad Norm: 0.9852 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6840/20628 | Avg Loss: 2.7843 | Grad Norm: 1.0791 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6850/20628 | Avg Loss: 2.5396 | Grad Norm: 1.0573 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6860/20628 | Avg Loss: 2.4491 | Grad Norm: 0.9355 | Step Time: 0.66s\n",
      "[Epoch 1/3] Step 6870/20628 | Avg Loss: 3.1267 | Grad Norm: 0.8640 | Step Time: 0.65s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Epoch 1 å®Œæˆ | Avg Loss: 2.9608 | Epoch Time: 01:16:50 | Total Time: 01:16:50\n",
      "\n",
      "ğŸ“ ç”ŸæˆéªŒè¯ï¼ˆéšæœºæ ·æœ¬ï¼‰:\n",
      "ä¸»é¢˜: Write a poem using the theme: Living,Death,Sorrow ...\n",
      "ç”Ÿæˆè¯—æ­Œ:\n",
      "In this house on its hilltop,\n",
      "The windows open to the sea;\n",
      "The curtains have been hung.\n",
      "On the sill is an odd old lamp\n",
      "With a long stem and black brass handleâ€”\n",
      "It was from some strangerâ€™s room.\n",
      "Outside in the cold darkness,\n",
      "A woman weeps.\n",
      "She has come from another world.\n",
      "No one knows where she comes from.\n",
      "She may be from the country\n",
      "Beyond the river that flows through the hills\n",
      "Into the sea.\n",
      "But she says the rain falls there\n",
      "Through the trees all day\n",
      "And her heart is like a flower\n",
      "That opens only at night.\n",
      "She cries for no one.\n",
      "What are your tears about? \n",
      "I will not answer you.\n",
      "You must go back home now\n",
      "To wherever it is you live.\n",
      "There is someone who waits for you:\n",
      "Her face is pale with sorrow.\n",
      "Who is this? I do not know.\n",
      "--------------------------------------------------\n",
      "[Epoch 2/3] Step 6880/20628 | Avg Loss: 2.9640 | Grad Norm: 0.8628 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 6890/20628 | Avg Loss: 2.9384 | Grad Norm: 0.9987 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 6900/20628 | Avg Loss: 2.4941 | Grad Norm: 1.2591 | Step Time: 0.85s\n",
      "[Epoch 2/3] Step 6910/20628 | Avg Loss: 2.4898 | Grad Norm: 0.7567 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 6920/20628 | Avg Loss: 3.2820 | Grad Norm: 1.1469 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 6930/20628 | Avg Loss: 2.7144 | Grad Norm: 1.2410 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 6940/20628 | Avg Loss: 3.0580 | Grad Norm: 0.8294 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 6950/20628 | Avg Loss: 2.8991 | Grad Norm: 0.9698 | Step Time: 0.68s\n",
      "[Epoch 2/3] Step 6960/20628 | Avg Loss: 3.2266 | Grad Norm: 0.9718 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 6970/20628 | Avg Loss: 3.0375 | Grad Norm: 1.0713 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 6980/20628 | Avg Loss: 2.6519 | Grad Norm: 1.0390 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 6990/20628 | Avg Loss: 2.8843 | Grad Norm: 0.9972 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7000/20628 | Avg Loss: 2.9563 | Grad Norm: 1.1187 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7010/20628 | Avg Loss: 2.9734 | Grad Norm: 1.1698 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7020/20628 | Avg Loss: 2.8176 | Grad Norm: 1.2591 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7030/20628 | Avg Loss: 2.9868 | Grad Norm: 1.1090 | Step Time: 0.86s\n",
      "[Epoch 2/3] Step 7040/20628 | Avg Loss: 3.1014 | Grad Norm: 0.9848 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7050/20628 | Avg Loss: 2.8300 | Grad Norm: 1.2033 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7060/20628 | Avg Loss: 2.6548 | Grad Norm: 1.1217 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7070/20628 | Avg Loss: 2.8623 | Grad Norm: 0.9483 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7080/20628 | Avg Loss: 2.5786 | Grad Norm: 1.1791 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7090/20628 | Avg Loss: 2.9769 | Grad Norm: 1.2471 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7100/20628 | Avg Loss: 2.4617 | Grad Norm: 0.9689 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7110/20628 | Avg Loss: 2.9139 | Grad Norm: 1.0532 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7120/20628 | Avg Loss: 2.9976 | Grad Norm: 1.2734 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7130/20628 | Avg Loss: 2.8590 | Grad Norm: 1.0307 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7140/20628 | Avg Loss: 2.9990 | Grad Norm: 1.0957 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7150/20628 | Avg Loss: 2.8724 | Grad Norm: 1.2519 | Step Time: 0.68s\n",
      "[Epoch 2/3] Step 7160/20628 | Avg Loss: 3.0088 | Grad Norm: 0.9443 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7170/20628 | Avg Loss: 2.2568 | Grad Norm: 0.7995 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7180/20628 | Avg Loss: 2.7935 | Grad Norm: 1.2572 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7190/20628 | Avg Loss: 3.1047 | Grad Norm: 1.1868 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 7200/20628 | Avg Loss: 2.5873 | Grad Norm: 0.9336 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7210/20628 | Avg Loss: 3.0080 | Grad Norm: 1.3851 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7220/20628 | Avg Loss: 2.6223 | Grad Norm: 1.5561 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7230/20628 | Avg Loss: 3.0853 | Grad Norm: 1.1192 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 7240/20628 | Avg Loss: 2.9799 | Grad Norm: 1.1296 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7250/20628 | Avg Loss: 2.7040 | Grad Norm: 1.1120 | Step Time: 0.68s\n",
      "[Epoch 2/3] Step 7260/20628 | Avg Loss: 3.1368 | Grad Norm: 0.9795 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7270/20628 | Avg Loss: 2.8913 | Grad Norm: 1.3253 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 7280/20628 | Avg Loss: 3.1650 | Grad Norm: 1.4213 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7290/20628 | Avg Loss: 2.9850 | Grad Norm: 1.3218 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7300/20628 | Avg Loss: 2.6145 | Grad Norm: 1.4116 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7310/20628 | Avg Loss: 2.4186 | Grad Norm: 0.9881 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7320/20628 | Avg Loss: 3.0510 | Grad Norm: 1.0498 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7330/20628 | Avg Loss: 3.2635 | Grad Norm: 1.0171 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7340/20628 | Avg Loss: 3.1502 | Grad Norm: 1.0681 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7350/20628 | Avg Loss: 2.6794 | Grad Norm: 1.2330 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7360/20628 | Avg Loss: 3.2383 | Grad Norm: 1.1029 | Step Time: 0.68s\n",
      "[Epoch 2/3] Step 7370/20628 | Avg Loss: 2.5662 | Grad Norm: 0.9677 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7380/20628 | Avg Loss: 2.8805 | Grad Norm: 0.9583 | Step Time: 0.68s\n",
      "[Epoch 2/3] Step 7390/20628 | Avg Loss: 3.1782 | Grad Norm: 1.2819 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7400/20628 | Avg Loss: 2.6437 | Grad Norm: 1.6092 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 7410/20628 | Avg Loss: 3.0037 | Grad Norm: 0.9885 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 7420/20628 | Avg Loss: 2.4031 | Grad Norm: 0.9185 | Step Time: 0.68s\n",
      "[Epoch 2/3] Step 7430/20628 | Avg Loss: 2.8302 | Grad Norm: 1.0616 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 7440/20628 | Avg Loss: 2.9371 | Grad Norm: 1.1310 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7450/20628 | Avg Loss: 2.8644 | Grad Norm: 1.1312 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 7460/20628 | Avg Loss: 3.0812 | Grad Norm: 1.2500 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 7470/20628 | Avg Loss: 2.5932 | Grad Norm: 1.0957 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7480/20628 | Avg Loss: 2.2492 | Grad Norm: 1.3484 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 7490/20628 | Avg Loss: 3.0276 | Grad Norm: 1.5331 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7500/20628 | Avg Loss: 2.9994 | Grad Norm: 0.8717 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7510/20628 | Avg Loss: 2.8657 | Grad Norm: 0.9282 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 7520/20628 | Avg Loss: 2.7465 | Grad Norm: 1.2569 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7530/20628 | Avg Loss: 3.2255 | Grad Norm: 1.1993 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7540/20628 | Avg Loss: 3.1770 | Grad Norm: 0.9284 | Step Time: 0.69s\n",
      "[Epoch 2/3] Step 7550/20628 | Avg Loss: 2.2650 | Grad Norm: 0.7907 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7560/20628 | Avg Loss: 2.6877 | Grad Norm: 1.2542 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7570/20628 | Avg Loss: 2.9218 | Grad Norm: 0.8801 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7580/20628 | Avg Loss: 3.1176 | Grad Norm: 1.1509 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7590/20628 | Avg Loss: 2.5622 | Grad Norm: 0.9007 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7600/20628 | Avg Loss: 2.9251 | Grad Norm: 0.8167 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7610/20628 | Avg Loss: 2.8349 | Grad Norm: 1.0637 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 7620/20628 | Avg Loss: 3.3736 | Grad Norm: 1.1824 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7630/20628 | Avg Loss: 3.1251 | Grad Norm: 1.0477 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7640/20628 | Avg Loss: 3.2858 | Grad Norm: 0.9914 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 7650/20628 | Avg Loss: 2.6327 | Grad Norm: 1.0080 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 7660/20628 | Avg Loss: 2.9800 | Grad Norm: 0.9468 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 7670/20628 | Avg Loss: 2.9730 | Grad Norm: 1.2975 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7680/20628 | Avg Loss: 2.8571 | Grad Norm: 1.0505 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7690/20628 | Avg Loss: 2.7614 | Grad Norm: 1.0133 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7700/20628 | Avg Loss: 3.0657 | Grad Norm: 1.0381 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7710/20628 | Avg Loss: 2.4821 | Grad Norm: 1.0706 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7720/20628 | Avg Loss: 2.9018 | Grad Norm: 1.0230 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7730/20628 | Avg Loss: 2.8454 | Grad Norm: 1.1595 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7740/20628 | Avg Loss: 2.9660 | Grad Norm: 1.1535 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 7750/20628 | Avg Loss: 2.7110 | Grad Norm: 1.0026 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7760/20628 | Avg Loss: 2.6926 | Grad Norm: 1.1010 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7770/20628 | Avg Loss: 2.8047 | Grad Norm: 1.4257 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7780/20628 | Avg Loss: 2.6704 | Grad Norm: 1.2427 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7790/20628 | Avg Loss: 2.2216 | Grad Norm: 0.7136 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 7800/20628 | Avg Loss: 2.3958 | Grad Norm: 1.0265 | Step Time: 0.65s\n",
      "[Epoch 2/3] Step 7810/20628 | Avg Loss: 2.5459 | Grad Norm: 0.9323 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7820/20628 | Avg Loss: 2.7885 | Grad Norm: 1.6366 | Step Time: 0.68s\n",
      "[Epoch 2/3] Step 7830/20628 | Avg Loss: 2.8842 | Grad Norm: 1.7113 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7840/20628 | Avg Loss: 3.0105 | Grad Norm: 1.0582 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7850/20628 | Avg Loss: 3.2359 | Grad Norm: 1.1283 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7860/20628 | Avg Loss: 3.3084 | Grad Norm: 1.2132 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7870/20628 | Avg Loss: 2.6532 | Grad Norm: 1.1439 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7880/20628 | Avg Loss: 2.8487 | Grad Norm: 1.1463 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7890/20628 | Avg Loss: 2.6454 | Grad Norm: 1.0675 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7900/20628 | Avg Loss: 2.9799 | Grad Norm: 1.0251 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7910/20628 | Avg Loss: 3.1908 | Grad Norm: 0.9062 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7920/20628 | Avg Loss: 2.7274 | Grad Norm: 1.0141 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7930/20628 | Avg Loss: 2.3618 | Grad Norm: 0.9707 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7940/20628 | Avg Loss: 2.9898 | Grad Norm: 0.9568 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7950/20628 | Avg Loss: 2.9952 | Grad Norm: 1.1008 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 7960/20628 | Avg Loss: 3.3846 | Grad Norm: 1.2973 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7970/20628 | Avg Loss: 2.8900 | Grad Norm: 1.4581 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 7980/20628 | Avg Loss: 2.9803 | Grad Norm: 0.8672 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 7990/20628 | Avg Loss: 2.7536 | Grad Norm: 1.1533 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8000/20628 | Avg Loss: 2.7679 | Grad Norm: 1.2522 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8010/20628 | Avg Loss: 2.7756 | Grad Norm: 1.2250 | Step Time: 0.68s\n",
      "[Epoch 2/3] Step 8020/20628 | Avg Loss: 3.0144 | Grad Norm: 1.1614 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8030/20628 | Avg Loss: 3.0163 | Grad Norm: 0.9356 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8040/20628 | Avg Loss: 2.7678 | Grad Norm: 1.3624 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8050/20628 | Avg Loss: 3.0083 | Grad Norm: 1.1309 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8060/20628 | Avg Loss: 2.9611 | Grad Norm: 1.0735 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8070/20628 | Avg Loss: 2.5024 | Grad Norm: 1.3786 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8080/20628 | Avg Loss: 3.1296 | Grad Norm: 1.0463 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8090/20628 | Avg Loss: 3.1377 | Grad Norm: 1.0356 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8100/20628 | Avg Loss: 2.5038 | Grad Norm: 1.3764 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8110/20628 | Avg Loss: 3.1701 | Grad Norm: 0.9043 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8120/20628 | Avg Loss: 3.0379 | Grad Norm: 1.2560 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8130/20628 | Avg Loss: 2.7188 | Grad Norm: 0.9874 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8140/20628 | Avg Loss: 2.5335 | Grad Norm: 1.0814 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8150/20628 | Avg Loss: 2.8449 | Grad Norm: 0.9600 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8160/20628 | Avg Loss: 2.7311 | Grad Norm: 1.1131 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 8170/20628 | Avg Loss: 2.5790 | Grad Norm: 1.0162 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8180/20628 | Avg Loss: 2.6096 | Grad Norm: 1.0600 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8190/20628 | Avg Loss: 2.8568 | Grad Norm: 1.1324 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8200/20628 | Avg Loss: 2.8940 | Grad Norm: 1.1798 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8210/20628 | Avg Loss: 3.0656 | Grad Norm: 1.0819 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8220/20628 | Avg Loss: 2.3498 | Grad Norm: 1.0363 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8230/20628 | Avg Loss: 2.7846 | Grad Norm: 1.0821 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8240/20628 | Avg Loss: 2.9606 | Grad Norm: 1.1218 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8250/20628 | Avg Loss: 2.7559 | Grad Norm: 1.0635 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8260/20628 | Avg Loss: 2.6376 | Grad Norm: 0.9427 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8270/20628 | Avg Loss: 2.5214 | Grad Norm: 0.9756 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8280/20628 | Avg Loss: 2.1812 | Grad Norm: 1.0475 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8290/20628 | Avg Loss: 3.3203 | Grad Norm: 1.0566 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8300/20628 | Avg Loss: 2.7800 | Grad Norm: 1.1280 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8310/20628 | Avg Loss: 2.7427 | Grad Norm: 1.5075 | Step Time: 0.68s\n",
      "[Epoch 2/3] Step 8320/20628 | Avg Loss: 3.1579 | Grad Norm: 1.2278 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8330/20628 | Avg Loss: 3.1100 | Grad Norm: 1.1850 | Step Time: 0.65s\n",
      "[Epoch 2/3] Step 8340/20628 | Avg Loss: 3.3339 | Grad Norm: 1.0094 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 8350/20628 | Avg Loss: 2.8550 | Grad Norm: 1.1520 | Step Time: 0.68s\n",
      "[Epoch 2/3] Step 8360/20628 | Avg Loss: 3.0665 | Grad Norm: 1.2944 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8370/20628 | Avg Loss: 3.3360 | Grad Norm: 1.1214 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8380/20628 | Avg Loss: 2.9294 | Grad Norm: 1.0261 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8390/20628 | Avg Loss: 2.9169 | Grad Norm: 1.1679 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 8400/20628 | Avg Loss: 2.9153 | Grad Norm: 1.3105 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8410/20628 | Avg Loss: 2.8525 | Grad Norm: 1.2152 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8420/20628 | Avg Loss: 2.6449 | Grad Norm: 1.4467 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8430/20628 | Avg Loss: 2.7503 | Grad Norm: 1.3743 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8440/20628 | Avg Loss: 2.7953 | Grad Norm: 1.1143 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8450/20628 | Avg Loss: 2.8667 | Grad Norm: 1.2437 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8460/20628 | Avg Loss: 2.6705 | Grad Norm: 0.8909 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8470/20628 | Avg Loss: 2.7832 | Grad Norm: 1.1627 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8480/20628 | Avg Loss: 3.2318 | Grad Norm: 1.0441 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8490/20628 | Avg Loss: 2.3313 | Grad Norm: 1.0972 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8500/20628 | Avg Loss: 2.6269 | Grad Norm: 1.2979 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8510/20628 | Avg Loss: 2.5562 | Grad Norm: 1.1028 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8520/20628 | Avg Loss: 2.8343 | Grad Norm: 1.0585 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8530/20628 | Avg Loss: 2.7886 | Grad Norm: 1.3270 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8540/20628 | Avg Loss: 2.6163 | Grad Norm: 1.0452 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8550/20628 | Avg Loss: 2.8549 | Grad Norm: 1.2417 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8560/20628 | Avg Loss: 2.8178 | Grad Norm: 1.1978 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8570/20628 | Avg Loss: 2.9674 | Grad Norm: 1.7055 | Step Time: 0.65s\n",
      "[Epoch 2/3] Step 8580/20628 | Avg Loss: 2.4287 | Grad Norm: 0.7208 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8590/20628 | Avg Loss: 2.6298 | Grad Norm: 1.2341 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 8600/20628 | Avg Loss: 2.6611 | Grad Norm: 0.9843 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8610/20628 | Avg Loss: 3.0289 | Grad Norm: 1.2530 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8620/20628 | Avg Loss: 2.9427 | Grad Norm: 0.9592 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8630/20628 | Avg Loss: 2.8427 | Grad Norm: 1.1394 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8640/20628 | Avg Loss: 2.7650 | Grad Norm: 1.1170 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8650/20628 | Avg Loss: 2.5667 | Grad Norm: 1.1599 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8660/20628 | Avg Loss: 3.1931 | Grad Norm: 0.9622 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8670/20628 | Avg Loss: 2.8879 | Grad Norm: 0.9285 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 8680/20628 | Avg Loss: 2.5308 | Grad Norm: 1.1173 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8690/20628 | Avg Loss: 2.9406 | Grad Norm: 1.0836 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8700/20628 | Avg Loss: 2.8960 | Grad Norm: 1.3491 | Step Time: 0.65s\n",
      "[Epoch 2/3] Step 8710/20628 | Avg Loss: 2.8155 | Grad Norm: 1.2139 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 8720/20628 | Avg Loss: 2.8554 | Grad Norm: 1.0101 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8730/20628 | Avg Loss: 3.1077 | Grad Norm: 1.1161 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8740/20628 | Avg Loss: 3.1465 | Grad Norm: 1.1667 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8750/20628 | Avg Loss: 2.7176 | Grad Norm: 0.9490 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8760/20628 | Avg Loss: 2.4731 | Grad Norm: 1.0194 | Step Time: 0.65s\n",
      "[Epoch 2/3] Step 8770/20628 | Avg Loss: 3.0269 | Grad Norm: 1.0425 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8780/20628 | Avg Loss: 3.2799 | Grad Norm: 0.8040 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8790/20628 | Avg Loss: 2.6872 | Grad Norm: 0.9720 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 8800/20628 | Avg Loss: 2.8033 | Grad Norm: 1.0740 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8810/20628 | Avg Loss: 3.2260 | Grad Norm: 1.0742 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8820/20628 | Avg Loss: 2.7449 | Grad Norm: 1.3746 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8830/20628 | Avg Loss: 3.0790 | Grad Norm: 1.2144 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8840/20628 | Avg Loss: 3.0315 | Grad Norm: 0.9212 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8850/20628 | Avg Loss: 2.6258 | Grad Norm: 1.1119 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8860/20628 | Avg Loss: 2.8454 | Grad Norm: 1.0562 | Step Time: 0.65s\n",
      "[Epoch 2/3] Step 8870/20628 | Avg Loss: 2.9477 | Grad Norm: 1.0809 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8880/20628 | Avg Loss: 2.8069 | Grad Norm: 1.0137 | Step Time: 0.65s\n",
      "[Epoch 2/3] Step 8890/20628 | Avg Loss: 2.9339 | Grad Norm: 1.0484 | Step Time: 0.65s\n",
      "[Epoch 2/3] Step 8900/20628 | Avg Loss: 2.8829 | Grad Norm: 1.0956 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8910/20628 | Avg Loss: 2.8367 | Grad Norm: 1.2803 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 8920/20628 | Avg Loss: 2.6746 | Grad Norm: 1.0593 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8930/20628 | Avg Loss: 2.4993 | Grad Norm: 0.9233 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8940/20628 | Avg Loss: 2.8461 | Grad Norm: 1.1542 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8950/20628 | Avg Loss: 2.6294 | Grad Norm: 1.7087 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8960/20628 | Avg Loss: 2.9168 | Grad Norm: 0.9590 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 8970/20628 | Avg Loss: 2.5531 | Grad Norm: 1.3993 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8980/20628 | Avg Loss: 3.1430 | Grad Norm: 1.3251 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 8990/20628 | Avg Loss: 2.7157 | Grad Norm: 1.0051 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9000/20628 | Avg Loss: 2.8422 | Grad Norm: 1.1440 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9010/20628 | Avg Loss: 2.4697 | Grad Norm: 1.4390 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9020/20628 | Avg Loss: 2.9502 | Grad Norm: 1.4201 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9030/20628 | Avg Loss: 2.9892 | Grad Norm: 1.2603 | Step Time: 0.68s\n",
      "[Epoch 2/3] Step 9040/20628 | Avg Loss: 2.6019 | Grad Norm: 1.0255 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9050/20628 | Avg Loss: 2.4914 | Grad Norm: 1.0326 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9060/20628 | Avg Loss: 2.5783 | Grad Norm: 1.1371 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9070/20628 | Avg Loss: 3.5273 | Grad Norm: 1.1550 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9080/20628 | Avg Loss: 3.0085 | Grad Norm: 1.2917 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9090/20628 | Avg Loss: 3.2681 | Grad Norm: 1.2174 | Step Time: 0.68s\n",
      "[Epoch 2/3] Step 9100/20628 | Avg Loss: 3.0477 | Grad Norm: 1.1594 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9110/20628 | Avg Loss: 3.4707 | Grad Norm: 1.1352 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9120/20628 | Avg Loss: 2.9113 | Grad Norm: 1.0980 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9130/20628 | Avg Loss: 2.6856 | Grad Norm: 1.2038 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 9140/20628 | Avg Loss: 3.2738 | Grad Norm: 1.2057 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9150/20628 | Avg Loss: 2.8861 | Grad Norm: 1.0481 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9160/20628 | Avg Loss: 2.7729 | Grad Norm: 0.8787 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9170/20628 | Avg Loss: 2.9113 | Grad Norm: 1.2372 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9180/20628 | Avg Loss: 2.7974 | Grad Norm: 1.3171 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9190/20628 | Avg Loss: 2.5930 | Grad Norm: 1.0109 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9200/20628 | Avg Loss: 3.3472 | Grad Norm: 1.1261 | Step Time: 0.65s\n",
      "[Epoch 2/3] Step 9210/20628 | Avg Loss: 2.9442 | Grad Norm: 1.1460 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9220/20628 | Avg Loss: 2.6768 | Grad Norm: 0.8876 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9230/20628 | Avg Loss: 3.1342 | Grad Norm: 1.1574 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9240/20628 | Avg Loss: 2.9214 | Grad Norm: 1.0303 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9250/20628 | Avg Loss: 2.8199 | Grad Norm: 0.9021 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9260/20628 | Avg Loss: 2.8811 | Grad Norm: 0.9602 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9270/20628 | Avg Loss: 2.6748 | Grad Norm: 1.1204 | Step Time: 0.68s\n",
      "[Epoch 2/3] Step 9280/20628 | Avg Loss: 3.0866 | Grad Norm: 1.0897 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9290/20628 | Avg Loss: 2.8006 | Grad Norm: 1.0330 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 9300/20628 | Avg Loss: 2.6261 | Grad Norm: 0.9978 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9310/20628 | Avg Loss: 2.8854 | Grad Norm: 1.1143 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9320/20628 | Avg Loss: 2.9750 | Grad Norm: 0.9710 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9330/20628 | Avg Loss: 2.4209 | Grad Norm: 1.1977 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9340/20628 | Avg Loss: 2.4339 | Grad Norm: 1.4853 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9350/20628 | Avg Loss: 2.9060 | Grad Norm: 1.2342 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 9360/20628 | Avg Loss: 2.8409 | Grad Norm: 0.7370 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 9370/20628 | Avg Loss: 3.2933 | Grad Norm: 1.1756 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9380/20628 | Avg Loss: 2.8605 | Grad Norm: 1.1209 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 9390/20628 | Avg Loss: 2.5529 | Grad Norm: 1.2292 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9400/20628 | Avg Loss: 3.2036 | Grad Norm: 1.0873 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9410/20628 | Avg Loss: 2.7954 | Grad Norm: 1.2898 | Step Time: 0.68s\n",
      "[Epoch 2/3] Step 9420/20628 | Avg Loss: 3.2799 | Grad Norm: 1.3682 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9430/20628 | Avg Loss: 2.6405 | Grad Norm: 0.9897 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9440/20628 | Avg Loss: 2.6111 | Grad Norm: 1.2513 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9450/20628 | Avg Loss: 2.8654 | Grad Norm: 1.0231 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9460/20628 | Avg Loss: 3.1024 | Grad Norm: 0.9056 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9470/20628 | Avg Loss: 2.4923 | Grad Norm: 1.1586 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9480/20628 | Avg Loss: 2.7993 | Grad Norm: 1.6813 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9490/20628 | Avg Loss: 3.2796 | Grad Norm: 1.1281 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 9500/20628 | Avg Loss: 3.3042 | Grad Norm: 1.0246 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9510/20628 | Avg Loss: 2.8451 | Grad Norm: 1.0443 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 9520/20628 | Avg Loss: 2.8843 | Grad Norm: 1.0884 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9530/20628 | Avg Loss: 3.0239 | Grad Norm: 1.0449 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9540/20628 | Avg Loss: 3.2926 | Grad Norm: 1.1115 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9550/20628 | Avg Loss: 2.8762 | Grad Norm: 1.1439 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 9560/20628 | Avg Loss: 3.0800 | Grad Norm: 1.4149 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9570/20628 | Avg Loss: 2.9694 | Grad Norm: 0.9720 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9580/20628 | Avg Loss: 2.6571 | Grad Norm: 1.4453 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9590/20628 | Avg Loss: 3.0590 | Grad Norm: 0.9802 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9600/20628 | Avg Loss: 2.8945 | Grad Norm: 1.0857 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 9610/20628 | Avg Loss: 2.3815 | Grad Norm: 1.0749 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9620/20628 | Avg Loss: 3.1562 | Grad Norm: 1.0942 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9630/20628 | Avg Loss: 2.8796 | Grad Norm: 0.9623 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9640/20628 | Avg Loss: 2.4823 | Grad Norm: 1.1623 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9650/20628 | Avg Loss: 2.6430 | Grad Norm: 1.1683 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9660/20628 | Avg Loss: 2.8645 | Grad Norm: 1.2432 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9670/20628 | Avg Loss: 2.9553 | Grad Norm: 0.9502 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9680/20628 | Avg Loss: 2.8602 | Grad Norm: 1.0281 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9690/20628 | Avg Loss: 3.0055 | Grad Norm: 1.0238 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9700/20628 | Avg Loss: 2.3723 | Grad Norm: 0.9177 | Step Time: 0.71s\n",
      "[Epoch 2/3] Step 9710/20628 | Avg Loss: 2.7454 | Grad Norm: 1.1763 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 9720/20628 | Avg Loss: 3.0598 | Grad Norm: 1.4012 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9730/20628 | Avg Loss: 3.1029 | Grad Norm: 1.2012 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9740/20628 | Avg Loss: 2.5749 | Grad Norm: 1.0611 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9750/20628 | Avg Loss: 3.1122 | Grad Norm: 1.0409 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9760/20628 | Avg Loss: 2.9394 | Grad Norm: 0.9595 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9770/20628 | Avg Loss: 3.2520 | Grad Norm: 1.0361 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 9780/20628 | Avg Loss: 3.1409 | Grad Norm: 0.9984 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9790/20628 | Avg Loss: 3.2872 | Grad Norm: 1.0256 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9800/20628 | Avg Loss: 2.7416 | Grad Norm: 1.0345 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9810/20628 | Avg Loss: 3.0446 | Grad Norm: 0.9360 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9820/20628 | Avg Loss: 3.1683 | Grad Norm: 1.0941 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9830/20628 | Avg Loss: 2.9382 | Grad Norm: 0.9544 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9840/20628 | Avg Loss: 3.0194 | Grad Norm: 1.0974 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9850/20628 | Avg Loss: 2.3155 | Grad Norm: 1.1276 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9860/20628 | Avg Loss: 3.2288 | Grad Norm: 0.9219 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9870/20628 | Avg Loss: 2.6815 | Grad Norm: 1.2085 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9880/20628 | Avg Loss: 3.0703 | Grad Norm: 1.0363 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9890/20628 | Avg Loss: 2.7452 | Grad Norm: 1.1565 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9900/20628 | Avg Loss: 3.0234 | Grad Norm: 0.9367 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9910/20628 | Avg Loss: 3.5593 | Grad Norm: 1.1762 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9920/20628 | Avg Loss: 2.2969 | Grad Norm: 0.9841 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9930/20628 | Avg Loss: 3.1327 | Grad Norm: 1.5841 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9940/20628 | Avg Loss: 3.0358 | Grad Norm: 1.1130 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9950/20628 | Avg Loss: 2.6967 | Grad Norm: 1.0189 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9960/20628 | Avg Loss: 3.0507 | Grad Norm: 0.9482 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9970/20628 | Avg Loss: 2.9127 | Grad Norm: 1.0752 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9980/20628 | Avg Loss: 2.6217 | Grad Norm: 1.0654 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 9990/20628 | Avg Loss: 2.6239 | Grad Norm: 0.9137 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10000/20628 | Avg Loss: 2.6684 | Grad Norm: 1.1753 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10010/20628 | Avg Loss: 2.8143 | Grad Norm: 1.1836 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 10020/20628 | Avg Loss: 2.4927 | Grad Norm: 1.0080 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10030/20628 | Avg Loss: 3.0242 | Grad Norm: 0.9787 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10040/20628 | Avg Loss: 2.4571 | Grad Norm: 1.2532 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10050/20628 | Avg Loss: 2.2536 | Grad Norm: 1.0893 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10060/20628 | Avg Loss: 3.1289 | Grad Norm: 1.3312 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 10070/20628 | Avg Loss: 3.0803 | Grad Norm: 1.3055 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10080/20628 | Avg Loss: 2.7650 | Grad Norm: 1.6330 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10090/20628 | Avg Loss: 2.9912 | Grad Norm: 0.8345 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10100/20628 | Avg Loss: 2.8539 | Grad Norm: 1.4950 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10110/20628 | Avg Loss: 2.9497 | Grad Norm: 1.1817 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10120/20628 | Avg Loss: 3.0147 | Grad Norm: 0.9326 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10130/20628 | Avg Loss: 2.7646 | Grad Norm: 1.0981 | Step Time: 0.68s\n",
      "[Epoch 2/3] Step 10140/20628 | Avg Loss: 2.4002 | Grad Norm: 1.2277 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10150/20628 | Avg Loss: 3.1532 | Grad Norm: 1.0416 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10160/20628 | Avg Loss: 2.8895 | Grad Norm: 0.9948 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10170/20628 | Avg Loss: 2.9861 | Grad Norm: 1.2066 | Step Time: 0.65s\n",
      "[Epoch 2/3] Step 10180/20628 | Avg Loss: 2.2605 | Grad Norm: 1.1702 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10190/20628 | Avg Loss: 2.8822 | Grad Norm: 1.2669 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10200/20628 | Avg Loss: 2.7525 | Grad Norm: 1.1659 | Step Time: 0.65s\n",
      "[Epoch 2/3] Step 10210/20628 | Avg Loss: 3.2791 | Grad Norm: 1.2550 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10220/20628 | Avg Loss: 2.6679 | Grad Norm: 1.0793 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10230/20628 | Avg Loss: 2.5792 | Grad Norm: 1.1750 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10240/20628 | Avg Loss: 3.2586 | Grad Norm: 1.0954 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 10250/20628 | Avg Loss: 2.7411 | Grad Norm: 1.5465 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10260/20628 | Avg Loss: 3.3036 | Grad Norm: 1.1533 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10270/20628 | Avg Loss: 3.1196 | Grad Norm: 0.9951 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10280/20628 | Avg Loss: 2.6617 | Grad Norm: 1.0313 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10290/20628 | Avg Loss: 2.8925 | Grad Norm: 1.0645 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10300/20628 | Avg Loss: 2.5472 | Grad Norm: 1.0820 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10310/20628 | Avg Loss: 2.7188 | Grad Norm: 1.2477 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10320/20628 | Avg Loss: 2.3835 | Grad Norm: 1.0456 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10330/20628 | Avg Loss: 2.9523 | Grad Norm: 1.2262 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 10340/20628 | Avg Loss: 2.8467 | Grad Norm: 1.0790 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 10350/20628 | Avg Loss: 3.0227 | Grad Norm: 1.0616 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10360/20628 | Avg Loss: 2.8109 | Grad Norm: 1.2815 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10370/20628 | Avg Loss: 2.7804 | Grad Norm: 1.2229 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 10380/20628 | Avg Loss: 3.0772 | Grad Norm: 1.7468 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10390/20628 | Avg Loss: 2.8264 | Grad Norm: 1.1274 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10400/20628 | Avg Loss: 2.8916 | Grad Norm: 1.0485 | Step Time: 0.68s\n",
      "[Epoch 2/3] Step 10410/20628 | Avg Loss: 2.8394 | Grad Norm: 1.0351 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10420/20628 | Avg Loss: 2.7715 | Grad Norm: 1.1263 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10430/20628 | Avg Loss: 2.8049 | Grad Norm: 1.3369 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10440/20628 | Avg Loss: 2.9975 | Grad Norm: 1.4825 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10450/20628 | Avg Loss: 2.8204 | Grad Norm: 1.2589 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 10460/20628 | Avg Loss: 2.6999 | Grad Norm: 0.9453 | Step Time: 0.68s\n",
      "[Epoch 2/3] Step 10470/20628 | Avg Loss: 3.1994 | Grad Norm: 1.2837 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10480/20628 | Avg Loss: 2.1048 | Grad Norm: 1.1681 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 10490/20628 | Avg Loss: 2.9482 | Grad Norm: 1.1323 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10500/20628 | Avg Loss: 2.9467 | Grad Norm: 1.0978 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10510/20628 | Avg Loss: 2.8578 | Grad Norm: 1.0244 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 10520/20628 | Avg Loss: 2.7941 | Grad Norm: 0.7738 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 10530/20628 | Avg Loss: 2.3916 | Grad Norm: 1.0177 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10540/20628 | Avg Loss: 2.3357 | Grad Norm: 1.0791 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10550/20628 | Avg Loss: 3.0099 | Grad Norm: 1.3042 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 10560/20628 | Avg Loss: 2.8918 | Grad Norm: 1.3828 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10570/20628 | Avg Loss: 2.4396 | Grad Norm: 0.9434 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 10580/20628 | Avg Loss: 3.0781 | Grad Norm: 1.5901 | Step Time: 0.65s\n",
      "[Epoch 2/3] Step 10590/20628 | Avg Loss: 2.9147 | Grad Norm: 0.8998 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10600/20628 | Avg Loss: 2.7254 | Grad Norm: 0.8919 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10610/20628 | Avg Loss: 2.9574 | Grad Norm: 1.0062 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10620/20628 | Avg Loss: 2.3946 | Grad Norm: 1.1183 | Step Time: 0.68s\n",
      "[Epoch 2/3] Step 10630/20628 | Avg Loss: 2.6323 | Grad Norm: 1.1720 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10640/20628 | Avg Loss: 2.1030 | Grad Norm: 1.6093 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10650/20628 | Avg Loss: 2.9116 | Grad Norm: 1.1495 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10660/20628 | Avg Loss: 2.9328 | Grad Norm: 1.0244 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10670/20628 | Avg Loss: 3.0693 | Grad Norm: 1.1085 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10680/20628 | Avg Loss: 3.0011 | Grad Norm: 0.9927 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 10690/20628 | Avg Loss: 2.6265 | Grad Norm: 0.9788 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10700/20628 | Avg Loss: 2.7726 | Grad Norm: 1.4183 | Step Time: 0.65s\n",
      "[Epoch 2/3] Step 10710/20628 | Avg Loss: 2.7573 | Grad Norm: 1.1045 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10720/20628 | Avg Loss: 3.0624 | Grad Norm: 1.0419 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10730/20628 | Avg Loss: 2.7246 | Grad Norm: 1.0973 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10740/20628 | Avg Loss: 2.9507 | Grad Norm: 1.1097 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10750/20628 | Avg Loss: 2.7524 | Grad Norm: 1.0428 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10760/20628 | Avg Loss: 2.4410 | Grad Norm: 1.2441 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10770/20628 | Avg Loss: 3.4907 | Grad Norm: 1.1318 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 10780/20628 | Avg Loss: 2.6527 | Grad Norm: 1.1166 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10790/20628 | Avg Loss: 2.9705 | Grad Norm: 1.9319 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10800/20628 | Avg Loss: 2.8591 | Grad Norm: 1.6257 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10810/20628 | Avg Loss: 2.7357 | Grad Norm: 1.0716 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10820/20628 | Avg Loss: 3.1672 | Grad Norm: 1.1711 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10830/20628 | Avg Loss: 2.8901 | Grad Norm: 1.1035 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10840/20628 | Avg Loss: 3.0176 | Grad Norm: 0.9513 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10850/20628 | Avg Loss: 2.7321 | Grad Norm: 1.0623 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10860/20628 | Avg Loss: 2.8019 | Grad Norm: 0.9445 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10870/20628 | Avg Loss: 2.7805 | Grad Norm: 1.1964 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 10880/20628 | Avg Loss: 2.3522 | Grad Norm: 1.1660 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10890/20628 | Avg Loss: 2.9254 | Grad Norm: 1.0812 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10900/20628 | Avg Loss: 2.7892 | Grad Norm: 1.6016 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10910/20628 | Avg Loss: 3.0626 | Grad Norm: 1.0082 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10920/20628 | Avg Loss: 3.2320 | Grad Norm: 1.2927 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 10930/20628 | Avg Loss: 2.9578 | Grad Norm: 1.1453 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10940/20628 | Avg Loss: 2.7459 | Grad Norm: 1.2078 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10950/20628 | Avg Loss: 3.0339 | Grad Norm: 1.2645 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10960/20628 | Avg Loss: 2.9991 | Grad Norm: 1.1825 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10970/20628 | Avg Loss: 2.6386 | Grad Norm: 0.9629 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10980/20628 | Avg Loss: 2.9620 | Grad Norm: 0.9485 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 10990/20628 | Avg Loss: 2.6585 | Grad Norm: 0.9798 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11000/20628 | Avg Loss: 2.6297 | Grad Norm: 0.9467 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11010/20628 | Avg Loss: 2.6999 | Grad Norm: 1.5223 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11020/20628 | Avg Loss: 2.8136 | Grad Norm: 1.2388 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11030/20628 | Avg Loss: 3.2417 | Grad Norm: 1.1852 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11040/20628 | Avg Loss: 2.6730 | Grad Norm: 1.2165 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11050/20628 | Avg Loss: 2.6519 | Grad Norm: 2.0724 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11060/20628 | Avg Loss: 3.0999 | Grad Norm: 1.2902 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11070/20628 | Avg Loss: 2.5805 | Grad Norm: 0.8163 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11080/20628 | Avg Loss: 2.9334 | Grad Norm: 1.2702 | Step Time: 0.68s\n",
      "[Epoch 2/3] Step 11090/20628 | Avg Loss: 2.5700 | Grad Norm: 1.3477 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11100/20628 | Avg Loss: 3.2508 | Grad Norm: 1.3340 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11110/20628 | Avg Loss: 2.9945 | Grad Norm: 1.6631 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11120/20628 | Avg Loss: 3.0299 | Grad Norm: 1.1558 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11130/20628 | Avg Loss: 2.9187 | Grad Norm: 1.1179 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11140/20628 | Avg Loss: 3.1326 | Grad Norm: 1.2026 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11150/20628 | Avg Loss: 2.6257 | Grad Norm: 1.1389 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11160/20628 | Avg Loss: 2.4723 | Grad Norm: 1.0021 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11170/20628 | Avg Loss: 2.3150 | Grad Norm: 1.4423 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 11180/20628 | Avg Loss: 3.0397 | Grad Norm: 1.3893 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11190/20628 | Avg Loss: 2.7160 | Grad Norm: 1.0857 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11200/20628 | Avg Loss: 2.5661 | Grad Norm: 1.0559 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 11210/20628 | Avg Loss: 2.3450 | Grad Norm: 1.2463 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11220/20628 | Avg Loss: 3.1607 | Grad Norm: 1.0610 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11230/20628 | Avg Loss: 2.7799 | Grad Norm: 1.3914 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11240/20628 | Avg Loss: 2.9305 | Grad Norm: 1.1396 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11250/20628 | Avg Loss: 3.3152 | Grad Norm: 1.1553 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11260/20628 | Avg Loss: 2.9563 | Grad Norm: 1.0565 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11270/20628 | Avg Loss: 2.8764 | Grad Norm: 1.1654 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11280/20628 | Avg Loss: 2.7309 | Grad Norm: 1.1830 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 11290/20628 | Avg Loss: 2.7556 | Grad Norm: 0.9299 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 11300/20628 | Avg Loss: 2.9156 | Grad Norm: 1.1753 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11310/20628 | Avg Loss: 3.0412 | Grad Norm: 1.1381 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11320/20628 | Avg Loss: 2.8136 | Grad Norm: 1.4354 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 11330/20628 | Avg Loss: 2.9310 | Grad Norm: 1.0359 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11340/20628 | Avg Loss: 3.2166 | Grad Norm: 1.0148 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11350/20628 | Avg Loss: 3.0065 | Grad Norm: 1.1714 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11360/20628 | Avg Loss: 2.8711 | Grad Norm: 1.0370 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11370/20628 | Avg Loss: 3.0714 | Grad Norm: 1.0511 | Step Time: 0.65s\n",
      "[Epoch 2/3] Step 11380/20628 | Avg Loss: 2.7457 | Grad Norm: 1.1321 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11390/20628 | Avg Loss: 2.9156 | Grad Norm: 0.9609 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11400/20628 | Avg Loss: 2.6360 | Grad Norm: 1.2941 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11410/20628 | Avg Loss: 3.0450 | Grad Norm: 1.1594 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11420/20628 | Avg Loss: 3.1542 | Grad Norm: 1.2578 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11430/20628 | Avg Loss: 2.3579 | Grad Norm: 0.9689 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11440/20628 | Avg Loss: 2.4653 | Grad Norm: 1.2417 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11450/20628 | Avg Loss: 2.8695 | Grad Norm: 0.9587 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11460/20628 | Avg Loss: 3.1569 | Grad Norm: 1.0084 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11470/20628 | Avg Loss: 3.0493 | Grad Norm: 1.2009 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11480/20628 | Avg Loss: 2.3790 | Grad Norm: 0.8722 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11490/20628 | Avg Loss: 2.6106 | Grad Norm: 0.7652 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11500/20628 | Avg Loss: 2.9468 | Grad Norm: 1.4004 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 11510/20628 | Avg Loss: 2.8486 | Grad Norm: 1.1674 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11520/20628 | Avg Loss: 2.9470 | Grad Norm: 1.2205 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 11530/20628 | Avg Loss: 3.5337 | Grad Norm: 1.0226 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11540/20628 | Avg Loss: 2.7322 | Grad Norm: 1.1210 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11550/20628 | Avg Loss: 2.4028 | Grad Norm: 1.1239 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11560/20628 | Avg Loss: 2.6642 | Grad Norm: 1.0679 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11570/20628 | Avg Loss: 3.1874 | Grad Norm: 1.5726 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11580/20628 | Avg Loss: 2.5547 | Grad Norm: 1.0810 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11590/20628 | Avg Loss: 3.2478 | Grad Norm: 1.2578 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 11600/20628 | Avg Loss: 2.9052 | Grad Norm: 2.2138 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11610/20628 | Avg Loss: 2.6252 | Grad Norm: 0.9770 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11620/20628 | Avg Loss: 3.1079 | Grad Norm: 1.0364 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11630/20628 | Avg Loss: 2.6299 | Grad Norm: 1.0013 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11640/20628 | Avg Loss: 2.3000 | Grad Norm: 1.2887 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11650/20628 | Avg Loss: 2.6722 | Grad Norm: 1.0309 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 11660/20628 | Avg Loss: 2.1185 | Grad Norm: 0.9706 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11670/20628 | Avg Loss: 2.8061 | Grad Norm: 1.0665 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11680/20628 | Avg Loss: 2.8671 | Grad Norm: 1.2167 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11690/20628 | Avg Loss: 3.2355 | Grad Norm: 1.1754 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11700/20628 | Avg Loss: 3.1351 | Grad Norm: 1.2125 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11710/20628 | Avg Loss: 2.9460 | Grad Norm: 1.1674 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11720/20628 | Avg Loss: 3.1658 | Grad Norm: 1.3304 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11730/20628 | Avg Loss: 2.8188 | Grad Norm: 1.1436 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11740/20628 | Avg Loss: 2.8513 | Grad Norm: 1.0429 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11750/20628 | Avg Loss: 2.8178 | Grad Norm: 1.1595 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11760/20628 | Avg Loss: 2.9467 | Grad Norm: 1.1280 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11770/20628 | Avg Loss: 2.8874 | Grad Norm: 1.2614 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 11780/20628 | Avg Loss: 2.6457 | Grad Norm: 1.0602 | Step Time: 0.68s\n",
      "[Epoch 2/3] Step 11790/20628 | Avg Loss: 2.4981 | Grad Norm: 1.4968 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11800/20628 | Avg Loss: 2.8501 | Grad Norm: 1.1237 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 11810/20628 | Avg Loss: 2.8220 | Grad Norm: 1.1260 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11820/20628 | Avg Loss: 2.4842 | Grad Norm: 1.3841 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 11830/20628 | Avg Loss: 2.8560 | Grad Norm: 1.1435 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 11840/20628 | Avg Loss: 2.9598 | Grad Norm: 1.4977 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11850/20628 | Avg Loss: 2.9097 | Grad Norm: 1.2579 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 11860/20628 | Avg Loss: 2.7312 | Grad Norm: 1.2324 | Step Time: 0.68s\n",
      "[Epoch 2/3] Step 11870/20628 | Avg Loss: 3.0753 | Grad Norm: 1.3545 | Step Time: 0.65s\n",
      "[Epoch 2/3] Step 11880/20628 | Avg Loss: 2.8026 | Grad Norm: 1.1166 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11890/20628 | Avg Loss: 3.0034 | Grad Norm: 1.0167 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 11900/20628 | Avg Loss: 2.1921 | Grad Norm: 1.2036 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11910/20628 | Avg Loss: 3.0850 | Grad Norm: 1.2554 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11920/20628 | Avg Loss: 2.4432 | Grad Norm: 1.1748 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11930/20628 | Avg Loss: 3.0604 | Grad Norm: 1.1256 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11940/20628 | Avg Loss: 2.6667 | Grad Norm: 0.9849 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11950/20628 | Avg Loss: 2.8185 | Grad Norm: 1.2391 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11960/20628 | Avg Loss: 2.9371 | Grad Norm: 1.3008 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11970/20628 | Avg Loss: 2.9156 | Grad Norm: 1.1805 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 11980/20628 | Avg Loss: 2.7563 | Grad Norm: 1.2540 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 11990/20628 | Avg Loss: 2.1250 | Grad Norm: 0.8236 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12000/20628 | Avg Loss: 2.8557 | Grad Norm: 1.1738 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12010/20628 | Avg Loss: 2.9970 | Grad Norm: 1.0409 | Step Time: 0.68s\n",
      "[Epoch 2/3] Step 12020/20628 | Avg Loss: 3.2878 | Grad Norm: 1.4107 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12030/20628 | Avg Loss: 2.4719 | Grad Norm: 1.2123 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12040/20628 | Avg Loss: 3.0866 | Grad Norm: 1.4609 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 12050/20628 | Avg Loss: 2.4572 | Grad Norm: 1.0736 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12060/20628 | Avg Loss: 2.8497 | Grad Norm: 1.1908 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12070/20628 | Avg Loss: 2.5468 | Grad Norm: 1.2004 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12080/20628 | Avg Loss: 2.9711 | Grad Norm: 1.0124 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12090/20628 | Avg Loss: 3.1468 | Grad Norm: 1.4442 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12100/20628 | Avg Loss: 2.9716 | Grad Norm: 1.0823 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 12110/20628 | Avg Loss: 2.9340 | Grad Norm: 1.1105 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12120/20628 | Avg Loss: 2.7793 | Grad Norm: 1.0863 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12130/20628 | Avg Loss: 2.8404 | Grad Norm: 1.4075 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12140/20628 | Avg Loss: 2.8143 | Grad Norm: 1.2305 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12150/20628 | Avg Loss: 3.0295 | Grad Norm: 1.1396 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12160/20628 | Avg Loss: 2.9682 | Grad Norm: 1.1791 | Step Time: 0.68s\n",
      "[Epoch 2/3] Step 12170/20628 | Avg Loss: 2.1428 | Grad Norm: 1.0388 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12180/20628 | Avg Loss: 2.7929 | Grad Norm: 1.0748 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12190/20628 | Avg Loss: 2.6050 | Grad Norm: 1.2403 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12200/20628 | Avg Loss: 2.9015 | Grad Norm: 0.9368 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12210/20628 | Avg Loss: 2.9536 | Grad Norm: 1.3512 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12220/20628 | Avg Loss: 2.6966 | Grad Norm: 1.0021 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12230/20628 | Avg Loss: 3.1163 | Grad Norm: 1.0661 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12240/20628 | Avg Loss: 3.2957 | Grad Norm: 1.3116 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12250/20628 | Avg Loss: 3.2454 | Grad Norm: 1.2391 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 12260/20628 | Avg Loss: 2.9114 | Grad Norm: 0.9595 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12270/20628 | Avg Loss: 2.5242 | Grad Norm: 1.1591 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12280/20628 | Avg Loss: 2.6731 | Grad Norm: 1.1353 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12290/20628 | Avg Loss: 2.6975 | Grad Norm: 1.0549 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12300/20628 | Avg Loss: 2.6069 | Grad Norm: 1.0860 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12310/20628 | Avg Loss: 3.0900 | Grad Norm: 1.7148 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12320/20628 | Avg Loss: 2.4579 | Grad Norm: 0.8294 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12330/20628 | Avg Loss: 2.1977 | Grad Norm: 1.2040 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12340/20628 | Avg Loss: 2.4773 | Grad Norm: 0.8787 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12350/20628 | Avg Loss: 2.6692 | Grad Norm: 0.9007 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12360/20628 | Avg Loss: 2.7469 | Grad Norm: 1.5381 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12370/20628 | Avg Loss: 2.9439 | Grad Norm: 0.9138 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12380/20628 | Avg Loss: 2.4837 | Grad Norm: 1.0274 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12390/20628 | Avg Loss: 3.3355 | Grad Norm: 1.1098 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12400/20628 | Avg Loss: 3.2524 | Grad Norm: 1.1812 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 12410/20628 | Avg Loss: 2.7933 | Grad Norm: 0.9927 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12420/20628 | Avg Loss: 2.7937 | Grad Norm: 0.9395 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12430/20628 | Avg Loss: 2.8742 | Grad Norm: 1.5125 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 12440/20628 | Avg Loss: 2.5535 | Grad Norm: 1.1563 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12450/20628 | Avg Loss: 3.0869 | Grad Norm: 1.1042 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12460/20628 | Avg Loss: 2.8645 | Grad Norm: 1.1209 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 12470/20628 | Avg Loss: 3.1032 | Grad Norm: 1.2303 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12480/20628 | Avg Loss: 3.0638 | Grad Norm: 1.0127 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12490/20628 | Avg Loss: 2.7180 | Grad Norm: 1.1509 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12500/20628 | Avg Loss: 2.9702 | Grad Norm: 1.0653 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12510/20628 | Avg Loss: 3.0712 | Grad Norm: 1.3298 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12520/20628 | Avg Loss: 2.5902 | Grad Norm: 1.1316 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12530/20628 | Avg Loss: 2.2516 | Grad Norm: 1.1403 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12540/20628 | Avg Loss: 2.9306 | Grad Norm: 1.6702 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12550/20628 | Avg Loss: 3.0215 | Grad Norm: 1.6600 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 12560/20628 | Avg Loss: 2.9625 | Grad Norm: 1.1043 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12570/20628 | Avg Loss: 2.6087 | Grad Norm: 1.7360 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12580/20628 | Avg Loss: 2.9760 | Grad Norm: 1.3152 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12590/20628 | Avg Loss: 3.0165 | Grad Norm: 1.0103 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12600/20628 | Avg Loss: 3.0559 | Grad Norm: 1.1216 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12610/20628 | Avg Loss: 3.1019 | Grad Norm: 1.1186 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12620/20628 | Avg Loss: 3.0686 | Grad Norm: 1.0787 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12630/20628 | Avg Loss: 2.5628 | Grad Norm: 1.2691 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12640/20628 | Avg Loss: 2.9797 | Grad Norm: 1.1425 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12650/20628 | Avg Loss: 3.0228 | Grad Norm: 1.1200 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12660/20628 | Avg Loss: 2.8425 | Grad Norm: 1.0072 | Step Time: 0.65s\n",
      "[Epoch 2/3] Step 12670/20628 | Avg Loss: 3.4372 | Grad Norm: 1.4525 | Step Time: 0.68s\n",
      "[Epoch 2/3] Step 12680/20628 | Avg Loss: 2.9669 | Grad Norm: 1.1411 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12690/20628 | Avg Loss: 2.9449 | Grad Norm: 2.0978 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12700/20628 | Avg Loss: 2.8924 | Grad Norm: 1.6235 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 12710/20628 | Avg Loss: 2.6381 | Grad Norm: 0.9516 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12720/20628 | Avg Loss: 2.1977 | Grad Norm: 1.1124 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12730/20628 | Avg Loss: 3.0768 | Grad Norm: 1.2500 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12740/20628 | Avg Loss: 2.6251 | Grad Norm: 0.9682 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12750/20628 | Avg Loss: 2.7791 | Grad Norm: 0.9775 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12760/20628 | Avg Loss: 3.4394 | Grad Norm: 1.4474 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12770/20628 | Avg Loss: 2.5801 | Grad Norm: 1.1822 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12780/20628 | Avg Loss: 2.7823 | Grad Norm: 1.1366 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12790/20628 | Avg Loss: 3.0097 | Grad Norm: 1.1213 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12800/20628 | Avg Loss: 2.9520 | Grad Norm: 1.1324 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12810/20628 | Avg Loss: 2.7857 | Grad Norm: 0.9961 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12820/20628 | Avg Loss: 3.2629 | Grad Norm: 1.3345 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12830/20628 | Avg Loss: 3.1583 | Grad Norm: 1.0794 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12840/20628 | Avg Loss: 2.7516 | Grad Norm: 1.0401 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12850/20628 | Avg Loss: 3.1441 | Grad Norm: 1.1620 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12860/20628 | Avg Loss: 2.8455 | Grad Norm: 0.9557 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12870/20628 | Avg Loss: 3.1859 | Grad Norm: 1.4749 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12880/20628 | Avg Loss: 2.7479 | Grad Norm: 1.1844 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12890/20628 | Avg Loss: 2.7819 | Grad Norm: 1.1352 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12900/20628 | Avg Loss: 2.4410 | Grad Norm: 1.0566 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12910/20628 | Avg Loss: 2.6431 | Grad Norm: 0.9487 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12920/20628 | Avg Loss: 3.0165 | Grad Norm: 1.0062 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12930/20628 | Avg Loss: 2.8522 | Grad Norm: 1.1421 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12940/20628 | Avg Loss: 3.4374 | Grad Norm: 1.4341 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12950/20628 | Avg Loss: 2.8575 | Grad Norm: 1.4379 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12960/20628 | Avg Loss: 2.9702 | Grad Norm: 1.0009 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12970/20628 | Avg Loss: 3.0914 | Grad Norm: 1.0870 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12980/20628 | Avg Loss: 2.7007 | Grad Norm: 1.2736 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 12990/20628 | Avg Loss: 2.2797 | Grad Norm: 1.1050 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13000/20628 | Avg Loss: 2.5544 | Grad Norm: 1.2883 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13010/20628 | Avg Loss: 2.7888 | Grad Norm: 1.3220 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13020/20628 | Avg Loss: 2.3677 | Grad Norm: 0.8754 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13030/20628 | Avg Loss: 2.7380 | Grad Norm: 0.9445 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13040/20628 | Avg Loss: 2.9124 | Grad Norm: 1.0907 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13050/20628 | Avg Loss: 2.8237 | Grad Norm: 1.1389 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13060/20628 | Avg Loss: 2.9308 | Grad Norm: 1.1087 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13070/20628 | Avg Loss: 2.5852 | Grad Norm: 1.1163 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13080/20628 | Avg Loss: 2.8921 | Grad Norm: 1.5150 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13090/20628 | Avg Loss: 2.5620 | Grad Norm: 1.3155 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13100/20628 | Avg Loss: 2.8556 | Grad Norm: 0.9438 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13110/20628 | Avg Loss: 2.4967 | Grad Norm: 1.0608 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13120/20628 | Avg Loss: 2.7357 | Grad Norm: 1.0212 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13130/20628 | Avg Loss: 2.6598 | Grad Norm: 1.0952 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13140/20628 | Avg Loss: 3.0075 | Grad Norm: 1.2125 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13150/20628 | Avg Loss: 2.7360 | Grad Norm: 0.9061 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13160/20628 | Avg Loss: 2.4648 | Grad Norm: 1.3517 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 13170/20628 | Avg Loss: 2.6381 | Grad Norm: 1.1031 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13180/20628 | Avg Loss: 2.9043 | Grad Norm: 1.1509 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13190/20628 | Avg Loss: 3.0999 | Grad Norm: 1.3163 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13200/20628 | Avg Loss: 2.8937 | Grad Norm: 1.1448 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13210/20628 | Avg Loss: 2.8064 | Grad Norm: 1.1593 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13220/20628 | Avg Loss: 2.4491 | Grad Norm: 1.0382 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13230/20628 | Avg Loss: 2.7994 | Grad Norm: 0.9477 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13240/20628 | Avg Loss: 2.8012 | Grad Norm: 1.1621 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13250/20628 | Avg Loss: 2.2434 | Grad Norm: 0.9097 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13260/20628 | Avg Loss: 2.5988 | Grad Norm: 1.1738 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13270/20628 | Avg Loss: 2.7181 | Grad Norm: 1.0862 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13280/20628 | Avg Loss: 2.8471 | Grad Norm: 1.2081 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13290/20628 | Avg Loss: 2.9165 | Grad Norm: 1.2105 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13300/20628 | Avg Loss: 2.8102 | Grad Norm: 1.0558 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13310/20628 | Avg Loss: 2.9009 | Grad Norm: 1.0440 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13320/20628 | Avg Loss: 2.7812 | Grad Norm: 1.2341 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13330/20628 | Avg Loss: 2.6578 | Grad Norm: 1.0646 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13340/20628 | Avg Loss: 3.1008 | Grad Norm: 1.2438 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13350/20628 | Avg Loss: 2.8699 | Grad Norm: 1.1450 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13360/20628 | Avg Loss: 3.1657 | Grad Norm: 1.1644 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13370/20628 | Avg Loss: 3.1420 | Grad Norm: 1.4954 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13380/20628 | Avg Loss: 2.8777 | Grad Norm: 1.1750 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13390/20628 | Avg Loss: 2.7982 | Grad Norm: 1.4042 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13400/20628 | Avg Loss: 3.0123 | Grad Norm: 1.2079 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13410/20628 | Avg Loss: 3.4121 | Grad Norm: 1.3373 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13420/20628 | Avg Loss: 2.9656 | Grad Norm: 1.2004 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13430/20628 | Avg Loss: 2.4392 | Grad Norm: 1.1256 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13440/20628 | Avg Loss: 2.6896 | Grad Norm: 1.3307 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13450/20628 | Avg Loss: 2.9330 | Grad Norm: 1.1446 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13460/20628 | Avg Loss: 3.1041 | Grad Norm: 1.4569 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13470/20628 | Avg Loss: 2.5620 | Grad Norm: 1.1809 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13480/20628 | Avg Loss: 3.5967 | Grad Norm: 1.1717 | Step Time: 0.68s\n",
      "[Epoch 2/3] Step 13490/20628 | Avg Loss: 2.9885 | Grad Norm: 0.7875 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13500/20628 | Avg Loss: 2.4672 | Grad Norm: 1.0937 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13510/20628 | Avg Loss: 3.1579 | Grad Norm: 1.2670 | Step Time: 0.68s\n",
      "[Epoch 2/3] Step 13520/20628 | Avg Loss: 2.6303 | Grad Norm: 1.0909 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13530/20628 | Avg Loss: 2.9901 | Grad Norm: 0.9598 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13540/20628 | Avg Loss: 3.0871 | Grad Norm: 1.3322 | Step Time: 0.68s\n",
      "[Epoch 2/3] Step 13550/20628 | Avg Loss: 2.9963 | Grad Norm: 0.9862 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13560/20628 | Avg Loss: 2.4430 | Grad Norm: 1.0931 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13570/20628 | Avg Loss: 3.1916 | Grad Norm: 1.3455 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13580/20628 | Avg Loss: 2.6756 | Grad Norm: 1.0900 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13590/20628 | Avg Loss: 2.8461 | Grad Norm: 1.2427 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13600/20628 | Avg Loss: 2.6873 | Grad Norm: 1.5333 | Step Time: 0.67s\n",
      "[Epoch 2/3] Step 13610/20628 | Avg Loss: 3.1335 | Grad Norm: 1.2446 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13620/20628 | Avg Loss: 2.7628 | Grad Norm: 1.1923 | Step Time: 0.65s\n",
      "[Epoch 2/3] Step 13630/20628 | Avg Loss: 2.6920 | Grad Norm: 1.6242 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13640/20628 | Avg Loss: 2.6688 | Grad Norm: 1.0519 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13650/20628 | Avg Loss: 2.8177 | Grad Norm: 1.1078 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13660/20628 | Avg Loss: 2.3618 | Grad Norm: 1.5070 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13670/20628 | Avg Loss: 2.5652 | Grad Norm: 1.4325 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13680/20628 | Avg Loss: 2.7356 | Grad Norm: 1.3723 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13690/20628 | Avg Loss: 2.5050 | Grad Norm: 0.9924 | Step Time: 0.70s\n",
      "[Epoch 2/3] Step 13700/20628 | Avg Loss: 3.1030 | Grad Norm: 1.2276 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13710/20628 | Avg Loss: 2.5956 | Grad Norm: 1.3138 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13720/20628 | Avg Loss: 2.8976 | Grad Norm: 1.1894 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13730/20628 | Avg Loss: 2.3826 | Grad Norm: 1.3535 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13740/20628 | Avg Loss: 2.5457 | Grad Norm: 1.1887 | Step Time: 0.66s\n",
      "[Epoch 2/3] Step 13750/20628 | Avg Loss: 2.9899 | Grad Norm: 1.1207 | Step Time: 0.68s\n",
      "\n",
      "âœ… Epoch 2 å®Œæˆ | Avg Loss: 2.8448 | Epoch Time: 01:16:53 | Total Time: 02:33:43\n",
      "\n",
      "ğŸ“ ç”ŸæˆéªŒè¯ï¼ˆéšæœºæ ·æœ¬ï¼‰:\n",
      "ä¸»é¢˜: Write a poem using the theme: Living,Youth,Activit...\n",
      "ç”Ÿæˆè¯—æ­Œ:\n",
      "Iâ€™ve seen them with their tails between their legs.\n",
      "Iâ€™ve heard them whining and whimpering.\n",
      "Itâ€™s not just me who is surprised that they arenâ€™t as strong or brave as weâ€™d like to believe.\n",
      "Weâ€™re afraid theyâ€™ll be disappointed when the day comes\n",
      "and they realize it isnâ€™t going to be a forever thing.\n",
      "They donâ€™t even want to go outside for walks anymore.\n",
      "Maybe one day they wonâ€™t come running at the sound of our voices,\n",
      "or wait by the door hoping we will bring home treats.\n",
      "This morning, I put on my running shoes to jog around the park.\n",
      "As I came up over the hill from the parking lot,\n",
      "my dog ran out with his leash attached to his collar.\n",
      "He wasnâ€™t tied down to something.\n",
      "He just let himself run free into the world.\n",
      "In the blink of an eye he was gone.\n",
      "Iâ€™m not sure if he saw something moving or sensed danger,\n",
      "but there are moments when I think heâ€™s smarter than we\n",
      "--------------------------------------------------\n",
      "[Epoch 3/3] Step 13760/20628 | Avg Loss: 2.8788 | Grad Norm: 1.2807 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 13770/20628 | Avg Loss: 2.7744 | Grad Norm: 1.4698 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 13780/20628 | Avg Loss: 2.9990 | Grad Norm: 1.2563 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 13790/20628 | Avg Loss: 2.9630 | Grad Norm: 1.1630 | Step Time: 0.84s\n",
      "[Epoch 3/3] Step 13800/20628 | Avg Loss: 2.3290 | Grad Norm: 0.9981 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 13810/20628 | Avg Loss: 3.2043 | Grad Norm: 1.1970 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 13820/20628 | Avg Loss: 2.7968 | Grad Norm: 0.9644 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 13830/20628 | Avg Loss: 2.5939 | Grad Norm: 1.2602 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 13840/20628 | Avg Loss: 2.6629 | Grad Norm: 1.2076 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 13850/20628 | Avg Loss: 2.6982 | Grad Norm: 1.0667 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 13860/20628 | Avg Loss: 2.9006 | Grad Norm: 1.1745 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 13870/20628 | Avg Loss: 2.4520 | Grad Norm: 1.3664 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 13880/20628 | Avg Loss: 2.9367 | Grad Norm: 1.4851 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 13890/20628 | Avg Loss: 2.7099 | Grad Norm: 1.2922 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 13900/20628 | Avg Loss: 2.6560 | Grad Norm: 1.4434 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 13910/20628 | Avg Loss: 2.6894 | Grad Norm: 1.3327 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 13920/20628 | Avg Loss: 2.5422 | Grad Norm: 1.1402 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 13930/20628 | Avg Loss: 3.1711 | Grad Norm: 1.4266 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 13940/20628 | Avg Loss: 2.9189 | Grad Norm: 1.4643 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 13950/20628 | Avg Loss: 2.6672 | Grad Norm: 1.2132 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 13960/20628 | Avg Loss: 2.9127 | Grad Norm: 1.1221 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 13970/20628 | Avg Loss: 2.7142 | Grad Norm: 1.0796 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 13980/20628 | Avg Loss: 2.7173 | Grad Norm: 1.1255 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 13990/20628 | Avg Loss: 3.0283 | Grad Norm: 1.4734 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 14000/20628 | Avg Loss: 2.9838 | Grad Norm: 1.3919 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14010/20628 | Avg Loss: 2.8301 | Grad Norm: 1.1512 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14020/20628 | Avg Loss: 2.8581 | Grad Norm: 1.0487 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 14030/20628 | Avg Loss: 2.5760 | Grad Norm: 1.0656 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14040/20628 | Avg Loss: 2.7534 | Grad Norm: 1.1028 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14050/20628 | Avg Loss: 3.0804 | Grad Norm: 1.1206 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14060/20628 | Avg Loss: 2.4232 | Grad Norm: 1.0643 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14070/20628 | Avg Loss: 2.5808 | Grad Norm: 0.8248 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14080/20628 | Avg Loss: 2.9864 | Grad Norm: 1.0256 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14090/20628 | Avg Loss: 2.2040 | Grad Norm: 1.0196 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14100/20628 | Avg Loss: 2.7539 | Grad Norm: 1.3494 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14110/20628 | Avg Loss: 2.7710 | Grad Norm: 1.4871 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 14120/20628 | Avg Loss: 3.1567 | Grad Norm: 1.3146 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 14130/20628 | Avg Loss: 3.2483 | Grad Norm: 1.2727 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14140/20628 | Avg Loss: 2.5854 | Grad Norm: 1.1992 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14150/20628 | Avg Loss: 3.3449 | Grad Norm: 1.3227 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14160/20628 | Avg Loss: 2.7169 | Grad Norm: 1.5173 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14170/20628 | Avg Loss: 2.7559 | Grad Norm: 1.1990 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14180/20628 | Avg Loss: 2.1219 | Grad Norm: 1.4956 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14190/20628 | Avg Loss: 2.5058 | Grad Norm: 1.4763 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14200/20628 | Avg Loss: 2.7685 | Grad Norm: 1.7051 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 14210/20628 | Avg Loss: 2.8414 | Grad Norm: 1.2385 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14220/20628 | Avg Loss: 2.6423 | Grad Norm: 1.1234 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14230/20628 | Avg Loss: 3.2201 | Grad Norm: 1.4205 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14240/20628 | Avg Loss: 3.2064 | Grad Norm: 1.1130 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14250/20628 | Avg Loss: 2.1657 | Grad Norm: 0.8150 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14260/20628 | Avg Loss: 2.8502 | Grad Norm: 1.4428 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14270/20628 | Avg Loss: 2.8795 | Grad Norm: 1.1198 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14280/20628 | Avg Loss: 2.5628 | Grad Norm: 1.1342 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14290/20628 | Avg Loss: 3.3478 | Grad Norm: 1.6710 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 14300/20628 | Avg Loss: 2.8800 | Grad Norm: 1.0492 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14310/20628 | Avg Loss: 2.8857 | Grad Norm: 1.1177 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14320/20628 | Avg Loss: 2.7984 | Grad Norm: 1.1429 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14330/20628 | Avg Loss: 2.8673 | Grad Norm: 1.1553 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14340/20628 | Avg Loss: 2.8348 | Grad Norm: 1.3696 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 14350/20628 | Avg Loss: 3.1392 | Grad Norm: 1.3001 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14360/20628 | Avg Loss: 3.0815 | Grad Norm: 1.2834 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14370/20628 | Avg Loss: 2.9526 | Grad Norm: 0.9904 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 14380/20628 | Avg Loss: 3.0725 | Grad Norm: 1.1517 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14390/20628 | Avg Loss: 2.3698 | Grad Norm: 1.4333 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14400/20628 | Avg Loss: 2.7068 | Grad Norm: 1.3111 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14410/20628 | Avg Loss: 2.4981 | Grad Norm: 1.1789 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14420/20628 | Avg Loss: 3.0503 | Grad Norm: 1.2449 | Step Time: 0.68s\n",
      "[Epoch 3/3] Step 14430/20628 | Avg Loss: 2.9120 | Grad Norm: 1.5875 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 14440/20628 | Avg Loss: 2.2562 | Grad Norm: 1.1308 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14450/20628 | Avg Loss: 3.1118 | Grad Norm: 1.1552 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14460/20628 | Avg Loss: 3.1357 | Grad Norm: 1.1936 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14470/20628 | Avg Loss: 2.5951 | Grad Norm: 1.0947 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14480/20628 | Avg Loss: 2.6181 | Grad Norm: 0.9721 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14490/20628 | Avg Loss: 2.9708 | Grad Norm: 1.2606 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14500/20628 | Avg Loss: 2.6708 | Grad Norm: 1.1301 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14510/20628 | Avg Loss: 2.8456 | Grad Norm: 1.4435 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14520/20628 | Avg Loss: 3.3107 | Grad Norm: 1.2813 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14530/20628 | Avg Loss: 2.6618 | Grad Norm: 1.2646 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14540/20628 | Avg Loss: 2.7286 | Grad Norm: 1.3451 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14550/20628 | Avg Loss: 2.6308 | Grad Norm: 1.2625 | Step Time: 0.68s\n",
      "[Epoch 3/3] Step 14560/20628 | Avg Loss: 2.6820 | Grad Norm: 1.6942 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 14570/20628 | Avg Loss: 2.6557 | Grad Norm: 1.1401 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14580/20628 | Avg Loss: 3.2057 | Grad Norm: 1.2814 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14590/20628 | Avg Loss: 3.0446 | Grad Norm: 1.5244 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14600/20628 | Avg Loss: 2.4132 | Grad Norm: 1.0638 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14610/20628 | Avg Loss: 2.9109 | Grad Norm: 1.1340 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14620/20628 | Avg Loss: 2.8123 | Grad Norm: 1.4718 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14630/20628 | Avg Loss: 2.7735 | Grad Norm: 1.2489 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14640/20628 | Avg Loss: 2.8539 | Grad Norm: 1.2785 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14650/20628 | Avg Loss: 3.4275 | Grad Norm: 1.3623 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14660/20628 | Avg Loss: 2.9687 | Grad Norm: 1.4569 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14670/20628 | Avg Loss: 2.6707 | Grad Norm: 1.2059 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14680/20628 | Avg Loss: 2.4733 | Grad Norm: 1.5890 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 14690/20628 | Avg Loss: 2.9596 | Grad Norm: 1.0566 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14700/20628 | Avg Loss: 3.0140 | Grad Norm: 1.4262 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14710/20628 | Avg Loss: 2.9194 | Grad Norm: 1.2657 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14720/20628 | Avg Loss: 2.8014 | Grad Norm: 1.0914 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14730/20628 | Avg Loss: 2.6884 | Grad Norm: 1.1475 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14740/20628 | Avg Loss: 2.3869 | Grad Norm: 1.2110 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14750/20628 | Avg Loss: 2.6525 | Grad Norm: 1.4360 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14760/20628 | Avg Loss: 2.6265 | Grad Norm: 1.6961 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14770/20628 | Avg Loss: 2.9264 | Grad Norm: 1.3752 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14780/20628 | Avg Loss: 2.9759 | Grad Norm: 1.3065 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14790/20628 | Avg Loss: 2.9034 | Grad Norm: 0.9149 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14800/20628 | Avg Loss: 2.8775 | Grad Norm: 1.4921 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14810/20628 | Avg Loss: 2.6671 | Grad Norm: 1.5743 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14820/20628 | Avg Loss: 2.9287 | Grad Norm: 1.1987 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14830/20628 | Avg Loss: 2.3385 | Grad Norm: 1.6150 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14840/20628 | Avg Loss: 2.9573 | Grad Norm: 1.0913 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14850/20628 | Avg Loss: 2.6804 | Grad Norm: 1.0092 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 14860/20628 | Avg Loss: 2.8742 | Grad Norm: 1.1180 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14870/20628 | Avg Loss: 2.5654 | Grad Norm: 1.0081 | Step Time: 0.68s\n",
      "[Epoch 3/3] Step 14880/20628 | Avg Loss: 2.8382 | Grad Norm: 1.6869 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14890/20628 | Avg Loss: 2.8257 | Grad Norm: 1.3147 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14900/20628 | Avg Loss: 2.7684 | Grad Norm: 1.0789 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14910/20628 | Avg Loss: 3.0340 | Grad Norm: 1.2709 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 14920/20628 | Avg Loss: 2.6622 | Grad Norm: 1.1088 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14930/20628 | Avg Loss: 2.6782 | Grad Norm: 1.2592 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14940/20628 | Avg Loss: 3.2368 | Grad Norm: 1.0912 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14950/20628 | Avg Loss: 2.4869 | Grad Norm: 0.9771 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14960/20628 | Avg Loss: 2.9899 | Grad Norm: 1.1497 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14970/20628 | Avg Loss: 2.1215 | Grad Norm: 1.2156 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14980/20628 | Avg Loss: 2.9578 | Grad Norm: 1.2731 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 14990/20628 | Avg Loss: 2.8321 | Grad Norm: 1.1567 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15000/20628 | Avg Loss: 2.8741 | Grad Norm: 1.1630 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15010/20628 | Avg Loss: 2.8516 | Grad Norm: 1.3658 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15020/20628 | Avg Loss: 3.1364 | Grad Norm: 1.2047 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15030/20628 | Avg Loss: 2.9637 | Grad Norm: 1.8264 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15040/20628 | Avg Loss: 2.6661 | Grad Norm: 1.3912 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15050/20628 | Avg Loss: 2.4418 | Grad Norm: 1.2243 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15060/20628 | Avg Loss: 2.9801 | Grad Norm: 1.1278 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15070/20628 | Avg Loss: 3.0681 | Grad Norm: 1.2422 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15080/20628 | Avg Loss: 3.1421 | Grad Norm: 1.2671 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15090/20628 | Avg Loss: 2.6817 | Grad Norm: 1.0358 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15100/20628 | Avg Loss: 2.9581 | Grad Norm: 1.2830 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15110/20628 | Avg Loss: 2.7426 | Grad Norm: 1.4160 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 15120/20628 | Avg Loss: 2.8373 | Grad Norm: 1.4026 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 15130/20628 | Avg Loss: 2.9895 | Grad Norm: 1.2632 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15140/20628 | Avg Loss: 2.6378 | Grad Norm: 1.6285 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15150/20628 | Avg Loss: 2.6637 | Grad Norm: 1.0752 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15160/20628 | Avg Loss: 2.5618 | Grad Norm: 1.2551 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15170/20628 | Avg Loss: 2.6930 | Grad Norm: 1.1527 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15180/20628 | Avg Loss: 2.9586 | Grad Norm: 1.3523 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15190/20628 | Avg Loss: 3.0327 | Grad Norm: 1.2649 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 15200/20628 | Avg Loss: 3.0881 | Grad Norm: 1.2856 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 15210/20628 | Avg Loss: 2.3301 | Grad Norm: 1.0625 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15220/20628 | Avg Loss: 2.9198 | Grad Norm: 1.1534 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 15230/20628 | Avg Loss: 2.9969 | Grad Norm: 1.0698 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15240/20628 | Avg Loss: 3.2359 | Grad Norm: 1.0825 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15250/20628 | Avg Loss: 3.4000 | Grad Norm: 1.0807 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 15260/20628 | Avg Loss: 3.0891 | Grad Norm: 1.2927 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15270/20628 | Avg Loss: 2.6536 | Grad Norm: 1.4034 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15280/20628 | Avg Loss: 3.3110 | Grad Norm: 1.5038 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15290/20628 | Avg Loss: 3.0815 | Grad Norm: 1.3404 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15300/20628 | Avg Loss: 2.6970 | Grad Norm: 1.9830 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15310/20628 | Avg Loss: 2.8802 | Grad Norm: 1.1716 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15320/20628 | Avg Loss: 2.9731 | Grad Norm: 1.1467 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15330/20628 | Avg Loss: 2.8691 | Grad Norm: 1.1462 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15340/20628 | Avg Loss: 2.9207 | Grad Norm: 1.1904 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15350/20628 | Avg Loss: 2.6544 | Grad Norm: 1.2264 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15360/20628 | Avg Loss: 3.0677 | Grad Norm: 1.3405 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 15370/20628 | Avg Loss: 3.0033 | Grad Norm: 1.5224 | Step Time: 0.68s\n",
      "[Epoch 3/3] Step 15380/20628 | Avg Loss: 2.6600 | Grad Norm: 1.3296 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15390/20628 | Avg Loss: 2.8606 | Grad Norm: 1.2222 | Step Time: 0.65s\n",
      "[Epoch 3/3] Step 15400/20628 | Avg Loss: 2.9491 | Grad Norm: 1.0168 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 15410/20628 | Avg Loss: 2.8635 | Grad Norm: 1.3082 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 15420/20628 | Avg Loss: 2.8866 | Grad Norm: 1.4917 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15430/20628 | Avg Loss: 2.5819 | Grad Norm: 1.1997 | Step Time: 0.70s\n",
      "[Epoch 3/3] Step 15440/20628 | Avg Loss: 2.5310 | Grad Norm: 1.8547 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15450/20628 | Avg Loss: 3.3719 | Grad Norm: 1.4882 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15460/20628 | Avg Loss: 3.0254 | Grad Norm: 1.5631 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15470/20628 | Avg Loss: 3.2186 | Grad Norm: 1.3848 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15480/20628 | Avg Loss: 2.7888 | Grad Norm: 1.0346 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15490/20628 | Avg Loss: 3.0883 | Grad Norm: 1.4483 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 15500/20628 | Avg Loss: 2.6524 | Grad Norm: 1.1616 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15510/20628 | Avg Loss: 3.0303 | Grad Norm: 1.4243 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15520/20628 | Avg Loss: 2.4296 | Grad Norm: 1.1207 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 15530/20628 | Avg Loss: 2.8581 | Grad Norm: 1.5381 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15540/20628 | Avg Loss: 2.9558 | Grad Norm: 1.5501 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15550/20628 | Avg Loss: 2.6785 | Grad Norm: 1.2855 | Step Time: 0.68s\n",
      "[Epoch 3/3] Step 15560/20628 | Avg Loss: 3.2862 | Grad Norm: 1.4015 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15570/20628 | Avg Loss: 2.9710 | Grad Norm: 1.0931 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15580/20628 | Avg Loss: 3.2641 | Grad Norm: 1.3526 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15590/20628 | Avg Loss: 2.7042 | Grad Norm: 1.1381 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 15600/20628 | Avg Loss: 2.7980 | Grad Norm: 1.3036 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 15610/20628 | Avg Loss: 2.7894 | Grad Norm: 1.5038 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15620/20628 | Avg Loss: 2.9209 | Grad Norm: 0.8476 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15630/20628 | Avg Loss: 2.9657 | Grad Norm: 1.2992 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15640/20628 | Avg Loss: 2.6480 | Grad Norm: 1.1673 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15650/20628 | Avg Loss: 3.2073 | Grad Norm: 1.4156 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 15660/20628 | Avg Loss: 2.7038 | Grad Norm: 1.2024 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15670/20628 | Avg Loss: 2.6338 | Grad Norm: 1.3787 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 15680/20628 | Avg Loss: 3.1598 | Grad Norm: 1.5015 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15690/20628 | Avg Loss: 2.9349 | Grad Norm: 1.1187 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15700/20628 | Avg Loss: 3.0903 | Grad Norm: 1.3814 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15710/20628 | Avg Loss: 3.0021 | Grad Norm: 1.0180 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15720/20628 | Avg Loss: 2.6762 | Grad Norm: 1.1939 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15730/20628 | Avg Loss: 2.6580 | Grad Norm: 1.1789 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 15740/20628 | Avg Loss: 2.6853 | Grad Norm: 1.0876 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15750/20628 | Avg Loss: 2.8574 | Grad Norm: 1.1570 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15760/20628 | Avg Loss: 2.4243 | Grad Norm: 1.7882 | Step Time: 0.70s\n",
      "[Epoch 3/3] Step 15770/20628 | Avg Loss: 2.6707 | Grad Norm: 1.2228 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15780/20628 | Avg Loss: 2.9231 | Grad Norm: 0.9348 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15790/20628 | Avg Loss: 2.8154 | Grad Norm: 1.4651 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 15800/20628 | Avg Loss: 2.9115 | Grad Norm: 1.1905 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15810/20628 | Avg Loss: 3.1147 | Grad Norm: 1.4165 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15820/20628 | Avg Loss: 3.1719 | Grad Norm: 1.2112 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15830/20628 | Avg Loss: 2.8096 | Grad Norm: 1.2826 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15840/20628 | Avg Loss: 2.8917 | Grad Norm: 1.3011 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15850/20628 | Avg Loss: 3.1271 | Grad Norm: 1.4702 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 15860/20628 | Avg Loss: 2.4955 | Grad Norm: 1.3950 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15870/20628 | Avg Loss: 2.8187 | Grad Norm: 1.2630 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 15880/20628 | Avg Loss: 3.2343 | Grad Norm: 1.2773 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15890/20628 | Avg Loss: 2.7696 | Grad Norm: 1.1799 | Step Time: 0.69s\n",
      "[Epoch 3/3] Step 15900/20628 | Avg Loss: 3.1010 | Grad Norm: 1.1787 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15910/20628 | Avg Loss: 2.4254 | Grad Norm: 0.9769 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 15920/20628 | Avg Loss: 2.5929 | Grad Norm: 1.2663 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15930/20628 | Avg Loss: 2.5722 | Grad Norm: 1.4489 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15940/20628 | Avg Loss: 2.0674 | Grad Norm: 0.8304 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15950/20628 | Avg Loss: 2.7027 | Grad Norm: 1.9092 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15960/20628 | Avg Loss: 2.9751 | Grad Norm: 1.5055 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15970/20628 | Avg Loss: 2.9166 | Grad Norm: 1.4944 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15980/20628 | Avg Loss: 2.9230 | Grad Norm: 1.2291 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 15990/20628 | Avg Loss: 3.0417 | Grad Norm: 1.7441 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16000/20628 | Avg Loss: 2.6511 | Grad Norm: 1.3206 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 16010/20628 | Avg Loss: 2.9999 | Grad Norm: 1.1531 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16020/20628 | Avg Loss: 2.9206 | Grad Norm: 1.0463 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16030/20628 | Avg Loss: 3.0974 | Grad Norm: 1.5627 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16040/20628 | Avg Loss: 3.0761 | Grad Norm: 1.9358 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16050/20628 | Avg Loss: 2.9215 | Grad Norm: 1.1058 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16060/20628 | Avg Loss: 2.8524 | Grad Norm: 1.1468 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16070/20628 | Avg Loss: 2.7752 | Grad Norm: 1.1960 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16080/20628 | Avg Loss: 2.7150 | Grad Norm: 1.5285 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16090/20628 | Avg Loss: 2.6721 | Grad Norm: 1.0657 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 16100/20628 | Avg Loss: 2.5882 | Grad Norm: 1.0884 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16110/20628 | Avg Loss: 3.0316 | Grad Norm: 1.2499 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16120/20628 | Avg Loss: 3.1839 | Grad Norm: 1.4173 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16130/20628 | Avg Loss: 2.7827 | Grad Norm: 1.2004 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16140/20628 | Avg Loss: 2.8688 | Grad Norm: 1.5428 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16150/20628 | Avg Loss: 3.1056 | Grad Norm: 1.3721 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16160/20628 | Avg Loss: 2.4114 | Grad Norm: 1.0673 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16170/20628 | Avg Loss: 2.8136 | Grad Norm: 1.4500 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16180/20628 | Avg Loss: 2.1932 | Grad Norm: 1.1366 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 16190/20628 | Avg Loss: 2.4800 | Grad Norm: 1.0101 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16200/20628 | Avg Loss: 2.4713 | Grad Norm: 1.2754 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16210/20628 | Avg Loss: 2.1451 | Grad Norm: 1.1298 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16220/20628 | Avg Loss: 2.5135 | Grad Norm: 0.8789 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16230/20628 | Avg Loss: 2.8981 | Grad Norm: 1.1872 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16240/20628 | Avg Loss: 2.7657 | Grad Norm: 1.1317 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16250/20628 | Avg Loss: 3.2863 | Grad Norm: 1.2396 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16260/20628 | Avg Loss: 2.7557 | Grad Norm: 1.2717 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16270/20628 | Avg Loss: 3.4270 | Grad Norm: 1.1601 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 16280/20628 | Avg Loss: 2.8536 | Grad Norm: 1.1672 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16290/20628 | Avg Loss: 2.6303 | Grad Norm: 1.3517 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16300/20628 | Avg Loss: 2.9462 | Grad Norm: 1.0986 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 16310/20628 | Avg Loss: 2.6429 | Grad Norm: 1.1928 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16320/20628 | Avg Loss: 2.9539 | Grad Norm: 1.7731 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16330/20628 | Avg Loss: 3.0275 | Grad Norm: 1.3252 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16340/20628 | Avg Loss: 3.0750 | Grad Norm: 1.0916 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16350/20628 | Avg Loss: 2.3911 | Grad Norm: 1.1956 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16360/20628 | Avg Loss: 3.0471 | Grad Norm: 2.1070 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16370/20628 | Avg Loss: 2.7911 | Grad Norm: 1.4348 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16380/20628 | Avg Loss: 3.1020 | Grad Norm: 1.2743 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16390/20628 | Avg Loss: 3.1273 | Grad Norm: 1.3287 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16400/20628 | Avg Loss: 3.1855 | Grad Norm: 1.2975 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16410/20628 | Avg Loss: 2.9628 | Grad Norm: 1.3585 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16420/20628 | Avg Loss: 3.1565 | Grad Norm: 1.0748 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16430/20628 | Avg Loss: 3.1518 | Grad Norm: 1.1066 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16440/20628 | Avg Loss: 3.0493 | Grad Norm: 1.4448 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16450/20628 | Avg Loss: 3.3149 | Grad Norm: 1.2644 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16460/20628 | Avg Loss: 2.5887 | Grad Norm: 1.0999 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16470/20628 | Avg Loss: 2.7647 | Grad Norm: 1.0035 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16480/20628 | Avg Loss: 2.7212 | Grad Norm: 1.8810 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16490/20628 | Avg Loss: 2.9135 | Grad Norm: 1.3322 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16500/20628 | Avg Loss: 2.7630 | Grad Norm: 1.3199 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16510/20628 | Avg Loss: 2.7894 | Grad Norm: 1.1515 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16520/20628 | Avg Loss: 3.0959 | Grad Norm: 1.3442 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16530/20628 | Avg Loss: 3.1351 | Grad Norm: 1.2416 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16540/20628 | Avg Loss: 2.8952 | Grad Norm: 1.1208 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16550/20628 | Avg Loss: 2.8145 | Grad Norm: 1.6015 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16560/20628 | Avg Loss: 2.7694 | Grad Norm: 1.1005 | Step Time: 0.65s\n",
      "[Epoch 3/3] Step 16570/20628 | Avg Loss: 3.0678 | Grad Norm: 1.0851 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16580/20628 | Avg Loss: 2.7712 | Grad Norm: 1.2053 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16590/20628 | Avg Loss: 2.8822 | Grad Norm: 1.4061 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16600/20628 | Avg Loss: 3.0500 | Grad Norm: 1.1612 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16610/20628 | Avg Loss: 2.8121 | Grad Norm: 1.3285 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16620/20628 | Avg Loss: 2.8712 | Grad Norm: 1.1214 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16630/20628 | Avg Loss: 2.7371 | Grad Norm: 1.3747 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16640/20628 | Avg Loss: 3.1649 | Grad Norm: 1.3313 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16650/20628 | Avg Loss: 2.4663 | Grad Norm: 1.7795 | Step Time: 0.65s\n",
      "[Epoch 3/3] Step 16660/20628 | Avg Loss: 2.7413 | Grad Norm: 0.9928 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16670/20628 | Avg Loss: 2.8056 | Grad Norm: 1.3823 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16680/20628 | Avg Loss: 2.8005 | Grad Norm: 1.2018 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16690/20628 | Avg Loss: 3.1332 | Grad Norm: 1.4464 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16700/20628 | Avg Loss: 2.5822 | Grad Norm: 1.2463 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16710/20628 | Avg Loss: 2.8379 | Grad Norm: 1.1001 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16720/20628 | Avg Loss: 2.7077 | Grad Norm: 1.1909 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16730/20628 | Avg Loss: 3.0292 | Grad Norm: 1.2469 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16740/20628 | Avg Loss: 2.9834 | Grad Norm: 1.4689 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16750/20628 | Avg Loss: 2.3803 | Grad Norm: 2.0649 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 16760/20628 | Avg Loss: 2.5348 | Grad Norm: 1.2103 | Step Time: 0.72s\n",
      "[Epoch 3/3] Step 16770/20628 | Avg Loss: 2.9660 | Grad Norm: 1.0802 | Step Time: 0.68s\n",
      "[Epoch 3/3] Step 16780/20628 | Avg Loss: 2.9390 | Grad Norm: 1.1259 | Step Time: 0.71s\n",
      "[Epoch 3/3] Step 16790/20628 | Avg Loss: 2.9743 | Grad Norm: 1.3496 | Step Time: 0.72s\n",
      "[Epoch 3/3] Step 16800/20628 | Avg Loss: 2.5651 | Grad Norm: 1.1100 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16810/20628 | Avg Loss: 2.8287 | Grad Norm: 1.4000 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16820/20628 | Avg Loss: 2.6460 | Grad Norm: 1.4007 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16830/20628 | Avg Loss: 3.1736 | Grad Norm: 1.6789 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 16840/20628 | Avg Loss: 2.9543 | Grad Norm: 1.1385 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 16850/20628 | Avg Loss: 3.0049 | Grad Norm: 1.1397 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16860/20628 | Avg Loss: 2.9954 | Grad Norm: 1.2753 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16870/20628 | Avg Loss: 3.0947 | Grad Norm: 1.2898 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16880/20628 | Avg Loss: 2.7754 | Grad Norm: 1.1042 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16890/20628 | Avg Loss: 3.1243 | Grad Norm: 1.3243 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16900/20628 | Avg Loss: 2.7059 | Grad Norm: 1.2676 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16910/20628 | Avg Loss: 3.1399 | Grad Norm: 1.0819 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16920/20628 | Avg Loss: 2.6759 | Grad Norm: 1.6629 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16930/20628 | Avg Loss: 2.9790 | Grad Norm: 1.1590 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16940/20628 | Avg Loss: 2.9874 | Grad Norm: 1.1554 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16950/20628 | Avg Loss: 2.6432 | Grad Norm: 1.1116 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16960/20628 | Avg Loss: 2.8426 | Grad Norm: 1.3653 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 16970/20628 | Avg Loss: 3.1280 | Grad Norm: 1.1804 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16980/20628 | Avg Loss: 2.6484 | Grad Norm: 1.1754 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 16990/20628 | Avg Loss: 2.5026 | Grad Norm: 1.1810 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17000/20628 | Avg Loss: 2.6241 | Grad Norm: 1.2739 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17010/20628 | Avg Loss: 3.0250 | Grad Norm: 1.2665 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17020/20628 | Avg Loss: 2.8305 | Grad Norm: 1.5985 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17030/20628 | Avg Loss: 3.2302 | Grad Norm: 1.2351 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17040/20628 | Avg Loss: 2.5849 | Grad Norm: 1.0338 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17050/20628 | Avg Loss: 3.0020 | Grad Norm: 1.1430 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17060/20628 | Avg Loss: 2.3658 | Grad Norm: 1.3204 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17070/20628 | Avg Loss: 2.6998 | Grad Norm: 1.5070 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17080/20628 | Avg Loss: 2.6975 | Grad Norm: 1.1948 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17090/20628 | Avg Loss: 2.7757 | Grad Norm: 1.2046 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17100/20628 | Avg Loss: 2.5471 | Grad Norm: 1.0544 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17110/20628 | Avg Loss: 2.9927 | Grad Norm: 1.3867 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17120/20628 | Avg Loss: 2.4637 | Grad Norm: 1.6428 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17130/20628 | Avg Loss: 3.4885 | Grad Norm: 1.3459 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17140/20628 | Avg Loss: 3.1069 | Grad Norm: 1.3069 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17150/20628 | Avg Loss: 3.1980 | Grad Norm: 1.2750 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17160/20628 | Avg Loss: 2.7700 | Grad Norm: 1.1143 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17170/20628 | Avg Loss: 2.3751 | Grad Norm: 1.1694 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17180/20628 | Avg Loss: 2.6534 | Grad Norm: 1.2579 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17190/20628 | Avg Loss: 3.3533 | Grad Norm: 1.2948 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17200/20628 | Avg Loss: 3.1066 | Grad Norm: 1.3246 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17210/20628 | Avg Loss: 2.7369 | Grad Norm: 1.2918 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17220/20628 | Avg Loss: 2.9323 | Grad Norm: 1.1868 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17230/20628 | Avg Loss: 2.9167 | Grad Norm: 1.2164 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17240/20628 | Avg Loss: 2.6442 | Grad Norm: 1.1399 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17250/20628 | Avg Loss: 2.8021 | Grad Norm: 1.6751 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17260/20628 | Avg Loss: 2.8555 | Grad Norm: 1.2375 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17270/20628 | Avg Loss: 2.4732 | Grad Norm: 1.2131 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17280/20628 | Avg Loss: 2.7438 | Grad Norm: 1.5122 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17290/20628 | Avg Loss: 3.0793 | Grad Norm: 1.6898 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17300/20628 | Avg Loss: 2.9455 | Grad Norm: 1.1395 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17310/20628 | Avg Loss: 2.9289 | Grad Norm: 1.3346 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17320/20628 | Avg Loss: 2.6606 | Grad Norm: 1.1151 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17330/20628 | Avg Loss: 3.0958 | Grad Norm: 1.3708 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17340/20628 | Avg Loss: 3.0767 | Grad Norm: 1.3989 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17350/20628 | Avg Loss: 2.9668 | Grad Norm: 1.3129 | Step Time: 0.70s\n",
      "[Epoch 3/3] Step 17360/20628 | Avg Loss: 3.1989 | Grad Norm: 1.4069 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17370/20628 | Avg Loss: 2.9671 | Grad Norm: 1.1956 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17380/20628 | Avg Loss: 2.9107 | Grad Norm: 2.0590 | Step Time: 0.74s\n",
      "[Epoch 3/3] Step 17390/20628 | Avg Loss: 2.9884 | Grad Norm: 1.3228 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17400/20628 | Avg Loss: 3.3158 | Grad Norm: 1.3786 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17410/20628 | Avg Loss: 2.6373 | Grad Norm: 1.1755 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17420/20628 | Avg Loss: 2.9513 | Grad Norm: 1.1415 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17430/20628 | Avg Loss: 2.8263 | Grad Norm: 1.3076 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17440/20628 | Avg Loss: 2.7837 | Grad Norm: 1.7274 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17450/20628 | Avg Loss: 2.8453 | Grad Norm: 1.1878 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17460/20628 | Avg Loss: 3.5198 | Grad Norm: 1.1123 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17470/20628 | Avg Loss: 2.4959 | Grad Norm: 1.2066 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17480/20628 | Avg Loss: 2.7179 | Grad Norm: 1.2629 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17490/20628 | Avg Loss: 2.3283 | Grad Norm: 1.2464 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17500/20628 | Avg Loss: 2.8825 | Grad Norm: 1.3739 | Step Time: 0.68s\n",
      "[Epoch 3/3] Step 17510/20628 | Avg Loss: 2.5365 | Grad Norm: 1.2014 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17520/20628 | Avg Loss: 2.4638 | Grad Norm: 1.3164 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17530/20628 | Avg Loss: 2.6876 | Grad Norm: 1.3956 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17540/20628 | Avg Loss: 3.1527 | Grad Norm: 1.1643 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17550/20628 | Avg Loss: 3.2338 | Grad Norm: 1.9587 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17560/20628 | Avg Loss: 2.7107 | Grad Norm: 1.4904 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17570/20628 | Avg Loss: 2.7434 | Grad Norm: 1.1508 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17580/20628 | Avg Loss: 2.6384 | Grad Norm: 1.9371 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17590/20628 | Avg Loss: 2.8190 | Grad Norm: 1.5665 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17600/20628 | Avg Loss: 3.3658 | Grad Norm: 1.3932 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17610/20628 | Avg Loss: 2.7579 | Grad Norm: 1.3562 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17620/20628 | Avg Loss: 2.6320 | Grad Norm: 1.3053 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17630/20628 | Avg Loss: 2.6931 | Grad Norm: 1.3958 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17640/20628 | Avg Loss: 2.9191 | Grad Norm: 1.3695 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17650/20628 | Avg Loss: 2.9210 | Grad Norm: 1.0440 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17660/20628 | Avg Loss: 2.9332 | Grad Norm: 1.3813 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17670/20628 | Avg Loss: 3.0213 | Grad Norm: 1.3149 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17680/20628 | Avg Loss: 2.8409 | Grad Norm: 1.2538 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17690/20628 | Avg Loss: 2.8830 | Grad Norm: 1.5884 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17700/20628 | Avg Loss: 3.0884 | Grad Norm: 1.4086 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17710/20628 | Avg Loss: 2.9695 | Grad Norm: 1.1746 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17720/20628 | Avg Loss: 2.6652 | Grad Norm: 1.3153 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17730/20628 | Avg Loss: 2.7480 | Grad Norm: 1.3763 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17740/20628 | Avg Loss: 2.6994 | Grad Norm: 1.1050 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17750/20628 | Avg Loss: 2.6430 | Grad Norm: 1.1135 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17760/20628 | Avg Loss: 2.7429 | Grad Norm: 1.4695 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17770/20628 | Avg Loss: 2.7832 | Grad Norm: 1.6816 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17780/20628 | Avg Loss: 2.7362 | Grad Norm: 1.3436 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17790/20628 | Avg Loss: 2.7913 | Grad Norm: 1.1640 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17800/20628 | Avg Loss: 2.5742 | Grad Norm: 1.3301 | Step Time: 0.69s\n",
      "[Epoch 3/3] Step 17810/20628 | Avg Loss: 2.5307 | Grad Norm: 1.4701 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17820/20628 | Avg Loss: 2.4282 | Grad Norm: 1.1598 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17830/20628 | Avg Loss: 2.9733 | Grad Norm: 1.9017 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17840/20628 | Avg Loss: 3.2462 | Grad Norm: 1.4995 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17850/20628 | Avg Loss: 2.7715 | Grad Norm: 1.3342 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17860/20628 | Avg Loss: 2.5270 | Grad Norm: 1.2857 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17870/20628 | Avg Loss: 2.7281 | Grad Norm: 1.4696 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17880/20628 | Avg Loss: 2.5447 | Grad Norm: 1.8914 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17890/20628 | Avg Loss: 2.6141 | Grad Norm: 1.0217 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17900/20628 | Avg Loss: 2.6440 | Grad Norm: 1.3445 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17910/20628 | Avg Loss: 2.8709 | Grad Norm: 1.2794 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17920/20628 | Avg Loss: 2.8058 | Grad Norm: 1.4099 | Step Time: 0.68s\n",
      "[Epoch 3/3] Step 17930/20628 | Avg Loss: 2.9748 | Grad Norm: 1.4128 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17940/20628 | Avg Loss: 2.7616 | Grad Norm: 1.2927 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17950/20628 | Avg Loss: 3.3229 | Grad Norm: 1.3315 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17960/20628 | Avg Loss: 2.4827 | Grad Norm: 1.1537 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17970/20628 | Avg Loss: 2.6793 | Grad Norm: 1.2635 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 17980/20628 | Avg Loss: 2.9402 | Grad Norm: 1.3030 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 17990/20628 | Avg Loss: 2.6954 | Grad Norm: 1.7328 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18000/20628 | Avg Loss: 3.0321 | Grad Norm: 1.3200 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 18010/20628 | Avg Loss: 3.0786 | Grad Norm: 1.1867 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 18020/20628 | Avg Loss: 2.7728 | Grad Norm: 1.2596 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18030/20628 | Avg Loss: 3.1222 | Grad Norm: 1.1125 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18040/20628 | Avg Loss: 2.4224 | Grad Norm: 1.3551 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18050/20628 | Avg Loss: 2.7406 | Grad Norm: 1.2133 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18060/20628 | Avg Loss: 2.9157 | Grad Norm: 1.1531 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18070/20628 | Avg Loss: 2.9220 | Grad Norm: 1.1115 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18080/20628 | Avg Loss: 2.9428 | Grad Norm: 1.2833 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18090/20628 | Avg Loss: 3.1987 | Grad Norm: 1.5102 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 18100/20628 | Avg Loss: 3.2043 | Grad Norm: 1.3542 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18110/20628 | Avg Loss: 3.1198 | Grad Norm: 1.0857 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18120/20628 | Avg Loss: 2.7841 | Grad Norm: 1.5636 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 18130/20628 | Avg Loss: 3.3970 | Grad Norm: 1.6000 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 18140/20628 | Avg Loss: 2.0071 | Grad Norm: 1.2588 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18150/20628 | Avg Loss: 2.3462 | Grad Norm: 1.1229 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 18160/20628 | Avg Loss: 2.1139 | Grad Norm: 1.1008 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18170/20628 | Avg Loss: 2.8638 | Grad Norm: 1.1690 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18180/20628 | Avg Loss: 2.6922 | Grad Norm: 1.2458 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 18190/20628 | Avg Loss: 2.7361 | Grad Norm: 1.1035 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 18200/20628 | Avg Loss: 2.8344 | Grad Norm: 1.5603 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18210/20628 | Avg Loss: 2.7062 | Grad Norm: 1.2952 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18220/20628 | Avg Loss: 3.0327 | Grad Norm: 1.1285 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18230/20628 | Avg Loss: 2.9591 | Grad Norm: 1.4894 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18240/20628 | Avg Loss: 2.9358 | Grad Norm: 1.3523 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 18250/20628 | Avg Loss: 2.7674 | Grad Norm: 1.2037 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18260/20628 | Avg Loss: 2.7016 | Grad Norm: 1.4453 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18270/20628 | Avg Loss: 3.1253 | Grad Norm: 1.4113 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18280/20628 | Avg Loss: 2.8991 | Grad Norm: 1.4906 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18290/20628 | Avg Loss: 2.7807 | Grad Norm: 1.3243 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18300/20628 | Avg Loss: 2.6260 | Grad Norm: 1.2414 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18310/20628 | Avg Loss: 2.8055 | Grad Norm: 1.2446 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18320/20628 | Avg Loss: 2.7131 | Grad Norm: 1.1318 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18330/20628 | Avg Loss: 2.3528 | Grad Norm: 1.1023 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 18340/20628 | Avg Loss: 3.0879 | Grad Norm: 1.3544 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 18350/20628 | Avg Loss: 2.6300 | Grad Norm: 1.2377 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18360/20628 | Avg Loss: 2.5012 | Grad Norm: 1.4350 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18370/20628 | Avg Loss: 2.1174 | Grad Norm: 1.0142 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 18380/20628 | Avg Loss: 2.4924 | Grad Norm: 1.4610 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18390/20628 | Avg Loss: 3.0953 | Grad Norm: 1.2099 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 18400/20628 | Avg Loss: 2.9166 | Grad Norm: 1.1455 | Step Time: 0.69s\n",
      "[Epoch 3/3] Step 18410/20628 | Avg Loss: 2.8693 | Grad Norm: 1.4793 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18420/20628 | Avg Loss: 3.1231 | Grad Norm: 1.4256 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 18430/20628 | Avg Loss: 2.6816 | Grad Norm: 1.4074 | Step Time: 0.68s\n",
      "[Epoch 3/3] Step 18440/20628 | Avg Loss: 2.7614 | Grad Norm: 1.1450 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18450/20628 | Avg Loss: 3.2931 | Grad Norm: 1.4347 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 18460/20628 | Avg Loss: 2.8389 | Grad Norm: 1.6728 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18470/20628 | Avg Loss: 2.4868 | Grad Norm: 1.1248 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18480/20628 | Avg Loss: 2.9773 | Grad Norm: 1.0790 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18490/20628 | Avg Loss: 3.0930 | Grad Norm: 1.5229 | Step Time: 0.68s\n",
      "[Epoch 3/3] Step 18500/20628 | Avg Loss: 3.2260 | Grad Norm: 1.3960 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18510/20628 | Avg Loss: 2.9274 | Grad Norm: 1.4593 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18520/20628 | Avg Loss: 3.2348 | Grad Norm: 1.2887 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18530/20628 | Avg Loss: 2.3413 | Grad Norm: 1.2041 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18540/20628 | Avg Loss: 2.8420 | Grad Norm: 1.5334 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18550/20628 | Avg Loss: 3.0711 | Grad Norm: 1.1983 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18560/20628 | Avg Loss: 3.1587 | Grad Norm: 1.2716 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18570/20628 | Avg Loss: 2.3708 | Grad Norm: 1.2728 | Step Time: 0.68s\n",
      "[Epoch 3/3] Step 18580/20628 | Avg Loss: 2.6630 | Grad Norm: 1.1934 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18590/20628 | Avg Loss: 2.7236 | Grad Norm: 1.0449 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18600/20628 | Avg Loss: 3.0930 | Grad Norm: 1.2137 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18610/20628 | Avg Loss: 2.7605 | Grad Norm: 1.6073 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18620/20628 | Avg Loss: 2.5536 | Grad Norm: 1.1685 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18630/20628 | Avg Loss: 2.9198 | Grad Norm: 1.3414 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 18640/20628 | Avg Loss: 2.9800 | Grad Norm: 1.5337 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18650/20628 | Avg Loss: 3.0323 | Grad Norm: 1.1873 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18660/20628 | Avg Loss: 3.0033 | Grad Norm: 1.2469 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 18670/20628 | Avg Loss: 2.8017 | Grad Norm: 1.5419 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 18680/20628 | Avg Loss: 2.8814 | Grad Norm: 1.2471 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18690/20628 | Avg Loss: 2.7403 | Grad Norm: 1.2616 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18700/20628 | Avg Loss: 2.5559 | Grad Norm: 1.2178 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18710/20628 | Avg Loss: 3.1606 | Grad Norm: 1.3917 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18720/20628 | Avg Loss: 2.1969 | Grad Norm: 1.0015 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 18730/20628 | Avg Loss: 2.7513 | Grad Norm: 1.4116 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18740/20628 | Avg Loss: 2.8711 | Grad Norm: 1.5606 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 18750/20628 | Avg Loss: 3.1063 | Grad Norm: 1.1691 | Step Time: 0.68s\n",
      "[Epoch 3/3] Step 18760/20628 | Avg Loss: 2.9482 | Grad Norm: 1.4721 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18770/20628 | Avg Loss: 3.1322 | Grad Norm: 1.2454 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18780/20628 | Avg Loss: 3.0640 | Grad Norm: 1.3418 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18790/20628 | Avg Loss: 3.1493 | Grad Norm: 1.2352 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18800/20628 | Avg Loss: 2.5185 | Grad Norm: 1.1676 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18810/20628 | Avg Loss: 2.7253 | Grad Norm: 1.0579 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18820/20628 | Avg Loss: 3.1437 | Grad Norm: 1.2402 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18830/20628 | Avg Loss: 2.8658 | Grad Norm: 1.4346 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18840/20628 | Avg Loss: 2.4948 | Grad Norm: 1.3292 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 18850/20628 | Avg Loss: 3.0620 | Grad Norm: 1.3840 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18860/20628 | Avg Loss: 2.7019 | Grad Norm: 1.5160 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18870/20628 | Avg Loss: 2.9879 | Grad Norm: 1.0568 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 18880/20628 | Avg Loss: 2.7596 | Grad Norm: 1.1113 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18890/20628 | Avg Loss: 3.0524 | Grad Norm: 1.4934 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18900/20628 | Avg Loss: 3.0980 | Grad Norm: 1.1677 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 18910/20628 | Avg Loss: 2.8494 | Grad Norm: 1.5316 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18920/20628 | Avg Loss: 2.8824 | Grad Norm: 1.3443 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18930/20628 | Avg Loss: 2.8455 | Grad Norm: 1.2691 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18940/20628 | Avg Loss: 3.0327 | Grad Norm: 1.0748 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18950/20628 | Avg Loss: 3.3947 | Grad Norm: 1.8227 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18960/20628 | Avg Loss: 2.4723 | Grad Norm: 1.2236 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18970/20628 | Avg Loss: 2.2089 | Grad Norm: 1.4232 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18980/20628 | Avg Loss: 3.0777 | Grad Norm: 1.0432 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 18990/20628 | Avg Loss: 3.0750 | Grad Norm: 1.5050 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19000/20628 | Avg Loss: 2.9879 | Grad Norm: 1.6088 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19010/20628 | Avg Loss: 3.1115 | Grad Norm: 1.4032 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19020/20628 | Avg Loss: 3.1514 | Grad Norm: 1.2599 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 19030/20628 | Avg Loss: 2.4604 | Grad Norm: 1.1104 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 19040/20628 | Avg Loss: 2.6651 | Grad Norm: 1.6242 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 19050/20628 | Avg Loss: 3.0609 | Grad Norm: 1.2779 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 19060/20628 | Avg Loss: 3.4868 | Grad Norm: 1.5392 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19070/20628 | Avg Loss: 3.0020 | Grad Norm: 1.3607 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19080/20628 | Avg Loss: 3.0757 | Grad Norm: 1.2348 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19090/20628 | Avg Loss: 2.1248 | Grad Norm: 1.0205 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19100/20628 | Avg Loss: 2.6907 | Grad Norm: 1.3437 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19110/20628 | Avg Loss: 2.7423 | Grad Norm: 1.1691 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 19120/20628 | Avg Loss: 3.1556 | Grad Norm: 1.3089 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19130/20628 | Avg Loss: 3.0375 | Grad Norm: 1.4045 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19140/20628 | Avg Loss: 2.8092 | Grad Norm: 1.6453 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19150/20628 | Avg Loss: 2.6447 | Grad Norm: 1.2511 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19160/20628 | Avg Loss: 2.7837 | Grad Norm: 1.8011 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 19170/20628 | Avg Loss: 2.2511 | Grad Norm: 1.5796 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 19180/20628 | Avg Loss: 2.9986 | Grad Norm: 1.8286 | Step Time: 0.68s\n",
      "[Epoch 3/3] Step 19190/20628 | Avg Loss: 3.2722 | Grad Norm: 1.4402 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19200/20628 | Avg Loss: 2.8009 | Grad Norm: 1.1511 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19210/20628 | Avg Loss: 2.8595 | Grad Norm: 1.1952 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19220/20628 | Avg Loss: 3.0316 | Grad Norm: 1.5166 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19230/20628 | Avg Loss: 2.7069 | Grad Norm: 1.3717 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 19240/20628 | Avg Loss: 2.5727 | Grad Norm: 1.4805 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19250/20628 | Avg Loss: 2.6358 | Grad Norm: 1.0779 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19260/20628 | Avg Loss: 2.4768 | Grad Norm: 1.2742 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19270/20628 | Avg Loss: 2.9522 | Grad Norm: 1.1582 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19280/20628 | Avg Loss: 3.1485 | Grad Norm: 1.8872 | Step Time: 0.68s\n",
      "[Epoch 3/3] Step 19290/20628 | Avg Loss: 2.9055 | Grad Norm: 1.5620 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19300/20628 | Avg Loss: 2.5050 | Grad Norm: 1.2449 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19310/20628 | Avg Loss: 2.8731 | Grad Norm: 1.0944 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19320/20628 | Avg Loss: 2.5891 | Grad Norm: 1.2583 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 19330/20628 | Avg Loss: 3.0455 | Grad Norm: 1.2445 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19340/20628 | Avg Loss: 2.5574 | Grad Norm: 1.6971 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19350/20628 | Avg Loss: 2.9792 | Grad Norm: 1.4322 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19360/20628 | Avg Loss: 2.9320 | Grad Norm: 1.4129 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19370/20628 | Avg Loss: 3.2092 | Grad Norm: 1.4439 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19380/20628 | Avg Loss: 3.0663 | Grad Norm: 1.2467 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19390/20628 | Avg Loss: 2.9501 | Grad Norm: 1.3915 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19400/20628 | Avg Loss: 2.9144 | Grad Norm: 1.3563 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19410/20628 | Avg Loss: 2.7992 | Grad Norm: 1.4637 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19420/20628 | Avg Loss: 2.5450 | Grad Norm: 1.2590 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19430/20628 | Avg Loss: 2.9002 | Grad Norm: 1.6687 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19440/20628 | Avg Loss: 2.9660 | Grad Norm: 1.7926 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 19450/20628 | Avg Loss: 2.9525 | Grad Norm: 1.4428 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19460/20628 | Avg Loss: 2.6503 | Grad Norm: 1.2107 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19470/20628 | Avg Loss: 2.2596 | Grad Norm: 1.7145 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19480/20628 | Avg Loss: 2.7753 | Grad Norm: 1.6618 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19490/20628 | Avg Loss: 2.7317 | Grad Norm: 1.1776 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19500/20628 | Avg Loss: 2.6002 | Grad Norm: 1.4626 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19510/20628 | Avg Loss: 2.7223 | Grad Norm: 1.1550 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19520/20628 | Avg Loss: 3.2641 | Grad Norm: 1.2243 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19530/20628 | Avg Loss: 2.6440 | Grad Norm: 1.2084 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19540/20628 | Avg Loss: 2.7612 | Grad Norm: 1.6212 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19550/20628 | Avg Loss: 2.9230 | Grad Norm: 1.0803 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19560/20628 | Avg Loss: 2.5474 | Grad Norm: 1.0928 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19570/20628 | Avg Loss: 2.8143 | Grad Norm: 1.1449 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19580/20628 | Avg Loss: 3.1538 | Grad Norm: 1.2265 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19590/20628 | Avg Loss: 2.7012 | Grad Norm: 1.1748 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19600/20628 | Avg Loss: 2.8107 | Grad Norm: 1.2652 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19610/20628 | Avg Loss: 2.8284 | Grad Norm: 1.4235 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19620/20628 | Avg Loss: 2.3882 | Grad Norm: 1.1267 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 19630/20628 | Avg Loss: 3.1798 | Grad Norm: 1.4980 | Step Time: 0.68s\n",
      "[Epoch 3/3] Step 19640/20628 | Avg Loss: 2.7891 | Grad Norm: 1.3450 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19650/20628 | Avg Loss: 3.0853 | Grad Norm: 1.7634 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19660/20628 | Avg Loss: 2.7043 | Grad Norm: 1.1291 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19670/20628 | Avg Loss: 3.3338 | Grad Norm: 1.2289 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19680/20628 | Avg Loss: 3.2288 | Grad Norm: 1.1883 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19690/20628 | Avg Loss: 2.7815 | Grad Norm: 1.2977 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19700/20628 | Avg Loss: 2.8653 | Grad Norm: 1.4451 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19710/20628 | Avg Loss: 2.7719 | Grad Norm: 1.5831 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 19720/20628 | Avg Loss: 3.0104 | Grad Norm: 1.2471 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19730/20628 | Avg Loss: 2.7898 | Grad Norm: 1.6267 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 19740/20628 | Avg Loss: 2.7606 | Grad Norm: 1.3244 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19750/20628 | Avg Loss: 2.3570 | Grad Norm: 1.3888 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19760/20628 | Avg Loss: 2.6263 | Grad Norm: 1.1569 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19770/20628 | Avg Loss: 2.7292 | Grad Norm: 1.2368 | Step Time: 0.69s\n",
      "[Epoch 3/3] Step 19780/20628 | Avg Loss: 2.9913 | Grad Norm: 1.1824 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19790/20628 | Avg Loss: 2.8350 | Grad Norm: 1.3605 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19800/20628 | Avg Loss: 2.9329 | Grad Norm: 1.2579 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19810/20628 | Avg Loss: 3.0379 | Grad Norm: 5.0391 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19820/20628 | Avg Loss: 2.6467 | Grad Norm: 1.1604 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19830/20628 | Avg Loss: 2.9648 | Grad Norm: 1.3525 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19840/20628 | Avg Loss: 2.7993 | Grad Norm: 1.4227 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19850/20628 | Avg Loss: 2.6374 | Grad Norm: 1.0742 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19860/20628 | Avg Loss: 2.5793 | Grad Norm: 1.2145 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19870/20628 | Avg Loss: 2.8135 | Grad Norm: 1.0687 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19880/20628 | Avg Loss: 2.7158 | Grad Norm: 1.3584 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19890/20628 | Avg Loss: 2.3773 | Grad Norm: 1.8502 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 19900/20628 | Avg Loss: 2.6510 | Grad Norm: 1.0430 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19910/20628 | Avg Loss: 2.6076 | Grad Norm: 1.0577 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19920/20628 | Avg Loss: 2.3931 | Grad Norm: 1.7739 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19930/20628 | Avg Loss: 2.8583 | Grad Norm: 1.6669 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19940/20628 | Avg Loss: 2.8459 | Grad Norm: 1.1421 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19950/20628 | Avg Loss: 3.2985 | Grad Norm: 1.4694 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19960/20628 | Avg Loss: 2.5746 | Grad Norm: 1.2340 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19970/20628 | Avg Loss: 3.0630 | Grad Norm: 1.2969 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19980/20628 | Avg Loss: 3.0901 | Grad Norm: 1.2769 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 19990/20628 | Avg Loss: 3.2071 | Grad Norm: 1.3267 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20000/20628 | Avg Loss: 2.4467 | Grad Norm: 1.5722 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20010/20628 | Avg Loss: 3.1216 | Grad Norm: 1.2234 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 20020/20628 | Avg Loss: 2.5635 | Grad Norm: 1.2877 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20030/20628 | Avg Loss: 3.1080 | Grad Norm: 1.9895 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 20040/20628 | Avg Loss: 2.3987 | Grad Norm: 1.2740 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 20050/20628 | Avg Loss: 2.6972 | Grad Norm: 1.3687 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20060/20628 | Avg Loss: 2.2659 | Grad Norm: 1.3571 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 20070/20628 | Avg Loss: 2.8441 | Grad Norm: 1.3117 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 20080/20628 | Avg Loss: 2.8581 | Grad Norm: 1.4102 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 20090/20628 | Avg Loss: 2.5338 | Grad Norm: 1.4418 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20100/20628 | Avg Loss: 2.0533 | Grad Norm: 0.9617 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20110/20628 | Avg Loss: 2.6172 | Grad Norm: 1.5302 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 20120/20628 | Avg Loss: 2.6320 | Grad Norm: 1.1197 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 20130/20628 | Avg Loss: 2.9187 | Grad Norm: 1.2952 | Step Time: 0.69s\n",
      "[Epoch 3/3] Step 20140/20628 | Avg Loss: 3.3598 | Grad Norm: 1.4227 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 20150/20628 | Avg Loss: 2.7812 | Grad Norm: 1.4372 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 20160/20628 | Avg Loss: 3.0694 | Grad Norm: 1.0081 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20170/20628 | Avg Loss: 3.1842 | Grad Norm: 1.1765 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 20180/20628 | Avg Loss: 2.5509 | Grad Norm: 1.3297 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20190/20628 | Avg Loss: 3.2278 | Grad Norm: 1.1749 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 20200/20628 | Avg Loss: 2.4641 | Grad Norm: 1.0984 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20210/20628 | Avg Loss: 3.0070 | Grad Norm: 1.2660 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20220/20628 | Avg Loss: 2.8413 | Grad Norm: 1.2537 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20230/20628 | Avg Loss: 3.2917 | Grad Norm: 1.3067 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20240/20628 | Avg Loss: 3.2034 | Grad Norm: 1.5199 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20250/20628 | Avg Loss: 3.1333 | Grad Norm: 1.3326 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20260/20628 | Avg Loss: 2.3200 | Grad Norm: 1.2318 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20270/20628 | Avg Loss: 2.5476 | Grad Norm: 1.3225 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20280/20628 | Avg Loss: 2.9827 | Grad Norm: 1.0953 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20290/20628 | Avg Loss: 2.9468 | Grad Norm: 1.3863 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20300/20628 | Avg Loss: 3.0265 | Grad Norm: 1.4005 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20310/20628 | Avg Loss: 2.9551 | Grad Norm: 1.3691 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20320/20628 | Avg Loss: 2.9919 | Grad Norm: 1.3019 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20330/20628 | Avg Loss: 3.0746 | Grad Norm: 1.3119 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20340/20628 | Avg Loss: 3.1128 | Grad Norm: 1.2259 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20350/20628 | Avg Loss: 2.7593 | Grad Norm: 1.2631 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 20360/20628 | Avg Loss: 3.2842 | Grad Norm: 1.2836 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20370/20628 | Avg Loss: 2.6617 | Grad Norm: 1.3643 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20380/20628 | Avg Loss: 2.7767 | Grad Norm: 1.0814 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20390/20628 | Avg Loss: 3.1116 | Grad Norm: 1.6970 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20400/20628 | Avg Loss: 2.8777 | Grad Norm: 1.5103 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20410/20628 | Avg Loss: 3.0195 | Grad Norm: 1.1851 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20420/20628 | Avg Loss: 2.8664 | Grad Norm: 1.3677 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20430/20628 | Avg Loss: 2.8758 | Grad Norm: 1.5557 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20440/20628 | Avg Loss: 3.2279 | Grad Norm: 1.6123 | Step Time: 0.68s\n",
      "[Epoch 3/3] Step 20450/20628 | Avg Loss: 2.9422 | Grad Norm: 1.5440 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20460/20628 | Avg Loss: 2.1631 | Grad Norm: 1.5702 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 20470/20628 | Avg Loss: 2.6900 | Grad Norm: 1.4340 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20480/20628 | Avg Loss: 3.0727 | Grad Norm: 1.3605 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20490/20628 | Avg Loss: 2.7291 | Grad Norm: 1.4804 | Step Time: 0.68s\n",
      "[Epoch 3/3] Step 20500/20628 | Avg Loss: 3.4499 | Grad Norm: 1.5930 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 20510/20628 | Avg Loss: 2.4409 | Grad Norm: 1.1926 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20520/20628 | Avg Loss: 3.3408 | Grad Norm: 1.3299 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20530/20628 | Avg Loss: 2.8774 | Grad Norm: 1.5546 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20540/20628 | Avg Loss: 2.5689 | Grad Norm: 1.0233 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 20550/20628 | Avg Loss: 2.5129 | Grad Norm: 1.5238 | Step Time: 0.67s\n",
      "[Epoch 3/3] Step 20560/20628 | Avg Loss: 2.7174 | Grad Norm: 1.4134 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20570/20628 | Avg Loss: 3.1495 | Grad Norm: 1.1339 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20580/20628 | Avg Loss: 3.3389 | Grad Norm: 1.3761 | Step Time: 0.68s\n",
      "[Epoch 3/3] Step 20590/20628 | Avg Loss: 2.5984 | Grad Norm: 1.2091 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20600/20628 | Avg Loss: 2.7044 | Grad Norm: 1.5620 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20610/20628 | Avg Loss: 2.3820 | Grad Norm: 1.4345 | Step Time: 0.66s\n",
      "[Epoch 3/3] Step 20620/20628 | Avg Loss: 2.7445 | Grad Norm: 1.5036 | Step Time: 0.66s\n",
      "\n",
      "âœ… Epoch 3 å®Œæˆ | Avg Loss: 2.8372 | Epoch Time: 01:17:02 | Total Time: 03:50:45\n",
      "\n",
      "ğŸ“ ç”ŸæˆéªŒè¯ï¼ˆéšæœºæ ·æœ¬ï¼‰:\n",
      "ä¸»é¢˜: Write a poem using the theme: Living,Coming of Age...\n",
      "ç”Ÿæˆè¯—æ­Œ:\n",
      "Come thou and feed thy flock with love,\n",
      "    With understanding lift them up;\n",
      "The lambs shall feed among the rushes,\n",
      "    The goats on mountain's pleasant grass.\n",
      "The young ones play beside their dams,\n",
      "    Their bellies swelled with sweet milk full;\n",
      "They are not troubled by the wolf;\n",
      "    And though they sleep 'twill never kill.\n",
      "I have no need to look about,\n",
      "    Nor fear the roaring lion there;\n",
      "There is enough for all that dwell:\n",
      "    Here is the field of God's own care.\n",
      "Here doth my soul her food find out,\n",
      "    Not here; nor here will she remain,\n",
      "Till it can sing in heaven again.\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ“¦ ä¿å­˜è®­ç»ƒå®Œæˆæ¨¡å‹...\n",
      "âœ… æ¨¡å‹å·²ä¿å­˜è‡³: /root/autodl-tmp/Pro/qwen2.5-sft-full\n",
      "\n",
      "ğŸ¯ æœ€ç»ˆç”ŸæˆéªŒè¯ï¼ˆéšæœº3æ¡æ•°æ®ï¼‰:\n",
      "\n",
      "ğŸ“ ç”ŸæˆéªŒè¯ï¼ˆéšæœºæ ·æœ¬ï¼‰:\n",
      "ä¸»é¢˜: Write a poem using the theme: Living,Sorrow & Grie...\n",
      "ç”Ÿæˆè¯—æ­Œ:\n",
      "I was walking back to town last nightAnd there I saw the old man on the streetWearing his coat and hat so tightAround his head that he could hardly seeThe way home from the pub.I had seen him there before,But never like thisâ€”so huddled upIn his own misery.He couldnâ€™t even speakHe only pointed to the skyAnd shook his fist at the starsThere where they were blinkingSo brightly tonight.In our time itâ€™s always beenA matter of who can make you suffermost.It wasnâ€™t long agoWe used to go out to seaAnd watch them come out with the dead menâ€™s boatsAnd hear the cries in the darkness.They didnâ€™t need to be toldTo take away what they knew.The world had already given them enough.Nothingâ€™s changed except the place we are nowin.Wait till the sun comes up tomorrowMorning will be better then.\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ“ ç”ŸæˆéªŒè¯ï¼ˆéšæœºæ ·æœ¬ï¼‰:\n",
      "ä¸»é¢˜: Write a poem using the theme: Living,Coming of Age...\n",
      "ç”Ÿæˆè¯—æ­Œ:\n",
      "i had not yet been baptized.\n",
      "in church i was an orphan and no longer young enough to be adopted,\n",
      "so i sat next to my father who looked like a man you might trust with your money or life but only because he was black and lived among his people.\n",
      " \n",
      "the preacher said i needed a job, so my daddy took me downtown at 3 in the morning when there were more lights than cars on the streets.\n",
      "â€œSee that one over there?,â€ he asked. â€œThatâ€™s where weâ€™re going.â€\n",
      " \n",
      "we went inside and walked up three flights of stairs. it smelled like sweat and beer.\n",
      "at the end of the hall was a small office filled with papers and two desks. they told us to sit down.\n",
      " \n",
      "i thought for sure this was some kind of trick. it was too easy.\n",
      "i felt my dad shift as if his body was getting ready to move. i put my hand out.\n",
      "â€œHere is the application. We need to know what your work experience\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ“ ç”ŸæˆéªŒè¯ï¼ˆéšæœºæ ·æœ¬ï¼‰:\n",
      "ä¸»é¢˜: Write a poem using the theme: Relationships,Pets, ...\n",
      "ç”Ÿæˆè¯—æ­Œ:\n",
      "We're not even talking to each other.\n",
      "Not even thinking of each other.\n",
      "But we still think we love each other,\n",
      "And can't live without one another,\n",
      "Like a cat and mouse!\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ‰ å…¨é‡è®­ç»ƒå®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import PreTrainedTokenizerFast, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import nltk\n",
    "\n",
    "# -------------------------\n",
    "# 0. å…¨é‡è®­ç»ƒé…ç½®ï¼ˆå¹³è¡¡æ•ˆç‡ä¸ç¨³å®šæ€§ï¼‰\n",
    "# -------------------------\n",
    "# ç¼“å­˜é…ç½®\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/root/autodl-tmp/hf_cache\"\n",
    "os.environ[\"HF_HOME\"] = \"/root/autodl-tmp/hf_cache\"\n",
    "torch.hub.set_dir(\"/root/autodl-tmp/torch_cache\")\n",
    "os.environ[\"TORCH_HOME\"] = \"/root/autodl-tmp/torch_cache\"\n",
    "os.makedirs(\"/root/autodl-tmp/offload\", exist_ok=True)\n",
    "\n",
    "# å…¨é‡è®­ç»ƒé…ç½®\n",
    "CONFIG = {\n",
    "    \"model_name_or_path\": \"/root/autodl-tmp/qwen2.5-7b/qwen2.5-7b\",\n",
    "    \"data_path\": \"/root/autodl-tmp/Pro/poetry_sft.jsonl\",\n",
    "    \"output_dir\": \"/root/autodl-tmp/Pro/qwen2.5-sft-full\",\n",
    "    \"num_train_epochs\": 3,  # å…¨é‡æ•°æ®è®­ç»ƒ3è½®\n",
    "    \"batch_size\": 2,  # æŒ‰éœ€è°ƒæ•´ï¼ˆæ ¹æ®GPUæ˜¾å­˜ï¼‰\n",
    "    \"gradient_accumulation_steps\": 2,  # æ¢¯åº¦ç´¯ç§¯ï¼Œç­‰æ•ˆå¢å¤§æ‰¹æ¬¡\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"max_seq_length\": 256,  # é€‚é…æ›´é•¿çš„è¯—æ­Œæ–‡æœ¬\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"offload_folder\": \"/root/autodl-tmp/offload\",\n",
    "    # ç”Ÿæˆä¸“ç”¨é…ç½®\n",
    "    \"gen_max_new_tokens\": 200,\n",
    "    \"gen_temperature\": 0.85,\n",
    "    \"gen_top_k\": 30,\n",
    "    \"gen_top_p\": 0.92,\n",
    "    \"gen_repetition_penalty\": 1.15,\n",
    "    # è®­ç»ƒç›‘æ§\n",
    "    \"print_every_steps\": 10,  # æ¯10æ­¥æ‰“å°ä¸€æ¬¡Loss\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# 1. åŠ è½½Tokenizerï¼ˆå…¼å®¹å…¨é‡æ•°æ®ï¼‰\n",
    "# -------------------------\n",
    "tokenizer_file = os.path.join(CONFIG[\"model_name_or_path\"], \"tokenizer.json\")\n",
    "assert os.path.exists(tokenizer_file), f\"Tokenizeræ–‡ä»¶ä¸å­˜åœ¨: {tokenizer_file}\"\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_file)\n",
    "# Qwen2.5ä¸“ç”¨ç‰¹æ®Štokené…ç½®\n",
    "tokenizer.eos_token = \"<|endoftext|>\"\n",
    "tokenizer.bos_token = \"<|startoftext|>\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({\n",
    "    \"additional_special_tokens\": [\"<|im_start|>\", \"<|im_end|>\"]\n",
    "})\n",
    "print(\"âœ… TokenizeråŠ è½½å®Œæˆ\")\n",
    "\n",
    "# -------------------------\n",
    "# 2. å…¨é‡æ•°æ®Datasetï¼ˆåŠ è½½æ‰€æœ‰æœ‰æ•ˆè¯—æ­Œï¼‰\n",
    "# -------------------------\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, jsonl_path, tokenizer, max_len=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        # åŠ è½½æ‰€æœ‰æœ‰æ•ˆæ•°æ®\n",
    "        self.data = self._load_all_valid_samples(jsonl_path)\n",
    "        assert len(self.data) > 0, f\"æ•°æ®è·¯å¾„ {jsonl_path} ä¸‹æ— æœ‰æ•ˆæ•°æ®ï¼\"\n",
    "        print(f\"âœ… åŠ è½½æœ‰æ•ˆè¯—æ­Œæ•°æ®: {len(self.data)} æ¡\")\n",
    "\n",
    "    def _load_all_valid_samples(self, jsonl_path):\n",
    "        \"\"\"åŠ è½½æ‰€æœ‰æœ‰æ•ˆè¯—æ­Œæ•°æ®\"\"\"\n",
    "        valid_data = []\n",
    "        if not os.path.exists(jsonl_path):\n",
    "            raise FileNotFoundError(f\"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {jsonl_path}\")\n",
    "        \n",
    "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line_idx, line in enumerate(lines):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    sample = json.loads(line)\n",
    "                    # æ ¡éªŒmessagesç»“æ„å’Œå†…å®¹\n",
    "                    if len(sample.get(\"messages\", [])) < 2:\n",
    "                        print(f\"âš ï¸  è·³è¿‡ç¬¬{line_idx+1}è¡Œï¼šmessagesæ ¼å¼é”™è¯¯\")\n",
    "                        continue\n",
    "                    user_text = sample[\"messages\"][0][\"content\"].strip()\n",
    "                    assistant_text = sample[\"messages\"][1][\"content\"].strip()\n",
    "                    if not user_text or not assistant_text:\n",
    "                        print(f\"âš ï¸  è·³è¿‡ç¬¬{line_idx+1}è¡Œï¼šå†…å®¹ä¸ºç©º\")\n",
    "                        continue\n",
    "                    valid_data.append({\n",
    "                        \"user\": user_text,\n",
    "                        \"assistant\": assistant_text\n",
    "                    })\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"âš ï¸  è·³è¿‡ç¬¬{line_idx+1}è¡Œï¼šJSONè§£æå¤±è´¥\")\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸  è·³è¿‡ç¬¬{line_idx+1}è¡Œï¼š{str(e)}\")\n",
    "        return valid_data\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"è¿”å›å…¨é‡æ•°æ®é•¿åº¦\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"æŒ‰ç´¢å¼•åŠ è½½å•æ¡æ•°æ®\"\"\"\n",
    "        sample = self.data[idx]\n",
    "        # æ„å»ºQwen2.5æ ‡å‡†å¯¹è¯prompt\n",
    "        prompt = f\"<|im_start|>user\\n{sample['user']}<|im_end|>\\n<|im_start|>assistant\\n{sample['assistant']}<|im_end|>\"\n",
    "        \n",
    "        # ç¼–ç ï¼ˆé€‚é…é•¿åºåˆ—ï¼‰\n",
    "        encoding = self.tokenizer(\n",
    "            prompt,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            padding_side=\"right\",\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = encoding.input_ids.squeeze(0).long()\n",
    "        attention_mask = encoding.attention_mask.squeeze(0).long()\n",
    "\n",
    "        # æ„å»ºæ ‡ç­¾ï¼ˆä»…è®¡ç®—assistantéƒ¨åˆ†Lossï¼‰\n",
    "        user_prompt = f\"<|im_start|>user\\n{sample['user']}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "        user_len = len(self.tokenizer.encode(user_prompt, truncation=True, max_length=self.max_len))\n",
    "        labels = torch.full_like(input_ids, -100)  # useréƒ¨åˆ†å¿½ç•¥\n",
    "        # æ ‡æ³¨assistantå…¨éƒ¨æœ‰æ•ˆtokenï¼ˆé€‚é…é•¿åºåˆ—ï¼‰\n",
    "        labels[user_len:min(user_len + self.max_len - user_len, self.max_len)] = input_ids[user_len:min(user_len + self.max_len - user_len, self.max_len)]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"user_text\": sample[\"user\"],  # ä¿ç•™ç”¨æˆ·è¾“å…¥ï¼Œç”¨äºç”ŸæˆéªŒè¯\n",
    "            \"assistant_text\": sample[\"assistant\"]  # ä¿ç•™å‚è€ƒç­”æ¡ˆ\n",
    "        }\n",
    "\n",
    "# åŠ è½½å…¨é‡æ•°æ®é›†\n",
    "dataset = PoetryDataset(CONFIG[\"data_path\"], tokenizer, CONFIG[\"max_seq_length\"])\n",
    "# å…¨é‡æ•°æ®DataLoaderï¼ˆæ‰“ä¹±+æ‰¹æ¬¡åŠ è½½ï¼‰\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    shuffle=True,  # æ‰“ä¹±æ•°æ®ï¼Œæå‡æ³›åŒ–æ€§\n",
    "    num_workers=0,  # ç¦ç”¨å¤šè¿›ç¨‹ï¼Œé¿å…æ•°æ®åŠ è½½å¼‚å¸¸\n",
    "    drop_last=True  # ä¸¢å¼ƒæœ€åä¸å®Œæ•´æ‰¹æ¬¡\n",
    ")\n",
    "total_steps = len(train_loader) * CONFIG[\"num_train_epochs\"]\n",
    "print(f\"ğŸ“Œ å…¨é‡è®­ç»ƒé…ç½®ï¼š\")\n",
    "print(f\"- æ€»æ•°æ®é‡ï¼š{len(dataset)} æ¡\")\n",
    "print(f\"- æ¯Epochæ­¥æ•°ï¼š{len(train_loader)}\")\n",
    "print(f\"- æ€»è®­ç»ƒæ­¥æ•°ï¼š{total_steps}\")\n",
    "print(f\"- æœ‰æ•ˆæ‰¹æ¬¡å¤§å°ï¼š{CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']}\")\n",
    "\n",
    "# -------------------------\n",
    "# 3. åŠ è½½æ¨¡å‹ï¼ˆFP32+LoRAï¼Œå…¨é‡è®­ç»ƒç¨³å®šï¼‰\n",
    "# -------------------------\n",
    "print(\"\\nğŸ“Œ åŠ è½½Qwen2.5-7Bæ¨¡å‹...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"model_name_or_path\"],\n",
    "    torch_dtype=torch.float32,  # ä¿æŒFP32ç¨³å®šï¼Œå…¨é‡è®­ç»ƒä¸æº¢å‡º\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=CONFIG[\"offload_folder\"],\n",
    "    attn_implementation=\"eager\",  # ç¦ç”¨flash attentionï¼Œæå‡ç¨³å®šæ€§\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "model.train()\n",
    "\n",
    "# LoRAé…ç½®ï¼ˆé€‚åº¦å¢å¤§rå€¼ï¼Œæå‡å…¨é‡æ•°æ®æ‹Ÿåˆèƒ½åŠ›ï¼‰\n",
    "lora_cfg = LoraConfig(\n",
    "    r=4,  # æ¯”å•æ ·æœ¬è°ƒè¯•å¢å¤§ï¼Œé€‚é…å…¨é‡æ•°æ®\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.01,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # ä»…è®­ç»ƒå…³é”®æŠ•å½±å±‚\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    bias=\"none\"\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# -------------------------\n",
    "# 4. ä¼˜åŒ–å™¨é…ç½®ï¼ˆé€‚é…å…¨é‡è®­ç»ƒï¼‰\n",
    "# -------------------------\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG[\"learning_rate\"],\n",
    "    eps=1e-8,\n",
    "    weight_decay=0.001,\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "# æŸå¤±å‡½æ•°ï¼ˆå¿½ç•¥paddingéƒ¨åˆ†ï¼‰\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-100, reduction=\"mean\")\n",
    "\n",
    "# -------------------------\n",
    "# 5. è®­ç»ƒè¾…åŠ©å‡½æ•°\n",
    "# -------------------------\n",
    "def format_time(seconds):\n",
    "    \"\"\"æ ¼å¼åŒ–æ—¶é—´è¾“å‡º\"\"\"\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = int(seconds % 60)\n",
    "    return f\"{hours:02d}:{minutes:02d}:{secs:02d}\"\n",
    "\n",
    "def generate_validation_sample(model, tokenizer, dataset, config):\n",
    "    \"\"\"éšæœºé€‰1æ¡æ•°æ®éªŒè¯ç”Ÿæˆæ•ˆæœ\"\"\"\n",
    "    model.eval()\n",
    "    # éšæœºé€‰1æ¡æ•°æ®\n",
    "    sample_idx = random.randint(0, len(dataset)-1)\n",
    "    sample = dataset.data[sample_idx]\n",
    "    prompt = f\"<|im_start|>user\\n{sample['user']}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(config[\"device\"])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=config[\"gen_max_new_tokens\"],\n",
    "            do_sample=True,\n",
    "            temperature=config[\"gen_temperature\"],\n",
    "            top_k=config[\"gen_top_k\"],\n",
    "            top_p=config[\"gen_top_p\"],\n",
    "            repetition_penalty=config[\"gen_repetition_penalty\"],\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            early_stopping=True,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "    \n",
    "    # è§£ç å¹¶æ¸…ç†ç”Ÿæˆç»“æœ\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    if \"<|im_start|>assistant\\n\" in generated_text:\n",
    "        generated_poem = generated_text.split(\"<|im_start|>assistant\\n\")[1]\n",
    "        generated_poem = generated_poem.replace(\"<|endoftext|>\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
    "    else:\n",
    "        generated_poem = generated_text.strip()\n",
    "    \n",
    "    # æ‰“å°éªŒè¯ç»“æœ\n",
    "    print(\"\\nğŸ“ ç”ŸæˆéªŒè¯ï¼ˆéšæœºæ ·æœ¬ï¼‰:\")\n",
    "    print(f\"ä¸»é¢˜: {sample['user'][:50]}...\")\n",
    "    print(\"ç”Ÿæˆè¯—æ­Œ:\")\n",
    "    print(generated_poem)\n",
    "    print(\"-\" * 50)\n",
    "    model.train()  # å›åˆ°è®­ç»ƒæ¨¡å¼\n",
    "\n",
    "# -------------------------\n",
    "# 6. å…¨é‡è®­ç»ƒä¸»å¾ªç¯\n",
    "# -------------------------\n",
    "print(\"\\nğŸš€ å¼€å§‹å…¨é‡æ•°æ®è®­ç»ƒ...\")\n",
    "global_step = 0\n",
    "total_train_time = 0.0\n",
    "avg_loss_list = []\n",
    "\n",
    "for epoch in range(CONFIG[\"num_train_epochs\"]):\n",
    "    epoch_start = time.time()\n",
    "    epoch_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        step_start = time.time()\n",
    "        global_step += 1\n",
    "\n",
    "        # åŠ è½½æ‰¹æ¬¡æ•°æ®åˆ°è®¾å¤‡\n",
    "        input_ids = batch[\"input_ids\"].to(CONFIG[\"device\"], non_blocking=True)\n",
    "        labels = batch[\"labels\"].to(CONFIG[\"device\"], non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(CONFIG[\"device\"], non_blocking=True)\n",
    "\n",
    "        # å‰å‘ä¼ æ’­ï¼ˆæ¢¯åº¦æ£€æµ‹ï¼‰\n",
    "        with torch.autograd.detect_anomaly():\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            # æ¢¯åº¦ç´¯ç§¯ï¼šLossé™¤ä»¥ç´¯ç§¯æ­¥æ•°\n",
    "            loss = loss / CONFIG[\"gradient_accumulation_steps\"]\n",
    "\n",
    "        # åå‘ä¼ æ’­\n",
    "        loss.backward()\n",
    "\n",
    "        # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢çˆ†ç‚¸ï¼‰\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.5)\n",
    "\n",
    "        # ç´¯ç§¯æ­¥æ•°è¾¾æ ‡åæ›´æ–°å‚æ•°\n",
    "        if (step + 1) % CONFIG[\"gradient_accumulation_steps\"] == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # ç´¯è®¡Loss\n",
    "        epoch_loss += loss.item() * CONFIG[\"gradient_accumulation_steps\"]\n",
    "        avg_loss_list.append(loss.item() * CONFIG[\"gradient_accumulation_steps\"])\n",
    "\n",
    "        # æ‰“å°è®­ç»ƒè¿›åº¦\n",
    "        if global_step % CONFIG[\"print_every_steps\"] == 0:\n",
    "            step_time = time.time() - step_start\n",
    "            avg_loss = sum(avg_loss_list[-CONFIG[\"print_every_steps\"]:]) / CONFIG[\"print_every_steps\"]\n",
    "            print(f\"[Epoch {epoch+1}/{CONFIG['num_train_epochs']}] \"\n",
    "                  f\"Step {global_step}/{total_steps} | \"\n",
    "                  f\"Avg Loss: {avg_loss:.4f} | \"\n",
    "                  f\"Grad Norm: {grad_norm.item():.4f} | \"\n",
    "                  f\"Step Time: {step_time:.2f}s\")\n",
    "\n",
    "    # Epochç»“æŸå¤„ç†\n",
    "    epoch_time = format_time(time.time() - epoch_start)\n",
    "    total_train_time += time.time() - epoch_start\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"\\nâœ… Epoch {epoch+1} å®Œæˆ | \"\n",
    "          f\"Avg Loss: {avg_epoch_loss:.4f} | \"\n",
    "          f\"Epoch Time: {epoch_time} | \"\n",
    "          f\"Total Time: {format_time(total_train_time)}\")\n",
    "\n",
    "    # æ¯ä¸ªEpochç»“æŸåéªŒè¯ç”Ÿæˆæ•ˆæœ\n",
    "    generate_validation_sample(model, tokenizer, dataset, CONFIG)\n",
    "\n",
    "# -------------------------\n",
    "# 7. ä¿å­˜æœ€ç»ˆæ¨¡å‹\n",
    "# -------------------------\n",
    "print(\"\\nğŸ“¦ ä¿å­˜è®­ç»ƒå®Œæˆæ¨¡å‹...\")\n",
    "model.save_pretrained(CONFIG[\"output_dir\"])\n",
    "tokenizer.save_pretrained(CONFIG[\"output_dir\"])\n",
    "print(f\"âœ… æ¨¡å‹å·²ä¿å­˜è‡³: {CONFIG['output_dir']}\")\n",
    "\n",
    "# -------------------------\n",
    "# 8. æœ€ç»ˆç”ŸæˆéªŒè¯ï¼ˆéšæœºé€‰3æ¡æ•°æ®ï¼‰\n",
    "# -------------------------\n",
    "print(\"\\nğŸ¯ æœ€ç»ˆç”ŸæˆéªŒè¯ï¼ˆéšæœº3æ¡æ•°æ®ï¼‰:\")\n",
    "for i in range(3):\n",
    "    generate_validation_sample(model, tokenizer, dataset, CONFIG)\n",
    "\n",
    "print(\"\\nğŸ‰ å…¨é‡è®­ç»ƒå®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a452300-d8f1-48e9-877f-5dceed53b749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤ è¯·è¾“å…¥ä½ çš„ç”Ÿæˆ promptï¼ˆå¯ä½¿ç”¨é£æ ¼æ ‡ç­¾æˆ–è‹±æ–‡å…³é”®è¯ï¼‰:\n",
      "ğŸ“Œ ç¤ºä¾‹ï¼š <|style:byron|> Write a poem about moonlight and sorrow.\n",
      "è¾“å…¥åæŒ‰å›è½¦ï¼š\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your prompt:  write a poem in the style of virginia wolf, with keyword: willow, wind, fluffy, charmed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ¨ Generated Poem:\n",
      "\n",
      "The fluffiness of willows and the fluttering of their leaves, The gauze curtains that wave without any movement from within, The droning of bees, The whirring of motor-buses, The roar of trafficâ€”these are all things I want to do away with. If I could write my book as simply and as naturally as one breathes, I would forget about it for years, then suddenly feel some strange emotion rising up within me which would make everything else disappear, would leave me alone on an eminence where I should see only a few tall trees swaying gently in the wind, perhaps two or three white clouds floating far out over the sea. Then if I were still possessed by this feeling, I might begin to think again of my story. As it is, I can never remember anything when thereâ€™s a lot going on; but, like most people, I have to get used to living among other people before they start getting in the way. And\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# -------------------------\n",
    "# 1. åŠ è½½ä½ å¾®è°ƒå¥½çš„æ¨¡å‹\n",
    "# -------------------------\n",
    "MODEL_PATH = \"/root/autodl-tmp/Pro/qwen2.5-sft-full\"   # â† æ”¹æˆä½ çš„è·¯å¾„\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# -------------------------\n",
    "# 2. ç”Ÿæˆå‡½æ•°\n",
    "# -------------------------\n",
    "def generate_by_prompt(user_prompt: str, max_new_tokens=200):\n",
    "    \"\"\"\n",
    "    user_prompt: ä½ è‡ªå·±éšä¾¿è¾“å…¥çš„ prompt\n",
    "    ï¼ˆå¯ä»¥åŒ…å« <|style:xxx|>, ä¹Ÿå¯ä»¥çº¯è‹±æ–‡æè¿°ï¼‰\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"<|im_start|>user\\n\"\n",
    "        f\"{user_prompt}\\n\"\n",
    "        f\"<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n\"\n",
    "    )\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.9,\n",
    "            top_p=0.92,\n",
    "            top_k=40,\n",
    "            repetition_penalty=1.15,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    if \"<|im_start|>assistant\" in text:\n",
    "        poem = text.split(\"<|im_start|>assistant\")[-1]\n",
    "    else:\n",
    "        poem = text\n",
    "\n",
    "    poem = poem.replace(\"<|endoftext|>\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
    "    return poem\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3. ç”¨æˆ·è¾“å…¥\n",
    "# -------------------------\n",
    "print(\"ğŸ¤ è¯·è¾“å…¥ä½ çš„ç”Ÿæˆ promptï¼ˆå¯ä½¿ç”¨é£æ ¼æ ‡ç­¾æˆ–è‹±æ–‡å…³é”®è¯ï¼‰:\")\n",
    "print(\"ğŸ“Œ ç¤ºä¾‹ï¼š <|style:byron|> Write a poem about moonlight and sorrow.\")\n",
    "print(\"è¾“å…¥åæŒ‰å›è½¦ï¼š\\n\")\n",
    "\n",
    "user_input = input(\"Your prompt: \")\n",
    "\n",
    "# -------------------------\n",
    "# 4. ç”Ÿæˆ\n",
    "# -------------------------\n",
    "print(\"\\nâœ¨ Generated Poem:\\n\")\n",
    "result = generate_by_prompt(user_input)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d89ba79-3b97-4b78-ad1a-d66668c11e01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/qwen_text/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ç”Ÿæˆæ¨¡å‹TokenizeråŠ è½½å®Œæˆ\n",
      "\n",
      "ğŸ“Œ åŠ è½½AIæ£€æµ‹å™¨æ¨¡å‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /root/autodl-tmp/Pro/models--roberta-base-openai-detector/snapshots/6cba99c003b711c7fe94f8a3aa2be35a792cb6fa were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… AIæ£€æµ‹å™¨åŠ è½½å®Œæˆå¹¶å†»ç»“\n",
      "âš ï¸  è·³è¿‡ç¬¬920è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬946è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4092è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4192è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4194è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4199è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4200è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4224è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4242è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4243è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4252è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4254è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4301è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4302è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4303è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4457è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4458è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4459è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4541è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4602è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4603è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4762è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬4763è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5447è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5452è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5595è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5597è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5675è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5677è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5789è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5790è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5791è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5792è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5829è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5836è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5837è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5853è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5861è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5969è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5970è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5989è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬5990è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬6264è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬6334è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬6335è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬6367è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬6368è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬6369è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬6370è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬6371è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬6372è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬6373è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬6374è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬6375è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬6376è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬6432è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬6515è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬7667è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬8493è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬9186è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬9902è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬10128è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬10133è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬10162è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬10168è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬10170è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬10234è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12027è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12243è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12244è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12245è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12246è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12247è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12248è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12249è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12250è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12251è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12252è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12253è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12254è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12255è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12256è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12257è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12258è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬12259è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13828è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13829è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13837è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13838è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13839è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13840è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13841è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13842è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13843è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13844è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13845è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13846è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13847è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13848è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13851è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âš ï¸  è·³è¿‡ç¬¬13852è¡Œï¼šå†…å®¹ä¸ºç©º\n",
      "âœ… åŠ è½½æœ‰æ•ˆè¯—æ­Œæ•°æ®: 13753 æ¡\n",
      "ğŸ“Œ å…¨é‡è®­ç»ƒé…ç½®ï¼š\n",
      "- æ€»æ•°æ®é‡ï¼š13753 æ¡\n",
      "- æ¯Epochæ­¥æ•°ï¼š6876\n",
      "- æ€»è®­ç»ƒæ­¥æ•°ï¼š13752\n",
      "- æœ‰æ•ˆæ‰¹æ¬¡å¤§å°ï¼š4\n",
      "\n",
      "ğŸ“Œ åŠ è½½Qwen2.5-7Bæ¨¡å‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,261,568 || all params: 7,616,878,080 || trainable%: 0.0166\n",
      "\n",
      "ğŸš€ å¼€å§‹å…¨é‡æ•°æ®å¯¹æŠ—è®­ç»ƒ...\n",
      "[Epoch 1/2] Step 50/13752 | SFT Loss: 6.5800 | ADV Loss: 0.0000 | Total Loss: 6.5800 | Î»_adv: 0.020 | Grad Norm: 5.9683 | Step Time: 0.87s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ ADV Loss è°ƒè¯• - Step 100:\n",
      "  ç”¨æˆ·è¾“å…¥: Write a poem in the style of Judith Goldman....\n",
      "  ç”Ÿæˆæ–‡æœ¬: In twilight's hush, beneath the moon's soft gaze,\n",
      "The world whispers secrets, tender and true.\n",
      "A loo...\n",
      "  AIåˆ¤å®šæ¦‚ç‡: 1.000000 | åŸå§‹ADV Loss: 1.000000\n",
      "  åŠ æƒADV Loss (Ã—0.020): 0.020000\n",
      "[Epoch 1/2] Step 100/13752 | SFT Loss: 5.6021 | ADV Loss: 0.0004 | Total Loss: 5.6025 | Î»_adv: 0.020 | Grad Norm: 15.9153 | Step Time: 11.28s\n",
      "[Epoch 1/2] Step 150/13752 | SFT Loss: 5.4044 | ADV Loss: 0.0000 | Total Loss: 5.4044 | Î»_adv: 0.020 | Grad Norm: 7.1493 | Step Time: 0.87s\n",
      "\n",
      "ğŸ“ ADV Loss è°ƒè¯• - Step 200:\n",
      "  ç”¨æˆ·è¾“å…¥: Write a poem using the theme: Living,Time & Brevit...\n",
      "  ç”Ÿæˆæ–‡æœ¬: A river carves its path,\n",
      "Through valleys and mountainsides.\n",
      "In time it weaves a thread â€” \n",
      "Of ice or ...\n",
      "  AIåˆ¤å®šæ¦‚ç‡: 0.998535 | åŸå§‹ADV Loss: 0.998535\n",
      "  åŠ æƒADV Loss (Ã—0.020): 0.019971\n",
      "[Epoch 1/2] Step 200/13752 | SFT Loss: 4.4700 | ADV Loss: 0.0004 | Total Loss: 4.4704 | Î»_adv: 0.020 | Grad Norm: 12.2982 | Step Time: 10.90s\n",
      "[Epoch 1/2] Step 250/13752 | SFT Loss: 3.7014 | ADV Loss: 0.0000 | Total Loss: 3.7014 | Î»_adv: 0.020 | Grad Norm: 1.8878 | Step Time: 0.88s\n",
      "\n",
      "ğŸ“ ADV Loss è°ƒè¯• - Step 300:\n",
      "  ç”¨æˆ·è¾“å…¥: Write a poem using the theme: Living,Midlife,Paren...\n",
      "  ç”Ÿæˆæ–‡æœ¬: Palate's Plea\n",
      "\n",
      "The sun sets and I am left with half an apple,\n",
      "A child to feed, and a heart that won'...\n",
      "  AIåˆ¤å®šæ¦‚ç‡: 0.999023 | åŸå§‹ADV Loss: 0.999023\n",
      "  åŠ æƒADV Loss (Ã—0.020): 0.019980\n",
      "[Epoch 1/2] Step 300/13752 | SFT Loss: 3.4630 | ADV Loss: 0.0004 | Total Loss: 3.4634 | Î»_adv: 0.020 | Grad Norm: 1.6147 | Step Time: 10.93s\n",
      "[Epoch 1/2] Step 350/13752 | SFT Loss: 3.3913 | ADV Loss: 0.0000 | Total Loss: 3.3913 | Î»_adv: 0.020 | Grad Norm: 4.1799 | Step Time: 0.87s\n",
      "\n",
      "ğŸ“ ADV Loss è°ƒè¯• - Step 400:\n",
      "  ç”¨æˆ·è¾“å…¥: Write a poem using the theme: Activities,Indoor Ac...\n",
      "  ç”Ÿæˆæ–‡æœ¬: to get the day's news\n",
      "I sit in my armchair and watch television\n",
      "or sit in my living room with my fri...\n",
      "  AIåˆ¤å®šæ¦‚ç‡: 0.000597 | åŸå§‹ADV Loss: 0.000597\n",
      "  åŠ æƒADV Loss (Ã—0.020): 0.000012\n",
      "[Epoch 1/2] Step 400/13752 | SFT Loss: 3.2147 | ADV Loss: 0.0000 | Total Loss: 3.2147 | Î»_adv: 0.020 | Grad Norm: 1.3588 | Step Time: 10.93s\n",
      "[Epoch 1/2] Step 450/13752 | SFT Loss: 3.1046 | ADV Loss: 0.0000 | Total Loss: 3.1046 | Î»_adv: 0.020 | Grad Norm: 4.6444 | Step Time: 0.88s\n",
      "\n",
      "ğŸ“ ADV Loss è°ƒè¯• - Step 500:\n",
      "  ç”¨æˆ·è¾“å…¥: Write a poem using the theme: Arts & Sciences,Lang...\n",
      "  ç”Ÿæˆæ–‡æœ¬: æ— æœ‰æ•ˆç”Ÿæˆå†…å®¹...\n",
      "  AIåˆ¤å®šæ¦‚ç‡: 0.270996 | åŸå§‹ADV Loss: 0.270996\n",
      "  åŠ æƒADV Loss (Ã—0.020): 0.005420\n",
      "[Epoch 1/2] Step 500/13752 | SFT Loss: 2.9586 | ADV Loss: 0.0001 | Total Loss: 2.9587 | Î»_adv: 0.020 | Grad Norm: 0.6876 | Step Time: 1.07s\n",
      "[Epoch 1/2] Step 550/13752 | SFT Loss: 2.9688 | ADV Loss: 0.0000 | Total Loss: 2.9688 | Î»_adv: 0.020 | Grad Norm: 0.7257 | Step Time: 0.87s\n",
      "\n",
      "ğŸ“ ADV Loss è°ƒè¯• - Step 600:\n",
      "  ç”¨æˆ·è¾“å…¥: Write a poem using the theme: Love,Classic Love,De...\n",
      "  ç”Ÿæˆæ–‡æœ¬: O Goddesses who dwell upon Olympos!\n",
      "The day is far spent when you have taken away my heart.\n",
      "I am sei...\n",
      "  AIåˆ¤å®šæ¦‚ç‡: 0.000875 | åŸå§‹ADV Loss: 0.000875\n",
      "  åŠ æƒADV Loss (Ã—0.020): 0.000017\n",
      "[Epoch 1/2] Step 600/13752 | SFT Loss: 2.8623 | ADV Loss: 0.0000 | Total Loss: 2.8623 | Î»_adv: 0.020 | Grad Norm: 1.7159 | Step Time: 10.86s\n",
      "[Epoch 1/2] Step 650/13752 | SFT Loss: 3.0763 | ADV Loss: 0.0000 | Total Loss: 3.0763 | Î»_adv: 0.020 | Grad Norm: 0.9024 | Step Time: 0.88s\n",
      "\n",
      "ğŸ“ ADV Loss è°ƒè¯• - Step 700:\n",
      "  ç”¨æˆ·è¾“å…¥: Write a poem using the theme: Activities,Travels &...\n",
      "  ç”Ÿæˆæ–‡æœ¬: questioned by night, and wandering,\n",
      "In the midst of silence and stillness,\n",
      "I wandered through shadow...\n",
      "  AIåˆ¤å®šæ¦‚ç‡: 0.949707 | åŸå§‹ADV Loss: 0.949707\n",
      "  åŠ æƒADV Loss (Ã—0.020): 0.018994\n",
      "[Epoch 1/2] Step 700/13752 | SFT Loss: 2.9382 | ADV Loss: 0.0004 | Total Loss: 2.9386 | Î»_adv: 0.020 | Grad Norm: 1.4857 | Step Time: 10.90s\n",
      "[Epoch 1/2] Step 750/13752 | SFT Loss: 2.7411 | ADV Loss: 0.0000 | Total Loss: 2.7411 | Î»_adv: 0.020 | Grad Norm: 0.6759 | Step Time: 0.90s\n",
      "\n",
      "ğŸ“ ADV Loss è°ƒè¯• - Step 800:\n",
      "  ç”¨æˆ·è¾“å…¥: Write a poem using the theme: Relationships,Pets,N...\n",
      "  ç”Ÿæˆæ–‡æœ¬: ote to the reader: The following poem is composed with the deliberate intention that it should be re...\n",
      "  AIåˆ¤å®šæ¦‚ç‡: 0.777344 | åŸå§‹ADV Loss: 0.777344\n",
      "  åŠ æƒADV Loss (Ã—0.020): 0.015547\n",
      "[Epoch 1/2] Step 800/13752 | SFT Loss: 2.8240 | ADV Loss: 0.0003 | Total Loss: 2.8243 | Î»_adv: 0.020 | Grad Norm: 0.7377 | Step Time: 10.86s\n",
      "[Epoch 1/2] Step 850/13752 | SFT Loss: 2.8234 | ADV Loss: 0.0000 | Total Loss: 2.8234 | Î»_adv: 0.020 | Grad Norm: 0.6255 | Step Time: 0.88s\n",
      "\n",
      "ğŸ“ ADV Loss è°ƒè¯• - Step 900:\n",
      "  ç”¨æˆ·è¾“å…¥: Write a poem using the theme: Living,Life Choices,...\n",
      "  ç”Ÿæˆæ–‡æœ¬: When you come back to me with your heart's blood on\n",
      "the page, I say it is like a bird singing out it...\n",
      "  AIåˆ¤å®šæ¦‚ç‡: 0.000966 | åŸå§‹ADV Loss: 0.000966\n",
      "  åŠ æƒADV Loss (Ã—0.020): 0.000019\n",
      "[Epoch 1/2] Step 900/13752 | SFT Loss: 2.8897 | ADV Loss: 0.0000 | Total Loss: 2.8897 | Î»_adv: 0.020 | Grad Norm: 0.7302 | Step Time: 10.92s\n",
      "[Epoch 1/2] Step 950/13752 | SFT Loss: 2.8672 | ADV Loss: 0.0000 | Total Loss: 2.8672 | Î»_adv: 0.020 | Grad Norm: 0.9573 | Step Time: 0.89s\n",
      "\n",
      "ğŸ“ ADV Loss è°ƒè¯• - Step 1000:\n",
      "  ç”¨æˆ·è¾“å…¥: Write a poem using the theme: Living,Time & Brevit...\n",
      "  ç”Ÿæˆæ–‡æœ¬: I am not dead! No ghostly hand can seize me!\n",
      "Though Time's long scythe is lifted to my head,\n",
      "And tho...\n",
      "  AIåˆ¤å®šæ¦‚ç‡: 0.999512 | åŸå§‹ADV Loss: 0.999512\n",
      "  åŠ æƒADV Loss (Ã—0.020): 0.019990\n",
      "[Epoch 1/2] Step 1000/13752 | SFT Loss: 2.9255 | ADV Loss: 0.0004 | Total Loss: 2.9259 | Î»_adv: 0.020 | Grad Norm: 1.1714 | Step Time: 10.91s\n",
      "[Epoch 1/2] Step 1050/13752 | SFT Loss: 2.6594 | ADV Loss: 0.0000 | Total Loss: 2.6594 | Î»_adv: 0.020 | Grad Norm: 0.6920 | Step Time: 0.87s\n",
      "\n",
      "ğŸ“ ADV Loss è°ƒè¯• - Step 1100:\n",
      "  ç”¨æˆ·è¾“å…¥: Write a poem using the theme: Social Commentaries,...\n",
      "  ç”Ÿæˆæ–‡æœ¬: a man will die for his country; he'll live to see his son born and die for his country again\n",
      "\n",
      "for ye...\n",
      "  AIåˆ¤å®šæ¦‚ç‡: 0.894043 | åŸå§‹ADV Loss: 0.894043\n",
      "  åŠ æƒADV Loss (Ã—0.020): 0.017881\n",
      "[Epoch 1/2] Step 1100/13752 | SFT Loss: 2.8071 | ADV Loss: 0.0004 | Total Loss: 2.8075 | Î»_adv: 0.020 | Grad Norm: 0.5855 | Step Time: 10.89s\n",
      "[Epoch 1/2] Step 1150/13752 | SFT Loss: 2.8621 | ADV Loss: 0.0000 | Total Loss: 2.8621 | Î»_adv: 0.020 | Grad Norm: 0.7348 | Step Time: 0.88s\n",
      "\n",
      "ğŸ“ ADV Loss è°ƒè¯• - Step 1200:\n",
      "  ç”¨æˆ·è¾“å…¥: Write a poem using the theme: Nature,Trees & Flowe...\n",
      "  ç”Ÿæˆæ–‡æœ¬: question asked by  Guest on 2014-05-23 16:28:19 | # \n",
      " answer written by  Guest on 2017-05-08 16:59:2...\n",
      "  AIåˆ¤å®šæ¦‚ç‡: 0.904297 | åŸå§‹ADV Loss: 0.904297\n",
      "  åŠ æƒADV Loss (Ã—0.020): 0.018086\n",
      "[Epoch 1/2] Step 1200/13752 | SFT Loss: 2.7143 | ADV Loss: 0.0004 | Total Loss: 2.7147 | Î»_adv: 0.020 | Grad Norm: 0.6679 | Step Time: 10.90s\n",
      "[Epoch 1/2] Step 1250/13752 | SFT Loss: 2.7784 | ADV Loss: 0.0000 | Total Loss: 2.7784 | Î»_adv: 0.020 | Grad Norm: 0.7945 | Step Time: 0.87s\n",
      "\n",
      "ğŸ“ ADV Loss è°ƒè¯• - Step 1300:\n",
      "  ç”¨æˆ·è¾“å…¥: Write a poem using the theme: Activities,Jobs & Wo...\n",
      "  ç”Ÿæˆæ–‡æœ¬: We have to be here now,\n",
      "the earth is our stage.\n",
      "The sky and its clouds\n",
      "are our wings.\n",
      "It's our day t...\n",
      "  AIåˆ¤å®šæ¦‚ç‡: 0.896973 | åŸå§‹ADV Loss: 0.896973\n",
      "  åŠ æƒADV Loss (Ã—0.020): 0.017939\n",
      "[Epoch 1/2] Step 1300/13752 | SFT Loss: 2.8648 | ADV Loss: 0.0004 | Total Loss: 2.8652 | Î»_adv: 0.020 | Grad Norm: 0.7497 | Step Time: 10.94s\n",
      "[Epoch 1/2] Step 1350/13752 | SFT Loss: 2.9444 | ADV Loss: 0.0000 | Total Loss: 2.9444 | Î»_adv: 0.020 | Grad Norm: 0.6671 | Step Time: 0.87s\n",
      "\n",
      "ğŸ“ ADV Loss è°ƒè¯• - Step 1400:\n",
      "  ç”¨æˆ·è¾“å…¥: Write a poem using the theme: Living,Life Choices,...\n",
      "  ç”Ÿæˆæ–‡æœ¬: æ— æœ‰æ•ˆç”Ÿæˆå†…å®¹...\n",
      "  AIåˆ¤å®šæ¦‚ç‡: 0.270996 | åŸå§‹ADV Loss: 0.270996\n",
      "  åŠ æƒADV Loss (Ã—0.020): 0.005420\n",
      "[Epoch 1/2] Step 1400/13752 | SFT Loss: 2.7832 | ADV Loss: 0.0001 | Total Loss: 2.7833 | Î»_adv: 0.020 | Grad Norm: 0.6911 | Step Time: 0.99s\n",
      "[Epoch 1/2] Step 1450/13752 | SFT Loss: 2.9027 | ADV Loss: 0.0000 | Total Loss: 2.9027 | Î»_adv: 0.020 | Grad Norm: 0.8471 | Step Time: 0.89s\n",
      "\n",
      "ğŸ“ ADV Loss è°ƒè¯• - Step 1500:\n",
      "  ç”¨æˆ·è¾“å…¥: Write a poem using the theme: Activities,Travels &...\n",
      "  ç”Ÿæˆæ–‡æœ¬: A BRIEF NOTE ON THE HISTORY OF WATERPOLO \n",
      "\n",
      "IN 1896 water polo was contested as part of \n",
      "the Olympic ...\n",
      "  AIåˆ¤å®šæ¦‚ç‡: 0.980957 | åŸå§‹ADV Loss: 0.980957\n",
      "  åŠ æƒADV Loss (Ã—0.020): 0.019619\n",
      "[Epoch 1/2] Step 1500/13752 | SFT Loss: 2.9239 | ADV Loss: 0.0004 | Total Loss: 2.9243 | Î»_adv: 0.020 | Grad Norm: 0.8999 | Step Time: 10.93s\n",
      "[Epoch 1/2] Step 1550/13752 | SFT Loss: 2.9568 | ADV Loss: 0.0000 | Total Loss: 2.9568 | Î»_adv: 0.020 | Grad Norm: 1.1287 | Step Time: 0.89s\n",
      "\n",
      "ğŸ“ ADV Loss è°ƒè¯• - Step 1600:\n",
      "  ç”¨æˆ·è¾“å…¥: Write a poem in the style of W. S. Merwin....\n",
      "  ç”Ÿæˆæ–‡æœ¬: æ— æœ‰æ•ˆç”Ÿæˆå†…å®¹...\n",
      "  AIåˆ¤å®šæ¦‚ç‡: 0.270996 | åŸå§‹ADV Loss: 0.270996\n",
      "  åŠ æƒADV Loss (Ã—0.020): 0.005420\n",
      "[Epoch 1/2] Step 1600/13752 | SFT Loss: 2.7258 | ADV Loss: 0.0001 | Total Loss: 2.7259 | Î»_adv: 0.020 | Grad Norm: 0.6992 | Step Time: 0.99s\n",
      "[Epoch 1/2] Step 1650/13752 | SFT Loss: 2.8226 | ADV Loss: 0.0000 | Total Loss: 2.8226 | Î»_adv: 0.020 | Grad Norm: 0.8281 | Step Time: 0.88s\n",
      "\n",
      "ğŸ“ ADV Loss è°ƒè¯• - Step 1700:\n",
      "  ç”¨æˆ·è¾“å…¥: Write a poem using the theme: Living,Sorrow & Grie...\n",
      "  ç”Ÿæˆæ–‡æœ¬: æ— æœ‰æ•ˆç”Ÿæˆå†…å®¹...\n",
      "  AIåˆ¤å®šæ¦‚ç‡: 0.270996 | åŸå§‹ADV Loss: 0.270996\n",
      "  åŠ æƒADV Loss (Ã—0.020): 0.005420\n",
      "[Epoch 1/2] Step 1700/13752 | SFT Loss: 2.8052 | ADV Loss: 0.0001 | Total Loss: 2.8053 | Î»_adv: 0.020 | Grad Norm: 0.8650 | Step Time: 0.98s\n",
      "[Epoch 1/2] Step 1750/13752 | SFT Loss: 2.9677 | ADV Loss: 0.0000 | Total Loss: 2.9677 | Î»_adv: 0.020 | Grad Norm: 1.1376 | Step Time: 0.88s\n",
      "\n",
      "ğŸ“ ADV Loss è°ƒè¯• - Step 1800:\n",
      "  ç”¨æˆ·è¾“å…¥: Write a poem using the theme: Living,Separation & ...\n",
      "  ç”Ÿæˆæ–‡æœ¬: æ— æœ‰æ•ˆç”Ÿæˆå†…å®¹...\n",
      "  AIåˆ¤å®šæ¦‚ç‡: 0.270996 | åŸå§‹ADV Loss: 0.270996\n",
      "  åŠ æƒADV Loss (Ã—0.020): 0.005420\n",
      "[Epoch 1/2] Step 1800/13752 | SFT Loss: 3.0753 | ADV Loss: 0.0001 | Total Loss: 3.0754 | Î»_adv: 0.020 | Grad Norm: 1.4229 | Step Time: 0.98s\n",
      "[Epoch 1/2] Step 1850/13752 | SFT Loss: 2.9427 | ADV Loss: 0.0000 | Total Loss: 2.9427 | Î»_adv: 0.020 | Grad Norm: 0.8467 | Step Time: 0.89s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 449\u001b[39m\n\u001b[32m    446\u001b[39m total_loss = sft_loss + lambda_adv * adv_loss\n\u001b[32m    448\u001b[39m \u001b[38;5;66;03m# 4. åå‘ä¼ æ’­\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m \u001b[43mtotal_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;66;03m# 5. æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢çˆ†ç‚¸ï¼‰\u001b[39;00m\n\u001b[32m    452\u001b[39m grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.5\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen_text/lib/python3.11/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    PreTrainedTokenizerFast, AutoModelForCausalLM,\n",
    "    AutoTokenizer, AutoModelForSequenceClassification\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -------------------------\n",
    "# 0. å…¨é‡è®­ç»ƒé…ç½®ï¼ˆé›†æˆADV Lossï¼‰\n",
    "# -------------------------\n",
    "# ç¼“å­˜é…ç½®\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/root/autodl-tmp/hf_cache\"\n",
    "os.environ[\"HF_HOME\"] = \"/root/autodl-tmp/hf_cache\"\n",
    "torch.hub.set_dir(\"/root/autodl-tmp/torch_cache\")\n",
    "os.environ[\"TORCH_HOME\"] = \"/root/autodl-tmp/torch_cache\"\n",
    "os.makedirs(\"/root/autodl-tmp/offload\", exist_ok=True)\n",
    "\n",
    "# å…³é”®ï¼šå¯ç”¨CUDAåŒæ­¥æ‰§è¡Œï¼Œè§£å†³è®¾å¤‡ç«¯æ–­è¨€é”™è¯¯\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "\n",
    "# å…¨é‡è®­ç»ƒé…ç½®ï¼ˆé›†æˆå¯¹æŠ—è®­ç»ƒï¼‰\n",
    "CONFIG = {\n",
    "    \"model_name_or_path\": \"/root/autodl-tmp/qwen2.5-7b/qwen2.5-7b\",\n",
    "    \"data_path\": \"/root/autodl-tmp/Pro/poetry_sft.jsonl\",\n",
    "    \"output_dir\": \"/root/autodl-tmp/Pro/qwen2.5-sft-full-adv\",\n",
    "    \"num_train_epochs\": 2,  # å…¨é‡æ•°æ®è®­ç»ƒ3è½®\n",
    "    \"batch_size\": 2,  # æŒ‰éœ€è°ƒæ•´ï¼ˆæ ¹æ®GPUæ˜¾å­˜ï¼‰\n",
    "    \"gradient_accumulation_steps\": 2,  # æ¢¯åº¦ç´¯ç§¯ï¼Œç­‰æ•ˆå¢å¤§æ‰¹æ¬¡\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"max_seq_length\": 256,  # é€‚é…æ›´é•¿çš„è¯—æ­Œæ–‡æœ¬\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"offload_folder\": \"/root/autodl-tmp/offload\",\n",
    "    # ç”Ÿæˆä¸“ç”¨é…ç½®\n",
    "    \"gen_max_new_tokens\": 100,\n",
    "    \"gen_temperature\": 0.85,\n",
    "    \"gen_top_k\": 30,\n",
    "    \"gen_top_p\": 0.92,\n",
    "    \"gen_repetition_penalty\": 1.15,\n",
    "    # å¯¹æŠ—è®­ç»ƒé…ç½®\n",
    "    \"detector_model_path\": \"/root/autodl-tmp/Pro/models--roberta-base-openai-detector/snapshots/6cba99c003b711c7fe94f8a3aa2be35a792cb6fa\",\n",
    "    \"detector_every_steps\": 100,  # æ¯8æ­¥è®¡ç®—ä¸€æ¬¡å¯¹æŠ—æŸå¤±\n",
    "    \"lambda_adv_max\": 0.25,     # å¯¹æŠ—æŸå¤±æœ€å¤§æƒé‡\n",
    "    \"lambda_adv_base\": 0.02,    # å¯¹æŠ—æŸå¤±åŸºç¡€æƒé‡\n",
    "    \"lambda_adv_step\": 0.015,   # æ¯epochå¢åŠ çš„æƒé‡\n",
    "    \"detector_dtype\": torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    # è®­ç»ƒç›‘æ§\n",
    "    \"print_every_steps\": 50,  # æ¯10æ­¥æ‰“å°ä¸€æ¬¡Loss\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# 1. åŠ è½½Tokenizerï¼ˆå…¼å®¹å…¨é‡æ•°æ®ï¼‰\n",
    "# -------------------------\n",
    "tokenizer_file = os.path.join(CONFIG[\"model_name_or_path\"], \"tokenizer.json\")\n",
    "assert os.path.exists(tokenizer_file), f\"Tokenizeræ–‡ä»¶ä¸å­˜åœ¨: {tokenizer_file}\"\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_file)\n",
    "# Qwen2.5ä¸“ç”¨ç‰¹æ®Štokené…ç½®\n",
    "tokenizer.eos_token = \"<|endoftext|>\"\n",
    "tokenizer.bos_token = \"<|startoftext|>\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({\n",
    "    \"additional_special_tokens\": [\"<|im_start|>\", \"<|im_end|>\"]\n",
    "})\n",
    "print(\"âœ… ç”Ÿæˆæ¨¡å‹TokenizeråŠ è½½å®Œæˆ\")\n",
    "\n",
    "# -------------------------\n",
    "# 2. åŠ è½½AIæ£€æµ‹å™¨æ¨¡å‹ï¼ˆç”¨äºè®¡ç®—ADV Lossï¼‰\n",
    "# -------------------------\n",
    "print(\"\\nğŸ“Œ åŠ è½½AIæ£€æµ‹å™¨æ¨¡å‹...\")\n",
    "# åŠ è½½æ£€æµ‹å™¨tokenizer\n",
    "detector_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG[\"detector_model_path\"],\n",
    "    local_files_only=True,\n",
    "    padding_side=\"right\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# åŠ è½½æ£€æµ‹å™¨æ¨¡å‹\n",
    "detector_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    CONFIG[\"detector_model_path\"],\n",
    "    torch_dtype=CONFIG[\"detector_dtype\"],\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# æ£€æµ‹å™¨è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼\n",
    "detector_model.eval()\n",
    "for param in detector_model.parameters():\n",
    "    param.requires_grad = False  # å†»ç»“æ£€æµ‹å™¨å‚æ•°\n",
    "print(\"âœ… AIæ£€æµ‹å™¨åŠ è½½å®Œæˆå¹¶å†»ç»“\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def detector_ai_prob(text):\n",
    "    \"\"\"è®¡ç®—æ–‡æœ¬çš„AIç”Ÿæˆæ¦‚ç‡ï¼ˆè¶Šé«˜è¡¨ç¤ºè¶ŠåƒAIç”Ÿæˆï¼‰\"\"\"\n",
    "    # ç©ºæ–‡æœ¬å®¹é”™\n",
    "    if not text or text.strip() == \"\":\n",
    "        return 0.0\n",
    "    \n",
    "    # Tokenizeç”Ÿæˆæ–‡æœ¬\n",
    "    det_inputs = detector_tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True, \n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    ).to(CONFIG[\"device\"])\n",
    "    \n",
    "    # åˆ†åˆ«å¤„ç†tensorç±»å‹ï¼ˆå…³é”®ï¼šinput_idsä¿æŒLongç±»å‹ï¼‰\n",
    "    for key in det_inputs.keys():\n",
    "        if key == \"input_ids\":\n",
    "            det_inputs[key] = det_inputs[key].long()\n",
    "        elif key == \"attention_mask\":\n",
    "            det_inputs[key] = det_inputs[key].to(dtype=CONFIG[\"detector_dtype\"])\n",
    "        elif det_inputs[key].dtype in [torch.int32, torch.int64]:\n",
    "            det_inputs[key] = det_inputs[key].long()\n",
    "        else:\n",
    "            det_inputs[key] = det_inputs[key].to(dtype=CONFIG[\"detector_dtype\"])\n",
    "    \n",
    "    # æ£€æµ‹å™¨å‰å‘è®¡ç®—\n",
    "    det_outputs = detector_model(**det_inputs)\n",
    "    # roberta-base-openai-detectorè¾“å‡ºï¼š[0]=äººç±»æ¦‚ç‡ï¼Œ[1]=AIæ¦‚ç‡\n",
    "    probabilities = torch.softmax(det_outputs.logits, dim=-1)\n",
    "    ai_prob = probabilities[0, 1].item()  # AIç”Ÿæˆçš„æ¦‚ç‡ï¼ˆ0~1ï¼‰\n",
    "    \n",
    "    return ai_prob\n",
    "\n",
    "# -------------------------\n",
    "# 3. å…¨é‡æ•°æ®Datasetï¼ˆåŠ è½½æ‰€æœ‰æœ‰æ•ˆè¯—æ­Œï¼‰\n",
    "# -------------------------\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, jsonl_path, tokenizer, max_len=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        # åŠ è½½æ‰€æœ‰æœ‰æ•ˆæ•°æ®\n",
    "        self.data = self._load_all_valid_samples(jsonl_path)\n",
    "        assert len(self.data) > 0, f\"æ•°æ®è·¯å¾„ {jsonl_path} ä¸‹æ— æœ‰æ•ˆæ•°æ®ï¼\"\n",
    "        print(f\"âœ… åŠ è½½æœ‰æ•ˆè¯—æ­Œæ•°æ®: {len(self.data)} æ¡\")\n",
    "\n",
    "    def _load_all_valid_samples(self, jsonl_path):\n",
    "        \"\"\"åŠ è½½æ‰€æœ‰æœ‰æ•ˆè¯—æ­Œæ•°æ®\"\"\"\n",
    "        valid_data = []\n",
    "        if not os.path.exists(jsonl_path):\n",
    "            raise FileNotFoundError(f\"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {jsonl_path}\")\n",
    "        \n",
    "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line_idx, line in enumerate(lines):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    sample = json.loads(line)\n",
    "                    # æ ¡éªŒmessagesç»“æ„å’Œå†…å®¹\n",
    "                    if len(sample.get(\"messages\", [])) < 2:\n",
    "                        print(f\"âš ï¸  è·³è¿‡ç¬¬{line_idx+1}è¡Œï¼šmessagesæ ¼å¼é”™è¯¯\")\n",
    "                        continue\n",
    "                    user_text = sample[\"messages\"][0][\"content\"].strip()\n",
    "                    assistant_text = sample[\"messages\"][1][\"content\"].strip()\n",
    "                    if not user_text or not assistant_text:\n",
    "                        print(f\"âš ï¸  è·³è¿‡ç¬¬{line_idx+1}è¡Œï¼šå†…å®¹ä¸ºç©º\")\n",
    "                        continue\n",
    "                    valid_data.append({\n",
    "                        \"user\": user_text,\n",
    "                        \"assistant\": assistant_text\n",
    "                    })\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"âš ï¸  è·³è¿‡ç¬¬{line_idx+1}è¡Œï¼šJSONè§£æå¤±è´¥\")\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸  è·³è¿‡ç¬¬{line_idx+1}è¡Œï¼š{str(e)}\")\n",
    "        return valid_data\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"è¿”å›å…¨é‡æ•°æ®é•¿åº¦\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"æŒ‰ç´¢å¼•åŠ è½½å•æ¡æ•°æ®\"\"\"\n",
    "        sample = self.data[idx]\n",
    "        # æ„å»ºQwen2.5æ ‡å‡†å¯¹è¯prompt\n",
    "        prompt = f\"<|im_start|>user\\n{sample['user']}<|im_end|>\\n<|im_start|>assistant\\n{sample['assistant']}<|im_end|>\"\n",
    "        \n",
    "        # ç¼–ç ï¼ˆé€‚é…é•¿åºåˆ—ï¼‰\n",
    "        encoding = self.tokenizer(\n",
    "            prompt,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            padding_side=\"right\",\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = encoding.input_ids.squeeze(0).long()\n",
    "        attention_mask = encoding.attention_mask.squeeze(0).long()\n",
    "\n",
    "        # æ„å»ºæ ‡ç­¾ï¼ˆä»…è®¡ç®—assistantéƒ¨åˆ†Lossï¼‰\n",
    "        user_prompt = f\"<|im_start|>user\\n{sample['user']}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "        user_len = len(self.tokenizer.encode(user_prompt, truncation=True, max_length=self.max_len))\n",
    "        labels = torch.full_like(input_ids, -100)  # useréƒ¨åˆ†å¿½ç•¥\n",
    "        # æ ‡æ³¨assistantå…¨éƒ¨æœ‰æ•ˆtokenï¼ˆé€‚é…é•¿åºåˆ—ï¼‰\n",
    "        labels[user_len:min(user_len + self.max_len - user_len, self.max_len)] = input_ids[user_len:min(user_len + self.max_len - user_len, self.max_len)]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"user_text\": sample[\"user\"],  # ä¿ç•™ç”¨æˆ·è¾“å…¥ï¼Œç”¨äºç”ŸæˆéªŒè¯\n",
    "            \"assistant_text\": sample[\"assistant\"]  # ä¿ç•™å‚è€ƒç­”æ¡ˆ\n",
    "        }\n",
    "\n",
    "# åŠ è½½å…¨é‡æ•°æ®é›†\n",
    "dataset = PoetryDataset(CONFIG[\"data_path\"], tokenizer, CONFIG[\"max_seq_length\"])\n",
    "# å…¨é‡æ•°æ®DataLoaderï¼ˆæ‰“ä¹±+æ‰¹æ¬¡åŠ è½½ï¼‰\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    shuffle=True,  # æ‰“ä¹±æ•°æ®ï¼Œæå‡æ³›åŒ–æ€§\n",
    "    num_workers=0,  # ç¦ç”¨å¤šè¿›ç¨‹ï¼Œé¿å…æ•°æ®åŠ è½½å¼‚å¸¸\n",
    "    drop_last=True  # ä¸¢å¼ƒæœ€åä¸å®Œæ•´æ‰¹æ¬¡\n",
    ")\n",
    "total_steps = len(train_loader) * CONFIG[\"num_train_epochs\"]\n",
    "print(f\"ğŸ“Œ å…¨é‡è®­ç»ƒé…ç½®ï¼š\")\n",
    "print(f\"- æ€»æ•°æ®é‡ï¼š{len(dataset)} æ¡\")\n",
    "print(f\"- æ¯Epochæ­¥æ•°ï¼š{len(train_loader)}\")\n",
    "print(f\"- æ€»è®­ç»ƒæ­¥æ•°ï¼š{total_steps}\")\n",
    "print(f\"- æœ‰æ•ˆæ‰¹æ¬¡å¤§å°ï¼š{CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']}\")\n",
    "\n",
    "# -------------------------\n",
    "# 4. åŠ è½½ç”Ÿæˆæ¨¡å‹ï¼ˆFP32+LoRAï¼Œå…¨é‡è®­ç»ƒç¨³å®šï¼‰\n",
    "# -------------------------\n",
    "print(\"\\nğŸ“Œ åŠ è½½Qwen2.5-7Bæ¨¡å‹...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"model_name_or_path\"],\n",
    "    torch_dtype=torch.float32,  # ä¿æŒFP32ç¨³å®šï¼Œå…¨é‡è®­ç»ƒä¸æº¢å‡º\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=CONFIG[\"offload_folder\"],\n",
    "    attn_implementation=\"eager\",  # ç¦ç”¨flash attentionï¼Œæå‡ç¨³å®šæ€§\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "model.train()\n",
    "\n",
    "# LoRAé…ç½®ï¼ˆé€‚åº¦å¢å¤§rå€¼ï¼Œæå‡å…¨é‡æ•°æ®æ‹Ÿåˆèƒ½åŠ›ï¼‰\n",
    "lora_cfg = LoraConfig(\n",
    "    r=4,  # æ¯”å•æ ·æœ¬è°ƒè¯•å¢å¤§ï¼Œé€‚é…å…¨é‡æ•°æ®\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.01,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # ä»…è®­ç»ƒå…³é”®æŠ•å½±å±‚\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    bias=\"none\"\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# -------------------------\n",
    "# 5. ä¼˜åŒ–å™¨é…ç½®ï¼ˆé€‚é…å…¨é‡è®­ç»ƒ+å¯¹æŠ—è®­ç»ƒï¼‰\n",
    "# -------------------------\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG[\"learning_rate\"],\n",
    "    eps=1e-8,\n",
    "    weight_decay=0.001,\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 6. è®­ç»ƒè¾…åŠ©å‡½æ•°ï¼ˆé›†æˆADV Lossï¼‰\n",
    "# -------------------------\n",
    "def format_time(seconds):\n",
    "    \"\"\"æ ¼å¼åŒ–æ—¶é—´è¾“å‡º\"\"\"\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = int(seconds % 60)\n",
    "    return f\"{hours:02d}:{minutes:02d}:{secs:02d}\"\n",
    "\n",
    "def generate_for_detector(model, tokenizer, user_text):\n",
    "    \"\"\"ç”Ÿæˆæ–‡æœ¬ç”¨äºæ£€æµ‹å™¨è¯„ä¼°ï¼ˆè®¡ç®—ADV Lossï¼‰\"\"\"\n",
    "    # æ„å»ºæ ‡å‡†prompt\n",
    "    prompt = f\"<|im_start|>user\\n{user_text}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    \n",
    "    # Tokenize\n",
    "    generate_inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=CONFIG[\"max_seq_length\"] - CONFIG[\"gen_max_new_tokens\"]\n",
    "    ).to(CONFIG[\"device\"])\n",
    "    \n",
    "    input_ids = generate_inputs[\"input_ids\"].long()\n",
    "    attention_mask = generate_inputs[\"attention_mask\"].long()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=CONFIG[\"gen_max_new_tokens\"],\n",
    "            do_sample=True,\n",
    "            temperature=CONFIG[\"gen_temperature\"],\n",
    "            top_k=CONFIG[\"gen_top_k\"],\n",
    "            top_p=CONFIG[\"gen_top_p\"],\n",
    "            repetition_penalty=CONFIG[\"gen_repetition_penalty\"],\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            early_stopping=True,\n",
    "            num_return_sequences=1,\n",
    "            use_cache=False,\n",
    "            renormalize_logits=True,\n",
    "            remove_invalid_values=True\n",
    "        )\n",
    "    model.train()\n",
    "    \n",
    "    # è§£ç å¹¶æå–ç”Ÿæˆå†…å®¹\n",
    "    gen_text = tokenizer.decode(\n",
    "        outputs[0][input_ids.shape[-1]:],\n",
    "        skip_special_tokens=True\n",
    "    ).strip()\n",
    "    \n",
    "    # ç©ºæ–‡æœ¬å®¹é”™\n",
    "    if not gen_text:\n",
    "        gen_text = \"æ— æœ‰æ•ˆç”Ÿæˆå†…å®¹\"\n",
    "    \n",
    "    return gen_text\n",
    "\n",
    "def generate_validation_sample(model, tokenizer, dataset, config):\n",
    "    \"\"\"éšæœºé€‰1æ¡æ•°æ®éªŒè¯ç”Ÿæˆæ•ˆæœï¼ˆåŒ…å«ADV Lossè¯„ä¼°ï¼‰\"\"\"\n",
    "    model.eval()\n",
    "    # éšæœºé€‰1æ¡æ•°æ®\n",
    "    sample_idx = random.randint(0, len(dataset)-1)\n",
    "    sample = dataset.data[sample_idx]\n",
    "    prompt = f\"<|im_start|>user\\n{sample['user']}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(config[\"device\"])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=config[\"gen_max_new_tokens\"],\n",
    "            do_sample=True,\n",
    "            temperature=config[\"gen_temperature\"],\n",
    "            top_k=config[\"gen_top_k\"],\n",
    "            top_p=config[\"gen_top_p\"],\n",
    "            repetition_penalty=config[\"gen_repetition_penalty\"],\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            early_stopping=True,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "    \n",
    "    # è§£ç å¹¶æ¸…ç†ç”Ÿæˆç»“æœ\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    if \"<|im_start|>assistant\\n\" in generated_text:\n",
    "        generated_poem = generated_text.split(\"<|im_start|>assistant\\n\")[1]\n",
    "        generated_poem = generated_poem.replace(\"<|endoftext|>\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
    "    else:\n",
    "        generated_poem = generated_text.strip()\n",
    "    \n",
    "    # è®¡ç®—è¯¥ç”Ÿæˆæ–‡æœ¬çš„AIæ¦‚ç‡ï¼ˆADV Lossï¼‰\n",
    "    ai_prob = detector_ai_prob(generated_poem)\n",
    "    \n",
    "    # æ‰“å°éªŒè¯ç»“æœ\n",
    "    print(\"\\nğŸ“ ç”ŸæˆéªŒè¯ï¼ˆéšæœºæ ·æœ¬ï¼‰:\")\n",
    "    print(f\"ä¸»é¢˜: {sample['user'][:50]}...\")\n",
    "    print(\"ç”Ÿæˆè¯—æ­Œ:\")\n",
    "    print(generated_poem)\n",
    "    print(f\"AIåˆ¤å®šæ¦‚ç‡ (ADV Loss): {ai_prob:.4f} (è¶Šä½è¶Šåƒäººç±»åˆ›ä½œ)\")\n",
    "    print(\"-\" * 80)\n",
    "    model.train()  # å›åˆ°è®­ç»ƒæ¨¡å¼\n",
    "\n",
    "# -------------------------\n",
    "# 7. å…¨é‡è®­ç»ƒä¸»å¾ªç¯ï¼ˆé›†æˆADV Lossï¼‰\n",
    "# -------------------------\n",
    "print(\"\\nğŸš€ å¼€å§‹å…¨é‡æ•°æ®å¯¹æŠ—è®­ç»ƒ...\")\n",
    "global_step = 0\n",
    "total_train_time = 0.0\n",
    "avg_sft_loss_list = []\n",
    "avg_adv_loss_list = []\n",
    "\n",
    "for epoch in range(CONFIG[\"num_train_epochs\"]):\n",
    "    epoch_start = time.time()\n",
    "    epoch_sft_loss = 0.0\n",
    "    epoch_adv_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        step_start = time.time()\n",
    "        global_step += 1\n",
    "\n",
    "        # åŠ è½½æ‰¹æ¬¡æ•°æ®åˆ°è®¾å¤‡\n",
    "        input_ids = batch[\"input_ids\"].to(CONFIG[\"device\"], non_blocking=True)\n",
    "        labels = batch[\"labels\"].to(CONFIG[\"device\"], non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(CONFIG[\"device\"], non_blocking=True)\n",
    "        user_texts = batch[\"user_text\"]  # è·å–ç”¨æˆ·è¾“å…¥æ–‡æœ¬\n",
    "\n",
    "        # 1. è®¡ç®—SFTæŸå¤±ï¼ˆæ ‡å‡†ç›‘ç£è®­ç»ƒæŸå¤±ï¼‰\n",
    "        with torch.autograd.detect_anomaly():\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            sft_loss = outputs.loss\n",
    "            # æ¢¯åº¦ç´¯ç§¯ï¼šLossé™¤ä»¥ç´¯ç§¯æ­¥æ•°\n",
    "            sft_loss = sft_loss / CONFIG[\"gradient_accumulation_steps\"]\n",
    "\n",
    "        # 2. è®¡ç®—å¯¹æŠ—æŸå¤±ï¼ˆADV Lossï¼‰\n",
    "        adv_loss = torch.tensor(0.0, device=CONFIG[\"device\"], dtype=torch.float32)\n",
    "        # åŠ¨æ€è°ƒæ•´å¯¹æŠ—æŸå¤±æƒé‡\n",
    "        lambda_adv = min(\n",
    "            CONFIG[\"lambda_adv_max\"],\n",
    "            CONFIG[\"lambda_adv_base\"] + CONFIG[\"lambda_adv_step\"] * epoch\n",
    "        )\n",
    "        \n",
    "        # æ¯detector_every_stepsè®¡ç®—ä¸€æ¬¡å¯¹æŠ—æŸå¤±\n",
    "        if global_step % CONFIG[\"detector_every_steps\"] == 0:\n",
    "            # éšæœºé€‰æ‹©ä¸€ä¸ªæ ·æœ¬ç”Ÿæˆæ–‡æœ¬\n",
    "            sample_idx = random.randint(0, len(user_texts) - 1)\n",
    "            user_text = user_texts[sample_idx]\n",
    "            \n",
    "            # ç”Ÿæˆæ–‡æœ¬ç”¨äºæ£€æµ‹å™¨è¯„ä¼°\n",
    "            gen_text = generate_for_detector(model, tokenizer, user_text)\n",
    "            \n",
    "            # è®¡ç®—AIæ¦‚ç‡ä½œä¸ºå¯¹æŠ—æŸå¤±ï¼ˆç›®æ ‡ï¼šæœ€å°åŒ–AIæ¦‚ç‡ï¼‰\n",
    "            ai_prob = detector_ai_prob(gen_text)\n",
    "            adv_loss = torch.tensor(ai_prob, device=CONFIG[\"device\"], dtype=torch.float32, requires_grad=True)\n",
    "            # æ¢¯åº¦ç´¯ç§¯ï¼šå¯¹æŠ—æŸå¤±ä¹Ÿé™¤ä»¥ç´¯ç§¯æ­¥æ•°\n",
    "            adv_loss = adv_loss / CONFIG[\"gradient_accumulation_steps\"]\n",
    "            \n",
    "            # è°ƒè¯•æ‰“å°ï¼ˆæŸ¥çœ‹ADV Lossè¯¦æƒ…ï¼‰\n",
    "            if global_step % CONFIG[\"print_every_steps\"] == 0:\n",
    "                print(f\"\\nğŸ“ ADV Loss è°ƒè¯• - Step {global_step}:\")\n",
    "                print(f\"  ç”¨æˆ·è¾“å…¥: {user_text[:50]}...\")\n",
    "                print(f\"  ç”Ÿæˆæ–‡æœ¬: {gen_text[:100]}...\")\n",
    "                print(f\"  AIåˆ¤å®šæ¦‚ç‡: {ai_prob:.6f} | åŸå§‹ADV Loss: {ai_prob:.6f}\")\n",
    "                print(f\"  åŠ æƒADV Loss (Ã—{lambda_adv:.3f}): {ai_prob * lambda_adv:.6f}\")\n",
    "\n",
    "        # 3. è®¡ç®—æ€»æŸå¤±ï¼ˆSFT Loss + Î» * ADV Lossï¼‰\n",
    "        total_loss = sft_loss + lambda_adv * adv_loss\n",
    "\n",
    "        # 4. åå‘ä¼ æ’­\n",
    "        total_loss.backward()\n",
    "\n",
    "        # 5. æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢çˆ†ç‚¸ï¼‰\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.5)\n",
    "\n",
    "        # 6. ç´¯ç§¯æ­¥æ•°è¾¾æ ‡åæ›´æ–°å‚æ•°\n",
    "        if (step + 1) % CONFIG[\"gradient_accumulation_steps\"] == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # 7. ç´¯è®¡Loss\n",
    "        epoch_sft_loss += sft_loss.item() * CONFIG[\"gradient_accumulation_steps\"]\n",
    "        epoch_adv_loss += adv_loss.item() * CONFIG[\"gradient_accumulation_steps\"] * lambda_adv\n",
    "        avg_sft_loss_list.append(sft_loss.item() * CONFIG[\"gradient_accumulation_steps\"])\n",
    "        avg_adv_loss_list.append(adv_loss.item() * CONFIG[\"gradient_accumulation_steps\"] * lambda_adv)\n",
    "\n",
    "        # 8. æ‰“å°è®­ç»ƒè¿›åº¦\n",
    "        if global_step % CONFIG[\"print_every_steps\"] == 0:\n",
    "            step_time = time.time() - step_start\n",
    "            avg_sft_loss = sum(avg_sft_loss_list[-CONFIG[\"print_every_steps\"]:]) / CONFIG[\"print_every_steps\"]\n",
    "            avg_adv_loss = sum(avg_adv_loss_list[-CONFIG[\"print_every_steps\"]:]) / CONFIG[\"print_every_steps\"]\n",
    "            avg_total_loss = avg_sft_loss + avg_adv_loss\n",
    "            \n",
    "            print(f\"[Epoch {epoch+1}/{CONFIG['num_train_epochs']}] \"\n",
    "                  f\"Step {global_step}/{total_steps} | \"\n",
    "                  f\"SFT Loss: {avg_sft_loss:.4f} | \"\n",
    "                  f\"ADV Loss: {avg_adv_loss:.4f} | \"\n",
    "                  f\"Total Loss: {avg_total_loss:.4f} | \"\n",
    "                  f\"Î»_adv: {lambda_adv:.3f} | \"\n",
    "                  f\"Grad Norm: {grad_norm.item():.4f} | \"\n",
    "                  f\"Step Time: {step_time:.2f}s\")\n",
    "\n",
    "    # Epochç»“æŸå¤„ç†\n",
    "    epoch_time = format_time(time.time() - epoch_start)\n",
    "    total_train_time += time.time() - epoch_start\n",
    "    avg_epoch_sft_loss = epoch_sft_loss / len(train_loader)\n",
    "    avg_epoch_adv_loss = epoch_adv_loss / len(train_loader)\n",
    "    avg_epoch_total_loss = avg_epoch_sft_loss + avg_epoch_adv_loss\n",
    "    \n",
    "    print(f\"\\nâœ… Epoch {epoch+1} å®Œæˆ | \"\n",
    "          f\"Avg SFT Loss: {avg_epoch_sft_loss:.4f} | \"\n",
    "          f\"Avg ADV Loss: {avg_epoch_adv_loss:.4f} | \"\n",
    "          f\"Avg Total Loss: {avg_epoch_total_loss:.4f} | \"\n",
    "          f\"Î»_adv: {lambda_adv:.3f} | \"\n",
    "          f\"Epoch Time: {epoch_time} | \"\n",
    "          f\"Total Time: {format_time(total_train_time)}\")\n",
    "\n",
    "    # æ¯ä¸ªEpochç»“æŸåéªŒè¯ç”Ÿæˆæ•ˆæœï¼ˆåŒ…å«ADV Lossè¯„ä¼°ï¼‰\n",
    "    generate_validation_sample(model, tokenizer, dataset, CONFIG)\n",
    "\n",
    "# -------------------------\n",
    "# 8. ä¿å­˜æœ€ç»ˆæ¨¡å‹\n",
    "# -------------------------\n",
    "print(\"\\nğŸ“¦ ä¿å­˜è®­ç»ƒå®Œæˆæ¨¡å‹...\")\n",
    "model.save_pretrained(CONFIG[\"output_dir\"])\n",
    "tokenizer.save_pretrained(CONFIG[\"output_dir\"])\n",
    "print(f\"âœ… æ¨¡å‹å·²ä¿å­˜è‡³: {CONFIG['output_dir']}\")\n",
    "\n",
    "# -------------------------\n",
    "# 9. æœ€ç»ˆç”ŸæˆéªŒè¯ï¼ˆéšæœºé€‰3æ¡æ•°æ®ï¼ŒåŒ…å«ADV Lossè¯„ä¼°ï¼‰\n",
    "# -------------------------\n",
    "print(\"\\nğŸ¯ æœ€ç»ˆç”ŸæˆéªŒè¯ï¼ˆéšæœº3æ¡æ•°æ®ï¼‰:\")\n",
    "for i in range(3):\n",
    "    generate_validation_sample(model, tokenizer, dataset, CONFIG)\n",
    "\n",
    "print(\"\\nğŸ‰ å…¨é‡å¯¹æŠ—è®­ç»ƒå®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b65f5367-3d95-4ac3-91b4-d8fc66534bc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== å•æ ·æœ¬ ADV Loss è°ƒè¯• =====\n",
      "\n",
      "1. åŠ è½½ç¬¬ä¸€æ¡JSONLæ ·æœ¬...\n",
      "æ ·æœ¬Userå†…å®¹: Write a poem in the style of Michelle Menting....\n",
      "æ ·æœ¬Assistantå†…å®¹: Dog bone, stapler,\n",
      "cribbage board, garlic press\n",
      "     because this window is looseâ€”lacks\n",
      "suction, lac...\n",
      "\n",
      "2. åŠ è½½æ¨¡å‹å’Œæ£€æµ‹å™¨...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.21it/s]\n",
      "Some weights of the model checkpoint at /root/autodl-tmp/Pro/models--roberta-base-openai-detector/snapshots/6cba99c003b711c7fe94f8a3aa2be35a792cb6fa were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨¡å‹å’Œæ£€æµ‹å™¨åŠ è½½å®Œæˆï¼\n",
      "\n",
      "3. è®¡ç®—SFT Losså’ŒADV Loss...\n",
      "\n",
      "=== ADV Loss è°ƒè¯•ä¿¡æ¯ ===\n",
      "ç”Ÿæˆæ–‡æœ¬: In the quiet of dawn's first light,\n",
      "Where whispers of mist gently take flight,\n",
      "A world awakens, soft and serene,\n",
      "In hues of gold and shades of green.\n",
      "\n",
      "The sun peeks through the sleepy clouds,\n",
      "Its warm...\n",
      "æ£€æµ‹å™¨AIåˆ¤å®šæ¦‚ç‡: 0.999512\n",
      "åŸå§‹ADV Loss (AIæ¦‚ç‡): 0.999512\n",
      "åŠ æƒåADV Loss (Ã—Î»_adv=0.02): 0.019990\n",
      "\n",
      "===== æœ€ç»ˆç»“æœ =====\n",
      "SFT Loss: 3.845113\n",
      "åŸå§‹ADV Loss (AIæ¦‚ç‡): 0.999512\n",
      "åŠ æƒåADV Loss: 0.019990\n",
      "æ£€æµ‹å™¨åˆ¤å®šä¸ºAIçš„æ¦‚ç‡: 0.999512\n",
      "\n",
      "âœ… æ­£å¸¸ï¼šADV Loss é 0ï¼\n"
     ]
    }
   ],
   "source": [
    "#æµ‹è¯•ADV Losså¯è¿ä½œ\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification, AutoTokenizer as DetectorTokenizer\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ======================================================\n",
    "# é…ç½®ï¼ˆæ ¹æ®ä½ çš„å®é™…è·¯å¾„ä¿®æ”¹ï¼‰\n",
    "# ======================================================\n",
    "JSONL_PATH = \"/root/autodl-tmp/Pro/poetry_sft.jsonl\"\n",
    "BASE_MODEL_PATH = \"/root/autodl-tmp/qwen2.5-7b/qwen2.5-7b\"\n",
    "DETECTOR_MODEL_PATH = \"/root/autodl-tmp/Pro/models--roberta-base-openai-detector/snapshots/6cba99c003b711c7fe94f8a3aa2be35a792cb6fa\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "Î»_adv = 0.02  # å¯¹æŠ—æŸå¤±æƒé‡ï¼ˆå’Œä½ çš„è®­ç»ƒæ—¥å¿—ä¸€è‡´ï¼‰\n",
    "MAX_NEW_TOKENS = 128\n",
    "DETECTOR_DTYPE = torch.float16 if torch.cuda.is_available() else torch.float32  # ç»Ÿä¸€æ£€æµ‹å™¨ dtype\n",
    "\n",
    "# ======================================================\n",
    "# 1. ä»…åŠ è½½ç¬¬ä¸€æ¡JSONLæ ·æœ¬\n",
    "# ======================================================\n",
    "def load_single_sample(jsonl_path):\n",
    "    \"\"\"åŠ è½½JSONLæ–‡ä»¶çš„ç¬¬ä¸€æ¡æ ·æœ¬\"\"\"\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        first_line = next(f)\n",
    "        sample = json.loads(first_line.strip())\n",
    "    \n",
    "    # æ„é€ è®­ç»ƒç”¨çš„promptï¼ˆå’Œä½ çš„è®­ç»ƒæ ¼å¼ä¸€è‡´ï¼‰\n",
    "    user_msg = sample[\"messages\"][0][\"content\"]\n",
    "    assistant_msg = sample[\"messages\"][1][\"content\"]\n",
    "    \n",
    "    prompt = (\n",
    "        f\"<|im_start|>user\\n{user_msg}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n{assistant_msg}<|im_end|>\"\n",
    "    )\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"user_msg\": user_msg,\n",
    "        \"assistant_msg\": assistant_msg,\n",
    "        \"raw_sample\": sample\n",
    "    }\n",
    "\n",
    "# ======================================================\n",
    "# 2. åŠ è½½æ¨¡å‹å’Œæ£€æµ‹å™¨ï¼ˆä¿®å¤dtype + çœŸå®æ£€æµ‹å™¨ï¼‰\n",
    "# ======================================================\n",
    "def load_models():\n",
    "    \"\"\"åŠ è½½åŸºç¡€æ¨¡å‹å’ŒçœŸå®çš„AIæ£€æµ‹å™¨ï¼ˆroberta-base-openai-detectorï¼‰\"\"\"\n",
    "    # --------------------------\n",
    "    # 1. åŠ è½½ç”Ÿæˆæ¨¡å‹ï¼ˆQwen2.5-7Bï¼‰\n",
    "    # --------------------------\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        BASE_MODEL_PATH,\n",
    "        trust_remote_code=True,\n",
    "        padding_side=\"right\"\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_PATH,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model.eval()  # ä»…å‰å‘è®¡ç®—ï¼Œä¸è®­ç»ƒ\n",
    "    \n",
    "    # --------------------------\n",
    "    # 2. åŠ è½½çœŸå®çš„AIæ£€æµ‹å™¨ï¼ˆroberta-base-openai-detectorï¼‰\n",
    "    # --------------------------\n",
    "    # åŠ è½½æ£€æµ‹å™¨tokenizer\n",
    "    detector_tokenizer = DetectorTokenizer.from_pretrained(\n",
    "        DETECTOR_MODEL_PATH,\n",
    "        padding_side=\"right\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # åŠ è½½æ£€æµ‹å™¨æ¨¡å‹ï¼ˆä¿®å¤dtypeé—®é¢˜ï¼Œç»Ÿä¸€ä¸ºfloat16/float32ï¼‰\n",
    "    detector = AutoModelForSequenceClassification.from_pretrained(\n",
    "        DETECTOR_MODEL_PATH,\n",
    "        torch_dtype=DETECTOR_DTYPE,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    detector.eval()  # å†»ç»“æ£€æµ‹å™¨ï¼Œä»…å‰å‘\n",
    "    \n",
    "    return tokenizer, model, detector_tokenizer, detector\n",
    "\n",
    "# ======================================================\n",
    "# 3. æ ¸å¿ƒï¼šè®¡ç®—å•æ ·æœ¬çš„SFT Losså’ŒADV Loss\n",
    "# ======================================================\n",
    "def compute_losses(gen_tokenizer, gen_model, det_tokenizer, detector, sample):\n",
    "    \"\"\"è®¡ç®—å•æ ·æœ¬çš„SFT Losså’ŒADV Loss\"\"\"\n",
    "    # --------------------------\n",
    "    # æ­¥éª¤1ï¼šTokenizeè¾“å…¥ï¼ˆç”Ÿæˆæ¨¡å‹ï¼‰\n",
    "    # --------------------------\n",
    "    prompt = sample[\"prompt\"]\n",
    "    inputs = gen_tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=1024\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    \n",
    "    # --------------------------\n",
    "    # æ­¥éª¤2ï¼šè®¡ç®—SFT Lossï¼ˆæ ‡å‡†è¯­è¨€æ¨¡å‹æŸå¤±ï¼‰\n",
    "    # --------------------------\n",
    "    with torch.no_grad():\n",
    "        outputs = gen_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=input_ids  # è‡ªå›å½’æ ‡ç­¾\n",
    "        )\n",
    "    sft_loss = outputs.loss.item()\n",
    "    \n",
    "    # --------------------------\n",
    "    # æ­¥éª¤3ï¼šç”Ÿæˆæ–‡æœ¬ï¼ˆç”¨äºè®¡ç®—ADV Lossï¼‰\n",
    "    # --------------------------\n",
    "    generate_inputs = gen_tokenizer(\n",
    "        f\"<|im_start|>user\\n{sample['user_msg']}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        gen_ids = gen_model.generate(\n",
    "            **generate_inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            temperature=0.0,\n",
    "            do_sample=False,\n",
    "            pad_token_id=gen_tokenizer.pad_token_id,\n",
    "            eos_token_id=gen_tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # è§£ç ç”Ÿæˆçš„æ–‡æœ¬\n",
    "    gen_text = gen_tokenizer.decode(\n",
    "        gen_ids[0][generate_inputs[\"input_ids\"].shape[-1]:],\n",
    "        skip_special_tokens=True\n",
    "    ).strip()\n",
    "    \n",
    "    # --------------------------\n",
    "    # æ­¥éª¤4ï¼šè®¡ç®—ADV Lossï¼ˆæ ¸å¿ƒè°ƒè¯•éƒ¨åˆ†ï¼‰\n",
    "    # --------------------------\n",
    "    print(\"\\n=== ADV Loss è°ƒè¯•ä¿¡æ¯ ===\")\n",
    "    print(f\"ç”Ÿæˆæ–‡æœ¬: {gen_text[:200]}...\")\n",
    "    \n",
    "    # --------------------------\n",
    "    # 4.1 Tokenizeç”Ÿæˆæ–‡æœ¬ï¼ˆæ£€æµ‹å™¨ï¼‰\n",
    "    # --------------------------\n",
    "    det_inputs = det_tokenizer(\n",
    "        gen_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # --------------------------\n",
    "    # 4.2 æ£€æµ‹å™¨å‰å‘è®¡ç®—ï¼ˆä¿®å¤dtypeé—®é¢˜ï¼‰\n",
    "    # --------------------------\n",
    "    with torch.no_grad():\n",
    "        det_outputs = detector(**det_inputs)\n",
    "        # roberta-base-openai-detectorè¾“å‡ºlogitsï¼š[0]=äººç±»æ¦‚ç‡ï¼Œ[1]=AIæ¦‚ç‡\n",
    "        logits = det_outputs.logits\n",
    "        ai_prob = torch.softmax(logits, dim=-1)[0][1].item()  # AIç”Ÿæˆçš„æ¦‚ç‡ï¼ˆ0~1ï¼‰\n",
    "    \n",
    "    print(f\"æ£€æµ‹å™¨AIåˆ¤å®šæ¦‚ç‡: {ai_prob:.6f}\")\n",
    "    \n",
    "    # --------------------------\n",
    "    # 4.3 è®¡ç®—å¯¹æŠ—æŸå¤±ï¼ˆæœ€å°åŒ–AIåˆ¤å®šæ¦‚ç‡ï¼‰\n",
    "    # --------------------------\n",
    "    raw_adv_loss = ai_prob  # åŸå§‹å¯¹æŠ—æŸå¤±ï¼ˆAIæ¦‚ç‡å€¼ï¼‰\n",
    "    weighted_adv_loss = raw_adv_loss * Î»_adv  # åŠ æƒåå¯¹æŠ—æŸå¤±\n",
    "    \n",
    "    print(f\"åŸå§‹ADV Loss (AIæ¦‚ç‡): {raw_adv_loss:.6f}\")\n",
    "    print(f\"åŠ æƒåADV Loss (Ã—Î»_adv={Î»_adv}): {weighted_adv_loss:.6f}\")\n",
    "    \n",
    "    # --------------------------\n",
    "    # è¿”å›ç»“æœ\n",
    "    # --------------------------\n",
    "    return {\n",
    "        \"sft_loss\": sft_loss,\n",
    "        \"raw_adv_loss\": raw_adv_loss,\n",
    "        \"weighted_adv_loss\": weighted_adv_loss,\n",
    "        \"gen_text\": gen_text,\n",
    "        \"detector_ai_prob\": ai_prob\n",
    "    }\n",
    "\n",
    "# ======================================================\n",
    "# 4. ä¸»å‡½æ•°ï¼šè¿è¡Œå•æ ·æœ¬è°ƒè¯•\n",
    "# ======================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"===== å•æ ·æœ¬ ADV Loss è°ƒè¯• =====\")\n",
    "    \n",
    "    # åŠ è½½å•æ ·æœ¬\n",
    "    print(\"\\n1. åŠ è½½ç¬¬ä¸€æ¡JSONLæ ·æœ¬...\")\n",
    "    sample = load_single_sample(JSONL_PATH)\n",
    "    print(f\"æ ·æœ¬Userå†…å®¹: {sample['user_msg'][:100]}...\")\n",
    "    print(f\"æ ·æœ¬Assistantå†…å®¹: {sample['assistant_msg'][:100]}...\")\n",
    "    \n",
    "    # åŠ è½½æ¨¡å‹ï¼ˆç”Ÿæˆæ¨¡å‹ + çœŸå®æ£€æµ‹å™¨ï¼‰\n",
    "    print(\"\\n2. åŠ è½½æ¨¡å‹å’Œæ£€æµ‹å™¨...\")\n",
    "    gen_tokenizer, gen_model, det_tokenizer, detector = load_models()\n",
    "    print(\"æ¨¡å‹å’Œæ£€æµ‹å™¨åŠ è½½å®Œæˆï¼\")\n",
    "    \n",
    "    # è®¡ç®—æŸå¤±\n",
    "    print(\"\\n3. è®¡ç®—SFT Losså’ŒADV Loss...\")\n",
    "    loss_results = compute_losses(gen_tokenizer, gen_model, det_tokenizer, detector, sample)\n",
    "    \n",
    "    # æ‰“å°æœ€ç»ˆç»“æœ\n",
    "    print(\"\\n===== æœ€ç»ˆç»“æœ =====\")\n",
    "    print(f\"SFT Loss: {loss_results['sft_loss']:.6f}\")\n",
    "    print(f\"åŸå§‹ADV Loss (AIæ¦‚ç‡): {loss_results['raw_adv_loss']:.6f}\")\n",
    "    print(f\"åŠ æƒåADV Loss: {loss_results['weighted_adv_loss']:.6f}\")\n",
    "    print(f\"æ£€æµ‹å™¨åˆ¤å®šä¸ºAIçš„æ¦‚ç‡: {loss_results['detector_ai_prob']:.6f}\")\n",
    "    \n",
    "    # éªŒè¯æ˜¯å¦ä¸º0\n",
    "    if abs(loss_results[\"raw_adv_loss\"]) < 1e-6:\n",
    "        print(\"\\nâŒ å¼‚å¸¸ï¼šADV Loss æ¥è¿‘ 0ï¼\")\n",
    "    else:\n",
    "        print(\"\\nâœ… æ­£å¸¸ï¼šADV Loss é 0ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef862f01-9962-457c-b9f6-fd8db77d0cba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qwen_text)",
   "language": "python",
   "name": "qwen_text"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
